{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import umap\n",
    "import copy\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score as nmi,\\\n",
    "f1_score, roc_auc_score, roc_curve, accuracy_score, matthews_corrcoef as mcc\n",
    "from torch.utils.data import Subset\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "import numbers\n",
    "import torch.nn as nn\n",
    "from read_patients import *\n",
    "# from cac import batch_cac\n",
    "# from kmeans import batch_KMeans\n",
    "# from meanshift import batch_MeanShift\n",
    "# from autoencoder import AutoEncoder\n",
    "from sklearn.metrics import davies_bouldin_score as dbs, adjusted_rand_score as ari\n",
    "from matplotlib import pyplot as plt\n",
    "color = ['grey', 'red', 'blue', 'pink', 'brown', 'black', 'magenta', 'purple', 'orange', 'cyan', 'olive']\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.args = args\n",
    "        self.input_dim = args.input_dim\n",
    "        self.output_dim = self.input_dim\n",
    "        self.hidden_dimentions = copy.deepcopy(args.hidden_dims)\n",
    "        self.hidden_dimentions.append(args.latent_dim)\n",
    "        self.dims_list = (self.hidden_dimentions + \n",
    "                          self.hidden_dimentions[:-1][::-1])  # mirrored structure\n",
    "        self.n_layers = len(self.dims_list)\n",
    "        self.n_clusters = args.n_clusters\n",
    "        self.n_classes = args.n_classes\n",
    "        \n",
    "        # Validation check\n",
    "        assert self.n_layers % 2 > 0\n",
    "        assert self.dims_list[self.n_layers // 2] == args.latent_dim\n",
    "        # Encoder Network\n",
    "        layers = OrderedDict()\n",
    "        for idx, hidden_dim in enumerate(self.hidden_dimentions):\n",
    "            if idx == 0:\n",
    "                layers.update(\n",
    "                    {'linear0': nn.Linear(self.input_dim, hidden_dim),\n",
    "                     'activation0': nn.ReLU()\n",
    "                    })\n",
    "            else:\n",
    "                layers.update(\n",
    "                    {'linear{}'.format(idx): nn.Linear(\n",
    "                        self.hidden_dimentions[idx-1], hidden_dim),\n",
    "                     'activation{}'.format(idx): nn.ReLU(),\n",
    "                     'bn{}'.format(idx): nn.BatchNorm1d(self.hidden_dimentions[idx])\n",
    "                    })\n",
    "        self.encoder = nn.Sequential(layers)\n",
    "\n",
    "        # Decoder Network\n",
    "        layers = OrderedDict()\n",
    "        tmp_hidden_dims = self.hidden_dimentions[::-1]\n",
    "        for idx, hidden_dim in enumerate(tmp_hidden_dims):\n",
    "            if idx == len(tmp_hidden_dims) - 1:\n",
    "                layers.update(\n",
    "                    {'linear{}'.format(idx): nn.Linear(\n",
    "                        hidden_dim, self.output_dim),\n",
    "                    })\n",
    "            else:\n",
    "                layers.update(\n",
    "                    {'linear{}'.format(idx): nn.Linear(\n",
    "                        hidden_dim, tmp_hidden_dims[idx+1]),\n",
    "                     'activation{}'.format(idx): nn.ReLU(),\n",
    "                     'bn{}'.format(idx): nn.BatchNorm1d(tmp_hidden_dims[idx+1])\n",
    "                    })\n",
    "        self.decoder = nn.Sequential(layers)\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr_str = '[Structure]: {}-'.format(self.input_dim)\n",
    "        for idx, dim in enumerate(self.dims_list):\n",
    "                repr_str += '{}-'.format(dim)\n",
    "        repr_str += str(self.output_dim) + '\\n'\n",
    "        repr_str += '[n_layers]: {}'.format(self.n_layers) + '\\n'\n",
    "        repr_str += '[n_clusters]: {}'.format(self.n_clusters) + '\\n'\n",
    "        repr_str += '[n_classes]: {}'.format(self.n_classes) + '\\n'\n",
    "        repr_str += '[input_dims]: {}'.format(self.input_dim)\n",
    "        return repr_str\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def forward(self, X, latent=False, classifier_idx=0):\n",
    "        z = self.encoder(X)\n",
    "        if latent:\n",
    "            return z\n",
    "        # probs = self.classifiers[classifier_idx](body_output)\n",
    "        out = self.decoder(z)\n",
    "        return z, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Batch k-Means\n",
    "\n",
    "def _parallel_compute_distance(X, cluster):\n",
    "    n_samples = X.shape[0]\n",
    "    dis_mat = np.zeros((n_samples, 1))\n",
    "    for i in range(n_samples):\n",
    "        dis_mat[i] += np.sqrt(np.sum((X[i] - cluster) ** 2, axis=0))\n",
    "    return dis_mat\n",
    "\n",
    "class batch_KMeans(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.latent_dim = args.latent_dim\n",
    "        self.n_clusters = args.n_clusters\n",
    "        self.count = 100 * torch.ones((self.n_clusters))  # serve as learning rate\n",
    "        self.n_jobs = args.n_jobs\n",
    "        self.clusters = np.zeros((self.n_clusters, self.latent_dim))\n",
    "        self.positive_centers = np.zeros((self.n_clusters, self.latent_dim))\n",
    "        self.negative_centers = np.zeros((self.n_clusters, self.latent_dim))\n",
    "        self.cluster_stats = np.zeros((self.n_clusters,2))\n",
    "\n",
    "    \n",
    "    def _compute_dist(self, X):\n",
    "        dis_mat = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_parallel_compute_distance)(X, self.clusters[i])\n",
    "            for i in range(self.n_clusters))\n",
    "        dis_mat = np.hstack(dis_mat)\n",
    "        \n",
    "        return dis_mat\n",
    "\n",
    "\n",
    "    def init_cluster(self, X, y=None, indices=None):\n",
    "        \"\"\" Generate initial clusters using sklearn.Kmeans \"\"\"\n",
    "        model = KMeans(n_clusters=self.n_clusters,\n",
    "                       n_init=20)\n",
    "        model.fit(X)\n",
    "        self.clusters = model.cluster_centers_  # copy clusters\n",
    "        labels = model.labels_\n",
    "\n",
    "        for j in range(self.n_clusters):\n",
    "            pts_index = np.where(labels == j)[0]\n",
    "            cluster_pts = X[pts_index]        \n",
    "            n_class_index = np.where(y[pts_index] == 0)[0]\n",
    "            p_class_index = np.where(y[pts_index] == 1)[0]\n",
    "\n",
    "            self.cluster_stats[j][0] = len(p_class_index)\n",
    "            self.cluster_stats[j][1] = len(n_class_index)\n",
    "\n",
    "            n_class = cluster_pts[n_class_index]\n",
    "            p_class = cluster_pts[p_class_index]\n",
    "\n",
    "            self.negative_centers[j,:] = n_class.sum(axis=0)/(1+len(n_class))\n",
    "            self.positive_centers[j,:] = p_class.sum(axis=0)/(1+len(p_class))\n",
    "\n",
    "\n",
    "    def update_cluster(self, X, y, cluster_idx):\n",
    "        \"\"\" Update clusters in Kmeans on a batch of data \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        for i in range(n_samples):\n",
    "            self.count[cluster_idx] += 1\n",
    "            eta = 1.0 / self.count[cluster_idx]\n",
    "            updated_cluster = ((1 - eta) * self.clusters[cluster_idx] + \n",
    "                               eta * X[i])\n",
    "            self.clusters[cluster_idx] = updated_cluster\n",
    "            if y[i] == 0:\n",
    "                self.negative_centers[cluster_idx] = ((1 - eta) * self.negative_centers[cluster_idx] + \n",
    "                               eta * X[i])\n",
    "            else:\n",
    "                self.positive_centers[cluster_idx] = ((1 - eta) * self.positive_centers[cluster_idx] + \n",
    "                               eta * X[i])\n",
    "\n",
    "\n",
    "    def update_assign(self, X, y=None):\n",
    "        \"\"\" Assign samples in `X` to clusters \"\"\"\n",
    "        dis_mat = self._compute_dist(X)\n",
    "        new_labels = np.argmin(dis_mat, axis=1)\n",
    "\n",
    "#         if y is not None:\n",
    "#             for j in range(self.n_clusters):\n",
    "#                 pts_index = np.where(new_labels == j)[0]\n",
    "#                 cluster_pts = X[pts_index]        \n",
    "#                 n_class_index = np.where(y[pts_index] == 0)[0]\n",
    "#                 p_class_index = np.where(y[pts_index] == 1)[0]\n",
    "\n",
    "#                 self.cluster_stats[j][0] = len(p_class_index)\n",
    "#                 self.cluster_stats[j][1] = len(n_class_index)\n",
    "\n",
    "#                 n_class = cluster_pts[n_class_index]\n",
    "#                 p_class = cluster_pts[p_class_index]\n",
    "\n",
    "#                 self.negative_centers[j,:] = n_class.mean(axis=0)\n",
    "#                 self.positive_centers[j,:] = p_class.mean(axis=0)\n",
    "\n",
    "        return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "code_folding": [
     7
    ]
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import adjusted_rand_score as ari\n",
    "\n",
    "class batch_cac(object):    \n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.latent_dim = args.latent_dim\n",
    "        self.n_clusters = args.n_clusters\n",
    "        self.cluster_stats = np.zeros((self.n_clusters,2))\n",
    "        self.clusters = np.zeros((self.n_clusters, self.latent_dim))\n",
    "        self.positive_centers = np.zeros((self.n_clusters, self.latent_dim))\n",
    "        self.negative_centers = np.zeros((self.n_clusters, self.latent_dim))\n",
    "        self.positive_sse = np.zeros(self.n_clusters)\n",
    "        self.negative_sse = np.zeros(self.n_clusters)\n",
    "        self.count = 100 * np.ones((self.n_clusters))  # serve as learning rate\n",
    "        self.n_jobs = args.n_jobs\n",
    "\n",
    "\n",
    "    def calculate_gamma_old(self, pt, label, mu, mup, mun, p_sse, n_sse, cluster_stats, beta=1, alpha=2):\n",
    "        p, n = cluster_stats[0], cluster_stats[1]\n",
    "        if label == 0:\n",
    "            mun_new = (n/(n-1))*mun - (1/(n-1))*pt\n",
    "            mup_new = mup\n",
    "            new_n_sse = (n/(n-1))*n_sse - (1/(n-1))*np.linalg.norm(pt-mun_new)*np.linalg.norm(pt-mun)\n",
    "            new_p_sse = p_sse\n",
    "            n_new = n-1\n",
    "            p_new = p\n",
    "\n",
    "        else:\n",
    "            mup_new = (p/(p-1))*mup - (1/(p-1))*pt\n",
    "            mun_new = mun\n",
    "            new_p_sse = (p/(p-1))*p_sse - (1/(p-1))*np.linalg.norm(pt-mup_new)*np.linalg.norm(pt-mup)\n",
    "            new_n_sse = n_sse\n",
    "            p_new = p-1\n",
    "            n_new = n\n",
    "\n",
    "        mu_new = (p_new*mup_new + n_new*mun_new)/(p_new + n_new)\n",
    "        new_lin_sep = np.sum(np.square(mun_new - mup_new))\n",
    "        lin_sep = np.sum(np.square(mun - mup))\n",
    "#         new_lin_sep = np.sum(np.square(mun_new - mup_new))/(new_n_sse + new_p_sse)\n",
    "#         lin_sep = np.sum(np.square(mun - mup))/(n_sse + p_sse)\n",
    "        mu_sep = np.sum(np.square(mu - mu_new))\n",
    "#         gamma_p = -beta*np.sum(np.square(mu-pt)) - (p+n-1) * mu_sep + (p+n) * alpha*lin_sep - (p+n-1)*alpha*new_lin_sep\n",
    "        gamma_p = -np.sum(np.square(mu-pt)) - (p+n-1) * mu_sep + alpha*lin_sep - alpha*new_lin_sep\n",
    "        return gamma_p\n",
    "\n",
    "\n",
    "    def calculate_gamma_new(self, pt, label, mu, mup, mun, p_sse, n_sse, cluster_stats, beta=1, alpha=2):\n",
    "        p, n = cluster_stats[0], cluster_stats[1]\n",
    "        if label == 0:\n",
    "            mun_new = (n/(n+1))*mun + (1/(n+1))*pt\n",
    "            mup_new = mup\n",
    "            new_n_sse = (n/(n+1))*n_sse + (1/(n+1))*np.linalg.norm(pt-mun_new)*np.linalg.norm(pt-mun)\n",
    "            new_p_sse = p_sse\n",
    "            n_new = n+1\n",
    "            p_new = p\n",
    "\n",
    "        else:\n",
    "            mup_new = (p/(p+1))*mup + (1/(p+1))*pt\n",
    "            mun_new = mun\n",
    "            new_p_sse = (p/(p+1))*p_sse + (1/(p+1))*np.linalg.norm(pt-mup_new)*np.linalg.norm(pt-mup)\n",
    "            new_n_sse = n_sse\n",
    "            p_new = p+1\n",
    "            n_new = n\n",
    "\n",
    "        mu_new = (p_new*mup_new + n_new*mun_new)/(p_new + n_new)\n",
    "        new_lin_sep = np.sum(np.square(mun_new - mup_new))\n",
    "        lin_sep = np.sum(np.square(mun - mup))\n",
    "#         new_lin_sep = np.sum(np.square(mun_new - mup_new))/(new_n_sse + new_p_sse)\n",
    "#         lin_sep = np.sum(np.square(mun - mup))/(n_sse + p_sse)\n",
    "        mu_sep = np.sum(np.square(mu - mu_new))\n",
    "\n",
    "#         gamma_j = beta*np.sum(np.square(mu_new-pt)) + (p+n)*mu_sep + (p+n) * alpha*lin_sep - (p+n+1)*alpha*new_lin_sep\n",
    "        gamma_j = np.sum(np.square(mu_new-pt)) + (p+n)*mu_sep + alpha*lin_sep - alpha*new_lin_sep\n",
    "        return gamma_j\n",
    "\n",
    "    def predict_clusters(self, X_test, centers) -> np.array:\n",
    "        K = centers.shape[0]\n",
    "        dists = np.zeros(K)\n",
    "        test_labels = np.zeros(X_test.shape[0])\n",
    "\n",
    "        for pt in range(X_test.shape[0]):\n",
    "            for k in range(K):\n",
    "                min_dist = np.square(np.linalg.norm(centers[k] - X_test[pt]))\n",
    "                dists[k] = min_dist\n",
    "            test_labels[pt] = int(np.argmin(dists))\n",
    "        return test_labels.astype(int)\n",
    "\n",
    "\n",
    "    def update_cluster_centers(self, X, y, cluster_labels):\n",
    "        for j in range(self.n_clusters):\n",
    "            pts_index = np.where(cluster_labels == j)[0]\n",
    "            cluster_pts = X[pts_index]        \n",
    "            for pt in pts_index:\n",
    "                self.count[j] += 1\n",
    "                eta = 1/(self.count[j])\n",
    "                self.clusters[j,:] = (1-eta)*self.clusters[j,:] + eta*X[pt]\n",
    "\n",
    "                if y[pt] == 0:\n",
    "                    self.negative_centers[j,:] = (1-eta)*self.negative_centers[j,:] +\\\n",
    "                                                    eta*X[pt]\n",
    "                else:\n",
    "                    self.positive_centers[j,:] = (1-eta)*self.positive_centers[j,:] +\\\n",
    "                                                    eta*X[pt]\n",
    "\n",
    "            n_class_index = np.where(y[pts_index] == 0)[0]\n",
    "            p_class_index = np.where(y[pts_index] == 1)[0]\n",
    "            self.cluster_stats[j][0] = len(p_class_index)\n",
    "            self.cluster_stats[j][1] = len(n_class_index)\n",
    "            n_class = cluster_pts[n_class_index]\n",
    "            p_class = cluster_pts[p_class_index]\n",
    "            self.negative_sse[j] = np.square(np.linalg.norm(n_class - self.negative_centers[j]))\n",
    "            self.positive_sse[j] = np.square(np.linalg.norm(p_class - self.positive_centers[j]))\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def update(self, X, y, labels, beta, alpha):\n",
    "        total_iterations = 100\n",
    "        k = self.n_clusters\n",
    "        errors = np.zeros((total_iterations, k))\n",
    "        lbls = []\n",
    "        lbls.append(np.copy(labels))\n",
    "\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return cluster_stats, labels, self.clusters, self.positive_centers, self.negative_centers\n",
    "\n",
    "        old_p, old_n = np.copy(self.positive_centers), np.copy(self.negative_centers)\n",
    "\n",
    "        for iteration in range(0, total_iterations):\n",
    "            # print(cluster_stats)\n",
    "            N = X.shape[0]\n",
    "            cluster_label = []\n",
    "            for index_point in range(N):\n",
    "                distance = {}\n",
    "                pt = X[index_point]\n",
    "                pt_label = y[index_point]\n",
    "                cluster_id = labels[index_point]\n",
    "                p, n = self.cluster_stats[cluster_id][0], self.cluster_stats[cluster_id][1]\n",
    "                new_cluster = old_cluster = labels[index_point]\n",
    "                old_err = np.zeros(k)\n",
    "                # Ensure that degeneracy is not happening\n",
    "                if ((p > 2 and pt_label == 1) or (n > 2 and pt_label == 0)):\n",
    "                    for cluster_id in range(0, k):\n",
    "                        if cluster_id != old_cluster:\n",
    "                            distance[cluster_id] = self.calculate_gamma_new(pt, pt_label, self.clusters[cluster_id], self.positive_centers[cluster_id],\\\n",
    "                                                    self.negative_centers[cluster_id], self.positive_sse[cluster_id], self.negative_sse[cluster_id], self.cluster_stats[cluster_id], beta, alpha)\n",
    "                        else:\n",
    "                            distance[cluster_id] = np.infty\n",
    "\n",
    "                    old_gamma = self.calculate_gamma_old(pt, pt_label, self.clusters[old_cluster], self.positive_centers[old_cluster],\\\n",
    "                                                    self.negative_centers[old_cluster], self.positive_sse[old_cluster], self.negative_sse[old_cluster], self.cluster_stats[old_cluster], beta, alpha)\n",
    "                    # new update condition\n",
    "                    new_cluster = min(distance, key=distance.get)\n",
    "                    new_gamma = distance[new_cluster]\n",
    "\n",
    "                    if old_gamma + new_gamma < 0:\n",
    "                        # Remove point from old cluster\n",
    "                        p, n = self.cluster_stats[old_cluster] # Old cluster statistics\n",
    "                        t = p + n\n",
    "\n",
    "                        self.clusters[old_cluster] = (t/(t-1))*self.clusters[old_cluster] - (1/(t-1))*pt\n",
    "\n",
    "                        if pt_label == 0:\n",
    "                            new_mean = (n/(n-1))*self.negative_centers[old_cluster] - (1/(n-1)) * pt\n",
    "                            old_mean = self.negative_centers[old_cluster]\n",
    "                            self.negative_sse[old_cluster] = (n/(n-1))*self.negative_sse[old_cluster] - \\\n",
    "                                    (1/(n-1))*np.linalg.norm(pt-new_mean)*np.linalg.norm(pt-old_mean)\n",
    "                            self.negative_centers[old_cluster] = new_mean\n",
    "                            self.cluster_stats[old_cluster][1] -= 1\n",
    "\n",
    "                        else:\n",
    "                            new_mean = (p/(p-1))*self.positive_centers[old_cluster] - (1/(p-1)) * pt\n",
    "                            old_mean = self.positive_centers[old_cluster]\n",
    "                            self.positive_sse[old_cluster] = (p/(p-1))*self.positive_sse[old_cluster] - \\\n",
    "                                    (1/(p-1)) * np.linalg.norm(pt-new_mean)*np.linalg.norm(pt-old_mean)\n",
    "                            self.positive_centers[old_cluster] = new_mean\n",
    "                            self.cluster_stats[old_cluster][0] -= 1\n",
    "\n",
    "\n",
    "                        # Add point to new cluster\n",
    "                        p, n = self.cluster_stats[new_cluster] # New cluster statistics\n",
    "                        t = p + n\n",
    "                        self.clusters[new_cluster] = (t/(t+1))*self.clusters[new_cluster] + (1/(t+1))*pt\n",
    "\n",
    "                        if pt_label == 0:\n",
    "                            new_mean = (n/(n+1))*self.negative_centers[new_cluster] + (1/(n+1)) * pt\n",
    "                            old_mean = self.negative_centers[new_cluster]\n",
    "                            self.negative_sse[new_cluster] = (n/(n+1))*self.negative_sse[new_cluster] + \\\n",
    "                                    (1/(n+1)) * np.linalg.norm(pt-new_mean) * np.linalg.norm(pt-old_mean)\n",
    "                            self.negative_centers[new_cluster] = new_mean\n",
    "                            self.cluster_stats[new_cluster][1] += 1\n",
    "\n",
    "                        else:\n",
    "                            new_mean = (p/(p+1))*self.positive_centers[new_cluster] + (1/(p+1)) * pt\n",
    "                            old_mean = self.positive_centers[new_cluster]\n",
    "                            self.positive_sse[new_cluster] = (p/(p+1))*self.positive_sse[new_cluster] + \\\n",
    "                                    (1/(p+1)) * np.linalg.norm(pt-new_mean) * np.linalg.norm(pt-old_mean)\n",
    "                            self.positive_centers[new_cluster] = new_mean\n",
    "                            self.cluster_stats[new_cluster][0] += 1\n",
    "\n",
    "                        labels[index_point] = new_cluster\n",
    "\n",
    "\n",
    "            lbls.append(np.copy(labels))\n",
    "\n",
    "            if ((lbls[iteration] == lbls[iteration-1]).all()) and iteration > 0:\n",
    "#                 print(\"converged at itr: \", iteration)\n",
    "                break\n",
    "\n",
    "        return labels\n",
    "\n",
    "    \n",
    "    def cluster(self, X, y, beta, alpha):\n",
    "        # Update assigned cluster labels to points\n",
    "        cluster_labels = self.predict_clusters(X, self.clusters)\n",
    "#         print(\"Std of prev. labels: \", np.std(cluster_labels))\n",
    "        # Do we need this really? ... yes\n",
    "        self.update_cluster_centers(X, y, cluster_labels)\n",
    "\n",
    "        # update cluster centers\n",
    "        new_labels = self.update(X, y, cluster_labels, beta, alpha)\n",
    "#         print(\"Std of new labels: \", np.std(new_labels))\n",
    "\n",
    "        return new_labels\n",
    "\n",
    "\n",
    "    def init_cluster(self, X, y, indices=None):\n",
    "        \"\"\" Generate initial clusters using sklearn.Kmeans \"\"\"\n",
    "        \"\"\" X will be AE embeddings \"\"\"\n",
    "        model = KMeans(n_clusters=self.n_clusters,\n",
    "                       n_init=20)\n",
    "        model.fit(X)\n",
    "        self.clusters = model.cluster_centers_  # copy clusters\n",
    "        labels = model.labels_\n",
    "\n",
    "        for j in range(self.n_clusters):\n",
    "            pts_index = np.where(labels == j)[0]\n",
    "            cluster_pts = X[pts_index]\n",
    "#             assert(np.allclose(self.clusters[j,:], cluster_pts.mean(axis=0)))\n",
    "            n_class_index = np.where(y[pts_index] == 0)[0]\n",
    "            p_class_index = np.where(y[pts_index] == 1)[0]\n",
    "\n",
    "            self.cluster_stats[j][0] = len(p_class_index)\n",
    "            self.cluster_stats[j][1] = len(n_class_index)\n",
    "\n",
    "            n_class = cluster_pts[n_class_index]\n",
    "            p_class = cluster_pts[p_class_index]\n",
    "\n",
    "            self.negative_centers[j,:] = n_class.mean(axis=0)\n",
    "            self.positive_centers[j,:] = p_class.mean(axis=0)\n",
    "\n",
    "            self.negative_sse[j] = np.square(np.linalg.norm(n_class - self.negative_centers[j]))\n",
    "            self.positive_sse[j] = np.square(np.linalg.norm(p_class - self.positive_centers[j]))\n",
    "\n",
    "        # self.clusters = np.random.rand(self.n_clusters, self.latent_dim)  # copy clusters\n",
    "\n",
    "    def update_assign(self, X, target=None):\n",
    "        \"\"\" Assign samples in `X` to clusters \"\"\"\n",
    "        return self.predict_clusters(X, self.clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "def get_dataset(DATASET, base_dir):\n",
    "    if DATASET == \"cic\":\n",
    "        Xa = pd.read_csv(base_dir + \"/CIC/cic_set_a.csv\")\n",
    "        Xb = pd.read_csv(base_dir + \"/CIC/cic_set_b.csv\")\n",
    "        Xc = pd.read_csv(base_dir + \"/CIC/cic_set_c.csv\")\n",
    "\n",
    "        ya = Xa['In-hospital_death']\n",
    "        yb = Xb['In-hospital_death']\n",
    "        yc = Xc['In-hospital_death']\n",
    "\n",
    "        Xa = Xa.drop(columns=['recordid', 'Survival', 'In-hospital_death'])\n",
    "        Xb = Xb.drop(columns=['recordid', 'Survival', 'In-hospital_death'])\n",
    "        Xc = Xc.drop(columns=['recordid', 'Survival', 'In-hospital_death'])\n",
    "\n",
    "        cols = Xa.columns\n",
    "\n",
    "        scale = StandardScaler()\n",
    "        Xa = scale.fit_transform(Xa)\n",
    "        Xb = scale.fit_transform(Xb)\n",
    "        Xc = scale.fit_transform(Xc)\n",
    "\n",
    "        Xa = pd.DataFrame(Xa, columns=cols)\n",
    "        Xb = pd.DataFrame(Xb, columns=cols)\n",
    "        Xc = pd.DataFrame(Xc, columns=cols)\n",
    "\n",
    "        Xa = Xa.fillna(0)\n",
    "        Xb = Xb.fillna(0)\n",
    "        Xc = Xc.fillna(0)\n",
    "\n",
    "        X_train = pd.concat([Xa, Xb])\n",
    "        y_train = pd.concat([ya, yb])\n",
    "\n",
    "        X_test = Xc\n",
    "        y_test = yc\n",
    "\n",
    "        X = pd.concat([X_train, X_test]).to_numpy()\n",
    "        y = pd.concat([y_train, y_test]).to_numpy()\n",
    "\n",
    "    elif DATASET == \"titanic\":\n",
    "        X_train = pd.read_csv(base_dir + \"/\" + DATASET + \"/\" + \"X_train.csv\").to_numpy()\n",
    "        X_test = pd.read_csv(base_dir + \"/\" + DATASET + \"/\" + \"X_test.csv\").to_numpy()\n",
    "        y_train = pd.read_csv(base_dir + \"/\" + DATASET + \"/\" + \"y_train.csv\").to_numpy()\n",
    "        y_test = pd.read_csv(base_dir + \"/\" + DATASET + \"/\" + \"y_test.csv\").to_numpy()\n",
    "\n",
    "        X = np.vstack([X_train, X_test])\n",
    "        y = np.vstack([y_train, y_test])\n",
    "        y1 = []\n",
    "        for i in range(len(y)):\n",
    "            y1.append(y[i][0])\n",
    "        y = np.array(y1)\n",
    "        # X = pd.concat([X_train, X_test]).to_numpy()\n",
    "        # y = pd.concat([y_train, y_test]).to_numpy()\n",
    "    \n",
    "    elif DATASET == \"infant\":\n",
    "        X = pd.read_csv(base_dir + \"/\" + DATASET + \"/\" + \"X.csv\").to_numpy()\n",
    "        y = pd.read_csv(base_dir + \"/\" + DATASET + \"/\" + \"y.csv\").to_numpy()\n",
    "        y1 = []\n",
    "        \n",
    "        for i in range(len(y)):\n",
    "            y1.append(y[i][0])\n",
    "        y = np.array(y1)\n",
    "        y = y.astype(int)\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        X = enc.fit_transform(X).toarray()\n",
    "    \n",
    "    elif DATASET == \"kidney\":\n",
    "        print(\"Fetching Kidney Dataset\")\n",
    "        data = get_aki(base_dir)\n",
    "        X = pd.concat(data,axis=1).T\n",
    "        columns = X.columns\n",
    "\n",
    "        data_columns = list(columns[1:90]) + ['y'] # get the columns which have data, not mask\n",
    "        non_binary_columns = data_columns[:81] # only these columns have non-binary data fit for scaling\n",
    "\n",
    "        X = X.fillna(0)\n",
    "        X = X[data_columns]\n",
    "\n",
    "        y = X['y'].to_numpy().astype(int)\n",
    "        X = X.drop(columns=['y'])\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X[non_binary_columns] = scaler.fit_transform(np.nan_to_num(X[non_binary_columns]))\n",
    "        X = X.to_numpy()\n",
    "\n",
    "    else:\n",
    "        X = pd.read_csv(base_dir + \"/\" + DATASET + \"/\" + \"X.csv\").to_numpy()\n",
    "        y = pd.read_csv(base_dir + \"/\" + DATASET + \"/\" + \"y.csv\").to_numpy()\n",
    "        y1 = []\n",
    "        for i in range(len(y)):\n",
    "            y1.append(y[i][0])\n",
    "        y = np.array(y1)\n",
    "    return X, y\n",
    "\n",
    "def paper_synthetic(n_pts=1000, centers=4):\n",
    "    X, y = make_blobs(n_pts, centers=centers)\n",
    "    W = np.random.randn(10,2)\n",
    "    U = np.random.randn(100,10)\n",
    "    X1 = W.dot(X.T)\n",
    "    X1 = X1*(X1>0)\n",
    "    X2 = U.dot(X1)\n",
    "    X2 = X2*(X2>0)\n",
    "    return X2.T, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "code_folding": [
     150,
     216,
     222
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numbers\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "# from cac import batch_cac\n",
    "import torch.nn.functional as F\n",
    "# from kmeans import batch_KMeans\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import davies_bouldin_score as dbs, adjusted_rand_score as ari\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, f1_score, roc_auc_score, roc_curve, matthews_corrcoef as mcc\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, RidgeClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "color = ['grey', 'red', 'blue', 'pink', 'brown', 'black', 'magenta', 'purple', 'orange', 'cyan', 'olive']\n",
    "\n",
    "class DCN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(DCN, self).__init__()\n",
    "        self.args = args\n",
    "        self.beta = args.beta  # coefficient of the clustering term \n",
    "        self.lamda = args.lamda  # coefficient of the reconstruction term\n",
    "        self.device = torch.device(args.device)\n",
    "        self.latent_dim = args.latent_dim\n",
    "        self.n_clusters = args.n_clusters\n",
    "        \n",
    "        # Validation check\n",
    "        if not self.beta > 0:\n",
    "            msg = 'beta should be greater than 0 but got value = {}.'\n",
    "            raise ValueError(msg.format(self.beta))\n",
    "        \n",
    "        if not self.lamda > 0:\n",
    "            msg = 'lamda should be greater than 0 but got value = {}.'\n",
    "            raise ValueError(msg.format(self.lamda))\n",
    "        \n",
    "        if len(self.args.hidden_dims) == 0:\n",
    "            raise ValueError('No hidden layer specified.')\n",
    "        \n",
    "        if args.clustering == 'kmeans':\n",
    "            self.clustering = batch_KMeans(args)\n",
    "        elif args.clustering == 'meanshift':\n",
    "            self.clustering = batch_MeanShift(args)\n",
    "        elif args.clustering == \"cac\":\n",
    "            self.clustering = batch_cac(args)\n",
    "            self.classifier = args.classifier\n",
    "            self.cluster_classifiers = []\n",
    "            self.base_classifier = []\n",
    "        else:\n",
    "            raise RuntimeError('Error: no clustering chosen')\n",
    "            \n",
    "        self.autoencoder = AutoEncoder(args).to(self.device)\n",
    "        self.criterion  = nn.MSELoss(reduction='mean')\n",
    "        self.classification_criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                          lr=args.lr,\n",
    "                                          weight_decay=args.wd)\n",
    "\n",
    "        self.classifiers = []\n",
    "        for _ in range(self.args.n_clusters):\n",
    "            classifier = nn.Sequential(\n",
    "                nn.Linear(self.latent_dim, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, 8),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(8, self.args.n_classes),\n",
    "            )\n",
    "            optimizer = torch.optim.Adam(classifier.parameters(), lr=self.args.lr)\n",
    "            self.classifiers.append([classifier, optimizer])\n",
    "            \n",
    "\n",
    "    \"\"\" Compute the Equation (5) in the original paper on a data batch \"\"\"\n",
    "    def _loss(self, X, y, cluster_id):\n",
    "        batch_size = X.size()[0]\n",
    "        latent_X, rec_X = self.autoencoder(X)\n",
    "        \n",
    "        # Reconstruction error\n",
    "        rec_loss = self.lamda * self.criterion(X, rec_X)\n",
    "\n",
    "        # Regularization term on clustering\n",
    "        km_loss = torch.tensor(0.).to(self.device)\n",
    "        sep_loss = torch.tensor(0.).to(self.device)\n",
    "\n",
    "        # kmeans = KMeans(n_clusters=args.n_clusters, n_init=20)\n",
    "        # kmeans.fit(latent_X.cpu().numpy())\n",
    "        # clusters = torch.tensor(kmeans.cluster_centers_).to(device)\n",
    "        clusters = torch.zeros((args.n_clusters, args.latent_dim)).to(self.device)\n",
    "        positive_clusters = torch.zeros((args.n_clusters, args.latent_dim)).to(self.device)\n",
    "        negative_clusters = torch.zeros((args.n_clusters, args.latent_dim)).to(self.device)\n",
    "        cluster_stats = np.zeros((args.n_clusters,2))\n",
    "\n",
    "        for j in range(self.args.n_clusters):\n",
    "            pts_index = np.where(cluster_id == j)[0]\n",
    "            cluster_pts = latent_X[pts_index]\n",
    "            n_class_index = np.where(y[pts_index] == 0)[0]\n",
    "            p_class_index = np.where(y[pts_index] == 1)[0]\n",
    "\n",
    "            n_class = cluster_pts[n_class_index]\n",
    "            p_class = cluster_pts[p_class_index]\n",
    "\n",
    "            negative_clusters[j,:] = n_class.sum(axis=0)/(1+len(n_class))\n",
    "            positive_clusters[j,:] = p_class.sum(axis=0)/(1+len(p_class))\n",
    "            clusters[j,:] = cluster_pts.sum(axis=0)/(1+len(cluster_pts))\n",
    "            cluster_stats[j][0] = len(p_class_index)\n",
    "            cluster_stats[j][1] = len(n_class_index)\n",
    "\n",
    "        # clusters = torch.FloatTensor(self.clustering.clusters).to(self.device)\n",
    "        # positive_clusters = torch.FloatTensor(self.clustering.positive_centers).to(self.device)\n",
    "        # negative_clusters = torch.FloatTensor(self.clustering.negative_centers).to(self.device)\n",
    "#         print(cluster_stats)\n",
    "        for i in range(batch_size):\n",
    "            diff_vec = latent_X[i] - clusters[cluster_id[i]]\n",
    "            sample_dist_loss = torch.matmul(diff_vec.view(1, -1),\n",
    "                                            diff_vec.view(-1, 1))\n",
    "            km_loss += 0.5 * self.beta * torch.squeeze(sample_dist_loss)/(1+sum(cluster_stats[cluster_id[i]]))\n",
    "            \n",
    "            # if self.args.clustering == \"cac\":\n",
    "            #    diff_vec = positive_clusters[cluster_id[i]] - negative_clusters[cluster_id[i]]\n",
    "            if y[i] == 0:\n",
    "                diff_vec = latent_X[i] - negative_clusters[cluster_id[i]]\n",
    "            else:\n",
    "                diff_vec = latent_X[i] - positive_clusters[cluster_id[i]]\n",
    "\n",
    "            sample_sep_loss = torch.matmul(diff_vec.view(1, -1),\n",
    "                                            diff_vec.view(-1, 1))\n",
    "            sep_loss += self.args.delta * torch.squeeze(sample_sep_loss)/(1+cluster_stats[cluster_id[i]][y[i]])\n",
    "#             sep_loss += torch.exp(self.args.alpha * torch.squeeze(sample_sep_loss))\n",
    "\n",
    "        \"\"\"\n",
    "        # Just for printing\n",
    "        for j in range(self.args.n_clusters):\n",
    "            diff_vec = positive_clusters[j] - negative_clusters[j]\n",
    "            sample_sep_loss = torch.matmul(diff_vec.view(1, -1),\n",
    "                                            diff_vec.view(-1, 1))\n",
    "            sample_sep_loss = torch.squeeze(sample_sep_loss)\n",
    "            print(\"Log Class Dist: \", np.log(sample_sep_loss))\n",
    "\n",
    "        print(\"Rec Loss: \", rec_loss)\n",
    "        print(\"KM Dist: \", km_loss)\n",
    "        print(\"Sep Dist: \", sep_loss)\n",
    "        \"\"\"\n",
    "    \n",
    "        return (rec_loss + km_loss + sep_loss,\n",
    "                rec_loss,\n",
    "                sep_loss)\n",
    "    \n",
    "    def pretrain(self, train_loader, epoch=100, verbose=True):\n",
    "        if not self.args.pretrain:\n",
    "            return\n",
    "        \n",
    "        if not isinstance(epoch, numbers.Integral):\n",
    "            msg = '`epoch` should be an integer but got value = {}'\n",
    "            raise ValueError(msg.format(epoch))\n",
    "        \n",
    "        if verbose:\n",
    "            print('========== Start pretraining ==========')\n",
    "        \n",
    "        rec_loss_list = []\n",
    "        \n",
    "        self.train()\n",
    "        for e in range(epoch):\n",
    "            for batch_idx, (data, _) in enumerate(train_loader):\n",
    "                batch_size = data.size()[0]\n",
    "                data = data.to(self.device).view(batch_size, -1)\n",
    "                _, rec_X = self.autoencoder.forward(data)\n",
    "                loss = self.criterion(data, rec_X)\n",
    "                if verbose and (batch_idx+1) % self.args.log_interval == 0:\n",
    "                    msg = 'Epoch: {:02d} | Batch: {:03d} | Rec-Loss: {:.3f}'\n",
    "                    print(msg.format(e, batch_idx+1, \n",
    "                                     loss.detach().cpu().numpy()))\n",
    "                    rec_loss_list.append(loss.detach().cpu().numpy())\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        self.eval()\n",
    "        \n",
    "        if verbose:\n",
    "            print('========== End pretraining ==========\\n')\n",
    "\n",
    "        self.pre_cluster(train_loader)\n",
    "                \n",
    "        return rec_loss_list\n",
    "\n",
    "\n",
    "    def pre_cluster(self, train_loader):\n",
    "        # Initialize clusters in self.clustering after pre-training\n",
    "        batch_X = []\n",
    "        batch_y = []\n",
    "        for batch_idx, (data, y) in enumerate(train_loader):\n",
    "            batch_size = data.size()[0]\n",
    "            data = data.to(self.device).view(batch_size, -1)\n",
    "            latent_X = self.autoencoder(data, latent=True)\n",
    "            batch_X.append(latent_X.detach().cpu().numpy())\n",
    "            batch_y.extend(y.detach().cpu().numpy())\n",
    "\n",
    "        X = np.vstack(batch_X)\n",
    "        y = np.array(batch_y)\n",
    "\n",
    "        self.clustering.init_cluster(X, y)\n",
    "        return None\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        qs, z_test = self.forward(X_test)\n",
    "        q_test = qs[0]\n",
    "        cluster_ids = torch.argmax(q_test, axis=1)\n",
    "        preds = torch.zeros((self.n_clusters, 2))\n",
    "        for j in range(self.n_clusters):\n",
    "            preds[j,:] = self.classifiers[cluster_ids[j]]\n",
    "        return preds\n",
    "\n",
    "    # Takes input_dim vector\n",
    "    def forward(self, x, output=\"default\"):\n",
    "        z, x_bar = self.autoencoder(x)\n",
    "        clusters = torch.FloatTensor(self.clustering.clusters).to(self.device)\n",
    "        positive_clusters = torch.FloatTensor(self.clustering.positive_centers).to(self.device)\n",
    "        negative_clusters = torch.FloatTensor(self.clustering.negative_centers).to(self.device)\n",
    "        # Cluster\n",
    "        q = 1.0 / (1.0 + torch.sum(\n",
    "            torch.pow(z.unsqueeze(1) - clusters, 2), 2) / self.args.alpha)\n",
    "        q = q.pow((self.args.alpha + 1.0) / 2.0)\n",
    "        q = (q.t() / torch.sum(q, 1)).t()\n",
    "\n",
    "        q_p = 1.0 / (1.0 + torch.sum(\n",
    "            torch.pow(z.detach().unsqueeze(1) - positive_clusters, 2), 2) / self.args.alpha)\n",
    "        q_p = q_p.pow((self.args.alpha + 1.0) / 2.0)\n",
    "        q_p = (q_p.t() / torch.sum(q_p, 1)).t()\n",
    "\n",
    "        q_n = 1.0 / (1.0 + torch.sum(\n",
    "            torch.pow(z.detach().unsqueeze(1) - negative_clusters, 2), 2) / self.args.alpha)\n",
    "        q_n = q_n.pow((self.args.alpha + 1.0) / 2.0)\n",
    "        q_n = (q_n.t() / torch.sum(q_n, 1)).t()\n",
    "\n",
    "        if output == \"latent\":\n",
    "            return z, (q, q_p, q_n)\n",
    "\n",
    "        elif output == \"classifier\":\n",
    "            preds = torch.zeros((len(z), 2))\n",
    "            for j in range(len(z)):\n",
    "                preds[j,:] = self.classifiers[j](z)\n",
    "            return preds\n",
    "        \n",
    "        else:\n",
    "            return z, x_bar, (q, q_p, q_n)\n",
    "\n",
    "\n",
    "    def fit(self, epoch, train_loader, verbose=True):\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        cluster_ids_train = []\n",
    "        total_loss = torch.tensor(0.).to(self.device)\n",
    "        total_cac_loss = torch.tensor(0.).to(self.device)\n",
    "        total_rec_loss = torch.tensor(0.).to(self.device)\n",
    "        total_classification_loss = torch.tensor(0.).to(self.device)\n",
    "        \n",
    "        for batch_idx, (data, y) in enumerate(train_loader):\n",
    "            batch_size = data.size()[0]\n",
    "            data = data.view(batch_size, -1).to(self.device)\n",
    "\n",
    "            # Collect training data and labels for the later classifier\n",
    "            X_train.append(data.cpu().numpy())\n",
    "            y_train.extend(y.numpy())\n",
    "\n",
    "            # Get the latent features\n",
    "            with torch.no_grad():\n",
    "                latent_X = self.autoencoder(data, latent=True)\n",
    "\n",
    "            if self.args.clustering == \"cac\":\n",
    "                cluster_id = self.clustering.cluster(latent_X, y, self.args.beta, self.args.alpha)\n",
    "\n",
    "            else:\n",
    "                # [Step-1] Update the assignment results\n",
    "                cluster_id = self.clustering.update_assign(latent_X.detach().numpy(), y)\n",
    "\n",
    "                # [Step-2] Update cluster centers in batch Clustering\n",
    "                elem_count = np.bincount(cluster_id,\n",
    "                                         minlength=self.args.n_clusters)\n",
    "\n",
    "                for k in range(self.args.n_clusters):\n",
    "                    # avoid empty slicing\n",
    "                    if elem_count[k] == 0:\n",
    "                        continue\n",
    "                    # updating the cluster center\n",
    "                    self.clustering.update_cluster(latent_X[cluster_id == k].detach().numpy(), y, k)\n",
    "            \n",
    "            # [Step-3] Update the Local networks\n",
    "            X_latents, x_bar, qs = self.forward(data)\n",
    "            q, q_p, q_n = qs\n",
    "            classifier_labels = np.zeros(len(data))\n",
    "            class_loss = torch.tensor(0.).to(args.device)\n",
    "\n",
    "            for _ in range(epoch):\n",
    "            # Choose classifier for a point probabilistically\n",
    "                for j in range(len(data)):\n",
    "                    classifier_labels[j] = np.random.choice(range(self.n_clusters), p = q[j].detach().numpy())\n",
    "\n",
    "                for k in range(self.n_clusters):\n",
    "                    idx_cluster = np.where(classifier_labels == k)[0]\n",
    "                    X_cluster = latent_X[idx_cluster]\n",
    "                    y_cluster = y[idx_cluster]\n",
    "\n",
    "                    classifier_k, optimizer_k = self.classifiers[k]\n",
    "                    y_pred_cluster = classifier_k(X_cluster.detach())\n",
    "                    cluster_loss = self.classification_criterion(y_pred_cluster, y_cluster)\n",
    "                    optimizer_k.zero_grad()\n",
    "                    cluster_loss.backward(retain_graph=True)\n",
    "                    optimizer_k.step()\n",
    "\n",
    "            # Backprop through the encoder now\n",
    "            for k in range(args.n_clusters):\n",
    "                idx_cluster = np.where(classifier_labels == k)[0]\n",
    "                X_cluster = latent_X[idx_cluster]\n",
    "                y_cluster = y[idx_cluster]\n",
    "\n",
    "                classifier_k, optimizer_k = model.classifiers[k]\n",
    "                y_pred_cluster = classifier_k(X_cluster)\n",
    "                cluster_los = self.classification_criterion(y_pred_cluster, y_cluster)\n",
    "                class_loss += cluster_los\n",
    "\n",
    "            # [Step-4] Update the network parameters\n",
    "            loss, rec_loss, dist_loss = self._loss(data, y, cluster_id)\n",
    "            loss += self.args.gamma * (1-np.exp(-epoch/15)) * class_loss # Add classification loss\n",
    "            total_loss += loss\n",
    "            total_cac_loss += dist_loss\n",
    "            total_rec_loss += rec_loss\n",
    "            total_classification_loss += class_loss\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Propagate error through the reconstruction head\n",
    "            loss.backward()\n",
    "\n",
    "            # Propagate error through the classification head\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if verbose and (batch_idx+1) % self.args.log_interval == 0:\n",
    "                msg = 'Epoch: {:02d} | Batch: {:03d} | Loss: {:.3f} | Rec-' \\\n",
    "                      'Loss: {:.3f} | Dist-Loss: {:.3f} | Classification-Loss: {:.3f}'\n",
    "                print(msg.format(epoch, batch_idx+1, \n",
    "                                 loss.detach().cpu().numpy(),\n",
    "                                 rec_loss, dist_loss, class_loss))\n",
    "\n",
    "        msg = 'Epoch: {:02d} Loss: {:.3f} | Rec-' \\\n",
    "              'Loss: {:.3f} | Dist-Loss: {:.3f} | Classification-Loss: {:.3f}'\n",
    "        print(msg.format(epoch, \n",
    "                         total_loss.detach().cpu().numpy(),\n",
    "                         total_rec_loss, total_cac_loss, total_classification_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "code_folding": [
     0,
     53,
     78
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    y_classifier_pred = []\n",
    "    y_classifier_pred_proba = []\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        batch_size = data.size()[0]\n",
    "        X_test.append(data)\n",
    "        data = data.view(batch_size, -1).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            latent_X = model.autoencoder(data, latent=True)\n",
    "            latent_X = latent_X.detach().to(model.device).numpy()\n",
    "\n",
    "        y_test.append(target.view(-1, 1).numpy())\n",
    "    \n",
    "    X_test = torch.vstack(X_test)\n",
    "    latent_X = model.autoencoder(X_test, latent=True)\n",
    "    X_test = latent_X.detach().to(model.device).numpy()\n",
    "    y_test = np.vstack(y_test).reshape(-1)\n",
    "    y_pred = model.clustering.update_assign(X_test).reshape(-1)\n",
    "    nmi, ari = normalized_mutual_info_score(y_test, y_pred), adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "    if model.clustering == \"cac\":\n",
    "        base_f1 = f1_score(y_test, model.base_classifier[-1].predict(X_test))\n",
    "        base_mcc = mcc(y_test, model.base_classifier[-1].predict(X_test))\n",
    "        base_auc = roc_auc_score(y_test, model.base_classifier[-1].predict_proba(X_test)[:,1])\n",
    "\n",
    "        X_cluster_test = []\n",
    "        y_cluster_test = []\n",
    "\n",
    "        for j in range(model.args.n_clusters):\n",
    "            cluster_index = np.where(y_pred == j)[0]\n",
    "            X_cluster = X_test[cluster_index]\n",
    "            y_cluster = y_test[cluster_index]\n",
    "\n",
    "            X_cluster_test.append(X_cluster)\n",
    "            y_cluster_test.extend(y_cluster)\n",
    "\n",
    "            # Select the cluster classifiers appearing in the latest iteration\n",
    "            y_classifier_pred.extend(model.cluster_classifiers[-1][j].predict(X_cluster))\n",
    "            y_classifier_pred_proba.extend(model.cluster_classifiers[-1][j].predict_proba(X_cluster)[:,1])\n",
    "\n",
    "        cac_f1 = f1_score(y_cluster_test, y_classifier_pred)\n",
    "        cac_mcc = mcc(y_cluster_test, y_classifier_pred)\n",
    "        cac_auc = roc_auc_score(y_test, y_classifier_pred_proba)\n",
    "\n",
    "        return (nmi, ari, base_f1, base_mcc, base_auc, cac_f1, cac_mcc, cac_auc)\n",
    "    else:\n",
    "        return (nmi, ari)\n",
    "    \n",
    "def solver(args, model, X_train, train_loader, test_loader):\n",
    "    rec_loss_list = model.pretrain(train_loader, epoch=args.pre_epoch)\n",
    "    nmi_list = []\n",
    "    ari_list = []\n",
    "\n",
    "    for e in range(args.epoch):\n",
    "        model.train()\n",
    "        model.fit(e, train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        out = evaluate(model, test_loader)  # evaluation on the test_loader\n",
    "        if len(out) > 2:\n",
    "            NMI, ARI, base_f1, base_mcc, base_auc, cac_f1, cac_mcc, cac_auc = out\n",
    "            print('Epoch: {:02d} | NMI: {:.3f} | ARI: {:.3f} | Base_F1: {:.3f} | Base_MCC: {:.3f} | Base_AUC: {:.3f} | CAC_F1: {:.3f} | CAC_MCC: {:.3f} | CAC_AUC: {:.3f}'.format(\n",
    "                e+1, NMI, ARI, base_f1, base_mcc, base_auc, cac_f1, cac_mcc, cac_auc))\n",
    "        else:\n",
    "            NMI, ARI = out\n",
    "            print('Epoch: {:02d} | NMI: {:.3f} | ARI: {:.3f}'.format(e+1, NMI, ARI))\n",
    "\n",
    "        nmi_list.append(NMI)\n",
    "        ari_list.append(ARI)\n",
    "        \n",
    "        \n",
    "    return rec_loss_list, nmi_list, ari_list\n",
    "\n",
    "def create_imbalanced_data_clusters(n_samples=1000, n_features=8, n_informative=5, n_classes=2,\\\n",
    "                            n_clusters = 2, frac=0.4, outer_class_sep=0.5, inner_class_sep=0.2, clus_per_class=2, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    X = np.empty(shape=n_features)\n",
    "    Y = np.empty(shape=1)\n",
    "    offsets = np.random.normal(0, outer_class_sep, size=(n_clusters, n_features))\n",
    "    for i in range(n_clusters):\n",
    "        samples = int(np.random.normal(n_samples, n_samples/10))\n",
    "        x, y = make_classification(n_samples=samples, n_features=n_features, n_informative=n_informative,\\\n",
    "                                    n_classes=n_classes, class_sep=inner_class_sep, n_clusters_per_class=clus_per_class)\n",
    "                                    # n_repeated=0, n_redundant=0)\n",
    "        x += offsets[i]\n",
    "        y_0 = np.where(y == 0)[0]\n",
    "        y_1 = np.where(y != 0)[0]\n",
    "        y_1 = np.random.choice(y_1, int(np.random.normal(frac, frac/4)*len(y_1)))\n",
    "        index = np.hstack([y_0,y_1])\n",
    "        np.random.shuffle(index)\n",
    "        x_new = x[index]\n",
    "        y_new = y[index]\n",
    "\n",
    "        X = np.vstack((X,x_new))\n",
    "        Y = np.hstack((Y,y_new))\n",
    "\n",
    "    X = pd.DataFrame(X[1:,:])\n",
    "    Y = Y[1:]\n",
    "    return X, np.array(Y).astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "params = {\n",
    "'dir' : 'synthetic',\n",
    "'dataset' : 'magic',\n",
    "\n",
    "# Training parameters\n",
    "'lr' : 0.002,\n",
    "'alpha' : 1,\n",
    "'wd' : 5e-4,\n",
    "'batch_size' : 256,\n",
    "'epoch' : 50,\n",
    "'pre_epoch' : 10,\n",
    "'pretrain' : True,\n",
    "\"load_ae\": False,\n",
    "\"classifier\": \"LR\",\n",
    "\n",
    "# Model parameters\n",
    "'lamda' : 1, # reconstruction error weight\n",
    "'beta' : 5, # KM weight\n",
    "'gamma' : 1, # classifier weight\n",
    "'delta' : 2, # class separability weight\n",
    "'hidden_dims' : [128, 64, 32],\n",
    "'latent_dim' : 20,\n",
    "'n_clusters' : 2,\n",
    "'clustering' : 'kmeans',\n",
    "'n_classes'  : 2,\n",
    "\n",
    "# Utility parameters\n",
    "'n_jobs' : 6,\n",
    "'device' : 'cpu',\n",
    "'log_interval' : 10}\n",
    "\n",
    "class parameters(object):\n",
    "    def __init__(self, params):\n",
    "        self.dir = params['dir']\n",
    "        self.input_dim = -1\n",
    "        self.dataset = params['dataset']\n",
    "        \n",
    "        # Training parameters\n",
    "        self.lr = params['lr']\n",
    "        self.alpha = params['alpha']\n",
    "        self.delta = params['delta']\n",
    "        self.wd = params['wd']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.epoch = params['epoch']\n",
    "        self.pre_epoch = params['pre_epoch']\n",
    "        self.pretrain = params['pretrain']\n",
    "        self.load_ae = params['load_ae']\n",
    "        self.classifier = params['classifier']\n",
    "\n",
    "        # Model parameters\n",
    "        self.lamda = params['lamda']\n",
    "        self.beta = params['beta']\n",
    "        self.gamma = params['gamma']\n",
    "        self.hidden_dims = params['hidden_dims']\n",
    "        self.latent_dim = params['latent_dim']\n",
    "        self.n_clusters = params['n_clusters']\n",
    "        self.clustering = params['clustering']\n",
    "        self.n_classes = params['n_classes']\n",
    "\n",
    "        # Utility parameters\n",
    "        self.n_jobs = params['n_jobs']\n",
    "        self.device = params['device']\n",
    "        self.log_interval = params['log_interval']\n",
    "\n",
    "args = parameters(params)\n",
    "datasets = ['titanic', 'magic', 'creditcard', 'adult', 'diabetes',\\\n",
    "            'cic', 'sepsis', 'synthetic', 'paper_synthetic', 'kidney', 'infant', 'wid_mortality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset: magic\n"
     ]
    }
   ],
   "source": [
    "if args.dataset in datasets:\n",
    "    base_dir = \"/Users/shivin/Document/NUS/Research/Data\"\n",
    "    print(\"Loading Dataset:\", args.dataset)\n",
    "    if args.dataset != \"kidney\":\n",
    "        if args.dataset == \"synthetic\":\n",
    "            n_feat = 45\n",
    "            X, y = create_imbalanced_data_clusters(n_samples=5000,\\\n",
    "                   n_clusters=args.n_clusters, n_features = n_feat,\\\n",
    "                   inner_class_sep=0.2, outer_class_sep=2, seed=0)\n",
    "            args.input_dim = n_feat\n",
    "\n",
    "        elif args.dataset == \"paper_synthetic\":\n",
    "            n_feat = 100\n",
    "            X, y = paper_synthetic(2500, centers=4)\n",
    "            args.input_dim = n_feat\n",
    "            print(args.input_dim)\n",
    "\n",
    "        else:\n",
    "            X, y = get_dataset(args.dataset, base_dir)\n",
    "            args.input_dim = X.shape[1]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "        args.input_dim = X_train.shape[1]\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.fit_transform(X_test)\n",
    "        X_train_data_loader = list(zip(X_train.astype(np.float32), y_train))\n",
    "        X_test_data_loader  = list(zip(X_test.astype(np.float32), y_test))\n",
    "\n",
    "    else:\n",
    "        print(\"Loading Kidney Train\")\n",
    "#         X_train, y_train = get_dataset(args.dataset, \"/Users/shivin/Document/NUS/Research/Data/aki/train\")\n",
    "        args.input_dim = X_train.shape[1]\n",
    "        print(args.input_dim)\n",
    "\n",
    "        print(\"Loading Kidney Test\")\n",
    "#         X_test, y_test = get_dataset(args.dataset, \"/Users/shivin/Document/NUS/Research/Data/aki/test\")\n",
    "\n",
    "        X_train_data_loader = list(zip(X_train.astype(np.float32), y_train))\n",
    "        X_test_data_loader  = list(zip(X_test.astype(np.float32), y_test))\n",
    "\n",
    "        \n",
    "    train_loader = torch.utils.data.DataLoader(X_train_data_loader,\n",
    "        batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(X_test_data_loader, \n",
    "        batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_kidney' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-e364be780ffe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# X_test_kidney = X_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# y_test_kidney = y_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_kidney\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train_kidney\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_kidney\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_kidney' is not defined"
     ]
    }
   ],
   "source": [
    "# X_train_kidney = X_train\n",
    "# y_train_kidney = y_train\n",
    "# X_test_kidney = X_test\n",
    "# y_test_kidney = y_test\n",
    "X_train = X_train_kidney\n",
    "y_train = y_train_kidney\n",
    "X_test = X_test_kidney\n",
    "y_test = y_test_kidney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Start pretraining ==========\n",
      "Epoch: 00 | Batch: 010 | Rec-Loss: 1.032\n",
      "Epoch: 00 | Batch: 020 | Rec-Loss: 0.792\n",
      "Epoch: 00 | Batch: 030 | Rec-Loss: 0.738\n",
      "Epoch: 01 | Batch: 010 | Rec-Loss: 0.719\n",
      "Epoch: 01 | Batch: 020 | Rec-Loss: 0.726\n",
      "Epoch: 01 | Batch: 030 | Rec-Loss: 0.756\n",
      "Epoch: 02 | Batch: 010 | Rec-Loss: 0.631\n",
      "Epoch: 02 | Batch: 020 | Rec-Loss: 0.760\n",
      "Epoch: 02 | Batch: 030 | Rec-Loss: 0.649\n",
      "Epoch: 03 | Batch: 010 | Rec-Loss: 0.619\n",
      "Epoch: 03 | Batch: 020 | Rec-Loss: 0.651\n",
      "Epoch: 03 | Batch: 030 | Rec-Loss: 0.559\n",
      "Epoch: 04 | Batch: 010 | Rec-Loss: 0.612\n",
      "Epoch: 04 | Batch: 020 | Rec-Loss: 0.602\n",
      "Epoch: 04 | Batch: 030 | Rec-Loss: 0.578\n",
      "Epoch: 05 | Batch: 010 | Rec-Loss: 0.604\n",
      "Epoch: 05 | Batch: 020 | Rec-Loss: 0.546\n",
      "Epoch: 05 | Batch: 030 | Rec-Loss: 0.453\n",
      "Epoch: 06 | Batch: 010 | Rec-Loss: 0.626\n",
      "Epoch: 06 | Batch: 020 | Rec-Loss: 0.482\n",
      "Epoch: 06 | Batch: 030 | Rec-Loss: 0.506\n",
      "Epoch: 07 | Batch: 010 | Rec-Loss: 0.493\n",
      "Epoch: 07 | Batch: 020 | Rec-Loss: 0.598\n",
      "Epoch: 07 | Batch: 030 | Rec-Loss: 0.520\n",
      "Epoch: 08 | Batch: 010 | Rec-Loss: 0.462\n",
      "Epoch: 08 | Batch: 020 | Rec-Loss: 0.434\n",
      "Epoch: 08 | Batch: 030 | Rec-Loss: 0.425\n",
      "Epoch: 09 | Batch: 010 | Rec-Loss: 0.438\n",
      "Epoch: 09 | Batch: 020 | Rec-Loss: 0.455\n",
      "Epoch: 09 | Batch: 030 | Rec-Loss: 0.433\n",
      "========== End pretraining ==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# args.input_dim = X_train.shape[1]\n",
    "model = DCN(args)\n",
    "rec_loss_list = model.pretrain(train_loader, epoch=args.pre_epoch)\n",
    "pre_trained_AE = copy.deepcopy(model.autoencoder)\n",
    "# model.autoencoder = pre_trained_AE\n",
    "# model.pre_cluster(train_loader)\n",
    "nmi_list = []\n",
    "ari_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24308062575210587 0.6116144750803041\n",
      "0.2546075085324232 0.5228775744538596\n",
      "Iter 0 : Acc 0.0000 , nmi 0.0000 , ari 0.0000 , Test F1 0.255, Test AUC 0.523\n",
      "Epoch: 00 | Batch: 010 | Loss: nan | Rec-Loss: 0.970 | Dist-Loss: 38.446 | Classification-Loss: nan\n",
      "Epoch: 00 | Batch: 020 | Loss: nan | Rec-Loss: 0.864 | Dist-Loss: 18.473 | Classification-Loss: nan\n",
      "Epoch: 00 | Batch: 030 | Loss: nan | Rec-Loss: 0.844 | Dist-Loss: 9.908 | Classification-Loss: nan\n",
      "Epoch: 00 Loss: nan | Rec-Loss: 31.978 | Dist-Loss: 1769.373 | Classification-Loss: nan\n",
      "Epoch: 01 | NMI: 0.013 | ARI: 0.037\n",
      "Epoch: 01 | NMI: 0.013 | ARI: 0.037\n",
      "Epoch: 01 | Batch: 010 | Loss: 12.364 | Rec-Loss: 0.997 | Dist-Loss: 9.861 | Classification-Loss: 1.342\n",
      "Epoch: 01 | Batch: 020 | Loss: 7.131 | Rec-Loss: 0.860 | Dist-Loss: 5.440 | Classification-Loss: 1.173\n",
      "Epoch: 01 | Batch: 030 | Loss: 4.625 | Rec-Loss: 0.880 | Dist-Loss: 3.150 | Classification-Loss: 1.079\n",
      "Epoch: 01 Loss: 350.874 | Rec-Loss: 32.604 | Dist-Loss: 277.419 | Classification-Loss: 43.957\n",
      "Epoch: 02 | NMI: 0.011 | ARI: 0.030\n",
      "Epoch: 02 | NMI: 0.011 | ARI: 0.030\n",
      "Epoch: 02 | Batch: 010 | Loss: 6.219 | Rec-Loss: 0.975 | Dist-Loss: 4.503 | Classification-Loss: 0.955\n",
      "Epoch: 02 | Batch: 020 | Loss: 7.630 | Rec-Loss: 0.832 | Dist-Loss: 5.791 | Classification-Loss: 0.923\n",
      "Epoch: 02 | Batch: 030 | Loss: 7.152 | Rec-Loss: 1.016 | Dist-Loss: 5.240 | Classification-Loss: 0.954\n",
      "Epoch: 02 Loss: 273.661 | Rec-Loss: 33.086 | Dist-Loss: 210.026 | Classification-Loss: 31.313\n",
      "Epoch: 03 | NMI: 0.019 | ARI: 0.058\n",
      "Epoch: 03 | NMI: 0.019 | ARI: 0.058\n",
      "Epoch: 03 | Batch: 010 | Loss: 4.909 | Rec-Loss: 0.940 | Dist-Loss: 3.141 | Classification-Loss: 0.950\n",
      "Epoch: 03 | Batch: 020 | Loss: 4.054 | Rec-Loss: 0.894 | Dist-Loss: 2.590 | Classification-Loss: 0.855\n",
      "Epoch: 03 | Batch: 030 | Loss: 5.732 | Rec-Loss: 0.824 | Dist-Loss: 4.216 | Classification-Loss: 0.869\n",
      "Epoch: 03 Loss: 218.595 | Rec-Loss: 32.544 | Dist-Loss: 159.923 | Classification-Loss: 30.448\n",
      "Epoch: 04 | NMI: 0.016 | ARI: 0.049\n",
      "Epoch: 04 | NMI: 0.016 | ARI: 0.049\n",
      "Epoch: 04 | Batch: 010 | Loss: 3.653 | Rec-Loss: 0.846 | Dist-Loss: 2.318 | Classification-Loss: 0.891\n",
      "Epoch: 04 | Batch: 020 | Loss: 3.341 | Rec-Loss: 0.992 | Dist-Loss: 1.901 | Classification-Loss: 0.896\n",
      "Epoch: 04 | Batch: 030 | Loss: 6.598 | Rec-Loss: 0.865 | Dist-Loss: 5.012 | Classification-Loss: 0.717\n",
      "Epoch: 04 Loss: 209.257 | Rec-Loss: 32.513 | Dist-Loss: 152.309 | Classification-Loss: 30.392\n",
      "Epoch: 05 | NMI: 0.018 | ARI: 0.061\n",
      "Epoch: 05 | NMI: 0.018 | ARI: 0.061\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEVCAYAAAAFNZUcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABJgElEQVR4nO3dd3xUVfr48c8zk0mA0AIJVRCESBEFMbpYEVEUXbGtfZW1uyt2/S1rAYGvfUVlld0Fy9oVF1FXbICsiB2Bld47AUJNIG3K+f0xkzAJKVPunZbn/XrllcnMvec+yZw8c+65554jxhiUUkqlJke8A1BKKWUfTfJKKZXCNMkrpVQK0ySvlFIpTJO8UkqlME3ySimVwjTJK1uJyCMi8ma840gWInK6iGyO9b4qdWmSV1ETkatEZJ6I7BeRfBH5TEROsbD8LiJiRCTNqjLtJiJ/EJG58Y5DKU3yKioicg/wHPAY0BboDEwELohjWFUk04eDUlbTJK8iJiItgLHAbcaYD4wxB4wxbmPMf4wx99ew/SHdCSKyXkTODDw+IXBGUCgi20VkfGCzOYHvewNnCycGtr9eRJaJyB4R+UJEDg8q14jIbSKyClglfs+KyA4R2Sciv4pInxpivEJE5lV77m4R+Tjw+FwRWSoiRSKyRUTui+Dvdl0g7iIRWSsit9SwzQMisjPw97k66PkMEfmriGwM/I3+ISKNaznOnwMxFonIChEZHG6sKvlpklfROBFoBEyzqLzngeeNMc2BbsCUwPOnBb63NMY0NcZ8LyIXAg8AFwM5wDfAO9XKuxD4DdAbGBIo50igJXA5sKuGGD4GeohIbtBzVwFvBx6/DNxijGkG9AG+iuD33AH8FmgOXAc8KyL9g15vB2QDHYHhwCQR6RF47cnA79AP6B7YZlT1AwS2HwEcH4j1bGB9BLGqJKdJXkWjNbDTGOOxqDw30F1Eso0x+40xP9Sx7S3A48aYZYHjPwb0C27NB17fbYwpCZTdDOgJSGC//OqFGmOKgY+AKwECyb4n/uRfEWNvEWlujNljjJkf7i9pjJlujFlj/L4GvgROrbbZw8aYssDr04HLRESAm4C7A79XUeD3vqKGw3iBjECsLmPMemPMmnBjVclPk7yKxi4g28I+7xvwt1KXi8jPIvLbOrY9HHheRPaKyF5gNyD4W7YVNlU8MMZ8BbwAvAhsF5FJItK8lrLfJpDk8bfiPwwkf4BLgHOBDSLydUXXUThEZKiI/CAiuwOxn4u/5V5hjzHmQNDPG4AO+M9YmgC/BP3enweer8IYsxq4C3gE2CEi74pIh3BjVclPk7yKxvdAKf5ukVAcwJ+kABARJ0EJyhizyhhzJdAGf7fEv0UkE6hpqtRN+LtNWgZ9NTbGfBe0TZX9jDETjDHHAUfh/zA55LpBwJf4P7z64U/2FV01GGN+NsZcEIjxQw52KYVERDKAqcBfgbbGmJbAp/g/oCpkBX7vCp2BrcBOoAQ4Kuh3bmGMaVrTsYwxbxtjTsH/gWjw/01VA6NJXkXMGLMPf3/wiyJyoYg0ERFXoKX6VA27rAQaich5IuICHsLfpQCAiPxeRHKMMT5gb+BpL1AA+IAjgsr6B/AXETkqsG8LEbm0tlhF5HgR+U3guAfwfzh5a/m9PMC/gaeBVsCMQBnpInK1iLQwxriBwtrKOHhYaRT8BaQHfucCwCMiQ/FfL6huTOB4p+Lvv38/8HeZjL8Pv03gAB1F5OwaDtxDRM4IfKiU4v9wqCtWlaI0yauoGGPGA/fgT9gF+FvYI/C3cqtvuw/4E/ASsAV/sg0ebXMOsERE9uO/CHuFMaY00FXyKPBtoJtigDFmGv6W6bsiUggsBobWEWpz/AlyD/7uj134W9O1eRs4E39yDb7mcA2wPnDMW4Hf11HGSfiTa/WvO/CfAezB3x30cbX9tgVe2wq8BdxqjFkeeO3PwGrgh0AMM4EeHCoDeAJ/638b/jOPB+qIVaUo0UVDlFIqdWlLXimlUpgmeaWUSmGa5JVSKoVpkldKqRSmSV4ppVKYJnmllEphmuSVUiqFaZJXSqkUpkleKaVSmCZ5pZRKYZrklVIqhWmSV0qpFKZJXimlUpgmeaWUSmGa5JVSKoVpkldKqRSmSV4ppVJYWrwDCJadnW26dOkS7zBUCvvll192GmNy6t/SWlq3lZ3qqtcJleS7dOnCvHnz4h2GSmEisiEex9W6rexUV73W7hqllEphmuSVUiqFaZJXSqkUpkleKaVSmCb5BmzBggX87W9/45tvvol3KEpZxhjDlClTmDhxInv27Il3OHGXUKNrVGwUFxfz9NNPV/781Vdf8dVXX3HVVVeRm5sbx8iUis57773H8uXLwRgAJjz/PIgwevToOEcWP1G35EWkk4jMFpFlIrJERO4MPP+IiGwRkYWBr3OjD1dZITjBB3v77bfxeDwxjiZxad1OLsXFxSxftsyf4EUOfhnDmIcfjnd4cWNFd40HuNcY0wsYANwmIr0Drz1rjOkX+PrUgmMpmz366KPxDiGRaN1OIhMmTPA/EKn6ggg4nbz+4ouxDyoBRJ3kjTH5xpj5gcdFwDKgY7TlKnuYwGlsXWpr6Tc0WreTS1lZWZ2vrysooKSkJEbRJA5LL7yKSBfgWODHwFMjRORXEXlFRLJq2edmEZknIvMKCgqsDEfVQKq3cmpQXFzMzp07YxBN8tC6nfg6depU+4uBev/UE0/EKJrEYVmSF5GmwFTgLmNMIfB3oBvQD8gHnqlpP2PMJGNMnjEmLycn5lOKqFq82EBPbWuidTs5XH/99f4HtZ2tiiDGMOkvf4ldUAnAkiQvIi78/wRvGWM+ADDGbDfGeI0xPmAycIIVx1LRC3WkwZgxY2yOJPFp3U4uw3//e/+DWhK9cTjY5/Hw+XPPxS6oOLNidI0ALwPLjDHjg55vH7TZRcDiaI+lrDNq1KiQtnv88cdtjiRxad1OPl26d+eIzp1r30CEskaN2LZgAZs2bYpdYHFkRUv+ZOAa4IxqQ8qeEpFFIvIrMAi424JjKYuICH379q13u/Lycl5++eUYRJSQtG4noWuuv97fB+/z1fi6Ny2NfS1b8srkyezfvz/G0cWehDLaIlby8vKMTscaW2PHjg1pxI3L5eKBBx6IQUT2EpFfjDF5sT6u1u3Ye+L++ynLzDx0SCXQZts2drRtC8Zw86230r59+xpKSB511Wud1qCBC7Xbxu12M2bMmJA+EJRKBGeecQY5O3Yg1Vr04vWyMzu78maplyZO5D+TJsUpSvtpkldcdtllIW87duxYGyNRyjp5Q4eSVlpKq127cJWX4yorA2MwDge+tMCMLiL4nE4WbN7MZ++9F9+AbaJJXtGrVy9OPvnkkLcfM2YMpaWlNkaklDVufuUVdrVsSdbu3XgdgXRXwx2xxuFg5Tff8Nnzz8c+SJtpklcAnHnmmWRmZoa8/ZNPPomvlgtbSiWSh8eOZUfr1v7Wex03A+7NzuanPXt46cknYxid/TTJq0r33XdfWNuPGzdOE71KeA6HgzvurmcAVMVkZsCWkhLeqpgHJwVokldVnHrqqWFtP27cOH744QebolHKGlmtW1fOSFmnQKJfvXMnY8aMwe12xyA6e2mSV1WcccYZYe/zxRdfsGjRIhuiUco6AwcODG1DEXA4wBgee+yxpB9RpkleHaJZs2Zh7/PBBx/oXPQqoQ0cONDfkg81aQda9ZMnT7YxKvtpkleHuOWWWyLa79FHH2X27NkWR6OUNUSE444/PrQkH9S1k795M/++7DKKt22zOUJ7aJJXh8jMzAxr7HywOXPmMGbMGDZv3mxxVEpF77e//S1ZLVtWTfR1zFoJgMPBkt69Gf/iizyRhJP2aZJXNerVq1fEiR7g5Zdf1jnpVUK64+67aeXzHey6CeVirAhep5MyYxgzenRS9dNrkle16tWrV+gXq2rw4osvJtU/g2o4bh87lszANaSsXbtC2ylo3diJjz1mY3TW0iSv6nT66adzyimnRLx/KoxOUKnpvv/7PxAha98+JJxBAyLsdLtZPWuWfcFZSJO8qtfgwYMj3tfj8TB27FheeuklCyNSyhojRoxgbbdu5OzciXi9oY+8Ad76+mvGPPIIv/76q40RRk+TvArJH//4x6j237JlS8rcXKJSR+vWrcnOyWFHu3aYwNj4kDmdAEybNo2PP/7Ypgijp0lehaRNmzaWlPP000/reHqVUK666ir/g4qboOpjzMGRN4Ghlgvmz2fu3Ln2BRkFTfIqZJdeemnUZbjdbv72t79ZEI1S1sjKyiKtYurhUNQwiyXArBkz+N///mdhZNawYo3XTiIyW0SWicgSEbkz8HwrEZkhIqsC37OiD1fFU+/evenatWvU5RQWFvL8888n/AVZrdsNx4MPPnjok+HUz8Comw8//JBffvnFusAsYEVL3gPca4zpBQwAbhOR3sBIYJYxJheYFfhZJblrr72W66+/nsaNG0dVzt69exk3bhxlZWUWRWYLrdsNyKhRo/jNb36DI5Qum9oYwyf/+Q9T/vlP6wKLkuVrvIrIR8ALga/TjTH5gdXt/2uM6VHXvroOZvL58ssv+f7776Mq45RTTolqBE84olnjVet2w2GMYeKIEezMyalzDvpadq48C7j9zjtp1aqVDRFWFbM1XkWkC3As8CPQ1hiTDxD4bs2VO5VQhgwZQvPmzaMqY+7cuYwZM4ZZs2YlbBeO1u2GRUS4bfx4Wu3YEXq3TcV2FRdwRfjbhAmMGzcurtN8WJbkRaQpMBW4yxhTGMZ+N4vIPBGZV1BQYFU4Koburm9BhhDNnTuXsWPHJtxCJFq3G6iMDIY/9FDVqQ/qSvg1XZAVwef18vLkyXwRpzVkLUnyIuLC/0/wljHmg8DT2wOnsgS+76hpX2PMJGNMnjEmLycnx4pwVByMGjXKsrLGjRtHSUmJZeVFQ+t2w9a8QwcuufTSygQukTRAAsn+h2XLmBiHCc6sGF0jwMvAMmPM+KCXPgaGBx4PBz6K9lgqcYkI9957r2XlPfPMM5aVFSmt2wqgT58+HHbYYQA4fL6w74wFKhN9gTEsW7bMhihrZ0VL/mTgGuAMEVkY+DoXeAI4S0RWAWcFflYprGnTprRo0cKSsrxebyL0z2vdVoB/VBmA1+UCrzeqsj6NcbdNGHcA1MwYMxeo7fJzbIZMqIRx5513Mn78ePbv3x91WSUlJTRp0sSCqCKjdVtVcLlcXHfddbz66quY9PTwW/JBSmM8tYfe8aosVdFtc95550Vd1o8//mhBREpZo3PnzowaNYqWLVtGVY6jrIw9e/ZYE1Qox4vZkVSDkpeXxxVXXBFVGXPmzMEb5amxUlYSEe68806aZ2ZG1poXobxpU6ZPn259cLXQJK9s06NHDzp16hRVGcuWLUuEvnmlqrjrvvv8c9BHWDfXrFlDUVGRxVHVTJO8stXw4cOjuk186tSpjB07lu+++87CqJSKjojw54sv9g+pDCfRV8xgaQzjn3mGJ554ArvvodAkr2zldDp56KGHkHBvDa9mxowZTJ482aKolIpeRv/+3BgYdXOIENeNLSstZeILL1CwbZv1AQZokle2ExFuvvnmqMvZunUrn3/+uQURKWWNDt27c1SfPoe+UFujppa7Yl8eP54DBw5YHyCa5FWMtGvXjrPOOivqcn788UdWr15tQURKWeN3v/sdGRkZkRcgQlnjxrw0ahTl5eXWBRagSV7FzEknnRT1FMUAb731Fu+++64FESlljWuuuabqExHcEbu3aVMef/xxy4dXapJXMXXbbbeRlRX9GhsrVqxg6dKlFkSkVPQ6duzIhRdeiNPprDqhWaiClh6cMGGCpSPKNMmrmMrMzOSOO+6wpOvm/fffp7S01IKolIpe3759eeihh3CIkFXXFMX1JXBjePIJ62bK0CSv4iI7O9uScp588klLylHKKmnp6exp27buxUbqmbK4rKyMn6NcjKeCJnkVF9HeGh7sq6++sqwspaIl9V08DYyoqc+nX3xhydoKmuRVXLRpY91iSt98841lZSkVrW6ZmdEXEvgQeOfNN6MuSpO8Sgn/+9//4h2CUgAMu+EGawoS8Q8XjvIirCZ5FTfnnnuuZWV9+OGHOseNSggZWVmkud1RJ2cAHA5efOyx6IqIPgqlInP88cdbWt4HH3xQ/0ZKxcD9o0dbU5AIO93uqPrmo140REXB54MhQ+CHHyBwS7MPWNq7Nx9dcAGe9PSQLtCAfyKwLl262BerTXJyciyboGnJkiVccskllpSlorNtzRo+eeMNdng8eEQwIqR5PKR5PHTYupU0j4dWu3ezPzOTZb17401LQ4whd8UKmu/dy6ZOncjweHC1asWVTz/tH3+eRNIr/ncrJiSrrrbna7F48WKOOeaYiGLRJB8rn30G9XRPGKA8I4M5Awf6J/QKoxK89tprAIy2qgURIx06dLAsyWt3TewZt5tpEydy4Icf6LphA91WrcI4nbjKy9l6++2YtIMpxpOejic9nfVHHIHP6SS9rIzGxcUYEbL27GH4a6/RqLQUp9eL0+vF63DgczjY/u675I8fz3FRrk+QUMKcsG/NmjXxTfIi8grwW2CHMaZP4LlHgJuAiv/gB4wxn1pxvIRmDKxcCYMHw5YtYe0qwNxTTmF3q1b+tSQj8P7773PppZdGtG88nHvuuZZeNPV6vZa1+rReV+XxeFi3cCGzJ02iIDsbb1paZWvVecQRbOjWjfyOHbngo4947q67MLU0VHyB96c8I4Py9HQArnjnHZoVFlbpP07z+cDno+22bZh778V32WVRTVsdaz179qRgzhx25eSEndSrMIbly5bBRRdFtLtVLfl/AS8Ar1d7/lljzF8tOkbiKi6Go46C9eujLmpJnz4RJ3gg6W71Tw/8k1tl1qxZDBkyxKri/kVDrtfAvk2beG7y5Mpb7gHo0MH/PShxeQKJe2WPHvznt7+lpEmT0BKbCE63m5b79tV6gdBpDG127OCnl15igAWzmcbK0KFD2fjII/x04ols6tzZ/2QEyb5pURGuvXsjbsBYkuSNMXNEpIsVZSWNUaNg3DjLi3U0wOXuXC4XbosWN160aJFlSb4h1mvj8zHxqafYWVJyMCE5HCEnJ4/LxdKjjgrrmA6fz9/qr4PX6aR0w4awyo03EeFAs2Zs7dgx8pa8MRy+YQMbunZl+/btdKj4gA2D3ec+I0TkVxF5RURqnJVKRG4WkXkiMs/uFVKiVlTkb8WI2JLgAY6bP5+0KKYbTUtLvsssVk1xADHrl6+3XkNy1e1tq1fzj5tu4rl772Xv/v3kFBTQddUq/4thJiiHz+dvrITyXhiDx+XCV083jMPn4yirxp/HSGZmJnNPPhlvlN2HS486iv3NmkW88I6dSf7vQDegH5APPFPTRsaYScaYPGNMXk5Ojo3hRMjjgcaN/RW9eXPIz7f1cL/54Qe6rF+Pq7w8otns/vjHP9oUmX1cUXRPVVdeXm7ZWUEtQqrXkPh1e+OiRUy+8UYm/ulPTHr9dbZ37Ehhy5Z4XC4K2rZl4xFHRFRuutvNhRXDWSvqb0VdNgaHx0Oa202a203nDRsQn48PL7wQd1oa3kCyr6j1Bih3uZh11lnkRBhPvDgcDsoqckekRDCBD4n58+dHFkfkR6+bMWa7McZrjPEBk4ET7DqWLcrL/W+OywUxnOnQ4fNx3ocf0rywMOQ5LirceuuttGrVysbo7DFgwADLyjLGsKqiBWqDpK/XwE9ffcWjI0fy6tSpbD3sMAratPEnkmp1zRvBWaGrvJwhn39Or+XLGf7yy2QWFfmHCgP4fIjbTfv8fHqsWEHfBQvwAc7yclbm5jLpxhtZ2LcvK3JzWd6jB5s7dGB5jx58/oc/MHT6dAt+8zix6Oxy4cKFEZ2p2nZuLyLtjTEVzd6LgMV2HctSHo8/sdstKwveeANOOsn/OECAlsAI+yNIGL169bKsLI/HQ0lJiWXlVZe09RqY+803lI8aRd68efQvKWF727bMOOssNnTtWve47Zpeq5ZsMkpKyN65k26rV9Ny2DDK3n6bLr17c58Fo2Gsqx2xd9rQocyaNcuSsjweDz6fL+yLr1YNoXwHOB3IFpHNwGjgdBHph/+Maz1wixXHstW11/oTr9XS0+Hzz2HQIOvLThFHHnkkK1eutKSsvXv3WlJOytRrYMyYMZz0zTcM/PZb0gPdWR23buWqt9/mzWuuOTj6o5pWO3eyu+KaSVBib7x/P42Li2mTnc2w//f/aNy0qe2/QzI65dRTmfXll1DDmVIkSktLyQxzAjSrRtdcWcPTL1tRdkzs2wcWTn3LYYfBxo2WvKkNxZVXXsmYMWMsKWvu3LkMHjw46nKSvl4Ds6dMYc6SJYgxnDp3bmWCr5DudjNo1ixev+66Q1rsaeXltCko8Cf5sjLE5eKkvDzOPP/8WP8aSe3Ka67hnXfeqXr2E2FueOedd7jxxhvD2if5hmJYrVUrsGJNxbVroWvX6MtpwDp37szGjRstKWv58uX07NnTkrKSkc/r5dUbb2RL584gQuaBAzhrGZ7bdseOgz8EElHm/v203bqVvaeeyugRDanz0HqdOnXyPwjcE+ByuymN8ILsljBvsISGPEHZ6tX+P3KkCT4zEz799OCoAU3wUTvllFMsK6shT1b24+TJTLzjDjZ37owJjHEvbty41rHou1u1AmNwlZWRu3w5fZo1Y8TYsVzz9tvcogk+ao0bN6YZ0G/ePIZ99BFHrFkTeWHGsGLFirB2aZgt+TPPhEgvhuTm+qctUJZ7++23LSvL5mGUCclTVMSr993H9pwcTOvW/puYAl0wvrQ0vj/xRE78/vsqXTblaWnMOe00Om7cyFm33srhJyTdYKGEt3HFCjK2b2dx374s69On3nsC6rPw3XfpEcYcVQ0ryZeVweGHw/bt4e+blQXbtvkvoirLTZ061fIyf/3114gndUo2W7du5fXnn6dzURHk5FTODxPcJfDf00/Hk5bGyd9+S3p5OfszM3GVlzP0tNPIuvXWOEWe+l596y2cgfmoPMEj9yr66MPsttkd5r06Dae7Zu1aaNQosgQ/ciTs2qUJ3kaLF1s/EnHatGmWl5mIPv74YyZPmkSH/HyKmjWree4jY8DhYO6pp/L0ffex4JhjcLdsSePZszXB283hqP2u13D75UXY0a4dKz/8MORdGkZLfsgQmDEj/P127fJfmFVJy+fzJdXMheEoKSnhqaeeqmwRHv3rryzr3ZttNWwrxmDwj2dvlJnJcQsXxjLUBstbcbG7pjoYxei7L+fM4cgLLwxp29RP8pH+Id1uSMJ5YFRVRUVFtGjRIt5hWG7z+vW8/OqrVe6KFmDADz+wvmtX3EFnneL1knngAK137iT97LO56qqr4hR1w1NcXOx/EOYiIZVq2k+E3WGMlU/NJk6Ffv0i28/j0QQfQ3ZOKpaqF2BfnTTpkGkvFh19NIdt3szgmTNxlZeTUVpKWnk5bXfsoDwtjVaXXKIJPsZ++OEH/4OKVaKg7mkOjCHN7Sa9rAxXaal/rdgatpEw/mdSO5NFshjF2rX+u9NUzCxfvty2spNxLp/6bFq2DJ/LdTDB+3wgwtpu3fj16KM5dsEC+i5cyNYOHWhSXMzXAweyOyuLYcOGxTfwBqjKpGIV71dtLXqfj3M+/ZQmpaXsbdmSHTk5rOjZE/H5/ENhofIDotfq1SHHkNpJPlzjx+t49zhYsmSJbWWnYn/84p9+qnyck5/PzrZtK8fATx82jPl5eXRftQq3y8XiPn1ovmcPf3700XiF26CVlpaG3E3TZ/Fijps/n1lnnskveXm4Ax/k4vMd7LYJnBHkHH54yDGkbpL3eMLb/rbb4O677YlF1coYY2uST0U7AjOkis/H7tatD7byAvI7dCC/Qwf/DU4lJVz+3HMp+WGX6CrnYqqvPz5wQ2XrnTvZ1aoV844/vspQS+NwIF5v5Y1tAM7zzgs5jtR85z/6KLyZJDMy4IUX7ItH1WpGJKOeQhTuRE6JzuPx8MYbb7A+cGu7Aby1DesNTO87cNgwmjdvHqMIVbB33nnn4A+19aEbQ0ZgFa6VPXqw6sgjqWlLU211rlZhLLSTei35yZMh3HUgLVibVUXm+++/t63sbt262VZ2rHm9XiZOnMie3bvr7dtNKy9HAGnShJNPPjl2QapK27YFDWSteJ+MOdi/HvTelaenI14v+R070nrXLhzGcMgsQ9Xe665hdCunVpK/7TaYODG8fQoKwMLl51ToioqKbC2/e/futpYfK8YYnvy//6N5QQFHb93KvpYt2RiYeKw68XoZ9NVX/Hziidyp/fBxM2XKlCo/H7NwIWfOnEnmgQMU5OQw48wzWZOb61/5KWgk3+Kjj6535IzD4aBRo0Yhx5I6SX7BgvAT/FdfaYKPIzvXPXU6nfTo0cO28mPpuXHjGPb++/RYsaJy3pOiZs341x/+wIFmzSq3c3o8HLV4MQLcOX58nKJVAAcOHKh83HvxYs6bPr1yzqC2O3Zw2fvvM+uMM/jpxBOr7ihSdSK5av35Do+HPv37h7Xea+r0yffvH97277+vi3jEWbt27Wwru3Xr1qSnwDQUmzdvpte333LkypW4PB4yysvJKC8na/duLpk61d8F4PWS5nbTYu9eSps04YRvvol32A1eo0aNKvvhB82eXeM8/gPnzKl/aUAR8HpxeL2kl5WRvWsXJ/XtG1YsqZHkzzgjvO0/+gh+9zt7YlEhC3cZs3AUFhbaVnaseDweXn75ZY6fN++QJOE0hk6bNtGopATjdOJJS2N3ixZc8eabOPVGvrjr2LGj/4ExtNi3r8ZtGpWW4qxvFKAx4HRijKHvwoVc/9JLFK5bF1YsliR5EXlFRHaIyOKg51qJyAwRWRX4nlVXGRF75x2YPTu8fe65x5ZQVHhKbVwg3YpWfFzrNfD888/7h0EGEvz2Nm148+qreWLkSCbccQfz+/c/mCREIC2N9TqIICHsCVqnYnctN+QdyMysf7H0QLeMSUtjWe/e+JxOWvbuHVYsVrXk/wWcU+25kcAsY0wuMCvws7WMgUhu09682fJQVPi+/fZb28o++uijrSjmX8SjXgPr1q1jf+DC9LKePdmenc0rN9zAmu7dKWvUiD2tWjFjyJAqffKAJvkEsW3btsqbl2acdRbuasm83OVi5plnhlVmcZMmfDl4MDnt24e1nyVJ3hgzB9hd7ekLgNcCj18DLrTiWFVcc01k+3XoYG0cKiL/i2TaiRC1tGDN3rjVa2D600/7H4jw9emn899Bg/yJIuiCmyd4aoMAO69zqNB4qnXBrMnN5b3LL2db27a409LY2bo1H114Ib/26xfWpGXZBQUURXDPg52dd22NMfkAxph8EWljaelbtsBbb0W2r974FHeFhYWUl5fbVn5Wlm29KPbWa2DK00+zq83BYksyM1neq1fN09VWMAZxOBr0uraJ4o033jjkuTW5uf4hkxD+jJTGIG43g2fOZMWRR4YdT9wvvIrIzSIyT0TmhTykzueDww6L7IC33QbnnhvZvsoyf/vb32wt/7BI64eFIqnbv/76K8sqht8FJ4J6Ejwi3HrrrWENrVPWKygoqH8x+gjeI4fDQbOioqorS4W6b9h7hG67iLQHCHzfUdNGxphJxpg8Y0xeTk5OaCWHOYSokghcdx306OFfJer44/3zxquY+uKLLw45pbVaRkaGXUWHVK8hsrpduZpVmImgy+GHs337dh5//HEee+wx5s6dG9b+yhp/nzix6rDIaKbRDsxpgzF4nU6mXnIJLSPorrEzyX8MDA88Hg58ZEmpxkCkS8UZA3l5/oW4y8pg3jz/kn65uTB/fnRviAqJMebgHNvJyZ56Dfw9MJomkiXh1q9fzwdTp1JeVoa7vJxZM2fy+MiRbH/6aUoqFq5Qtvrus88wgTmDKhN0NIJmnWxUUsKunBz2H3dc2MVYNYTyHeB7oIeIbBaRG4AngLNEZBVwVuDn6NkxH8nq1XDccf5TYqfTfyes95DZI5QFXkii6yExrdfAjqBhd2GrSAhBX+WNGvHxihWsOPFEnr37bqaPGIG7pEQbMzbZ8NprVecVqraoSyVjKieQq/JcbZxOyjMycLrdtMrLCzsuSy68GmOurOWlwVaUX0WYNwKEzeeDwUFhH3UUzJmja71awBjD7t3VB6tYr3HjxpaUE9N6XcHKPnURtnbsSJPZs/E4HMzLzmbek08C/pupBp50EqcMGaL9+BZY9sYbrAzMRVOfpkVFFDduXDlFRZUVo+rY/4h16xgwYEDYscX9wmvYYr1e55Il/vlt1qyJ7XFT0OowVrOJxh/+8IeYHCdZvHPNNRQ3a1aldekV4avvv+e9cePiHV5K2DBtmr8XoL6zJJ+P/U2b+lf2qhDc6q9pf2Noun8/6aWlEd0lnnxJPswbASxhDHTvDsHTh6qwbYvR3y/kC/iJyOqulOrdBhV9xYHughU+H68+/ri1x2yANrZu7X9QX0ve4ah/pFR1ImSUlrK1YqqEMCXfJBc2rgdar4oPGBF/l86XX1p7ep3iFixYEJPjJHX3g5Wx13T6H2gtdl+1ios//JB9LVowa9Ag/vHHPyLG0GbPHo6/5RYOC3c+qAasrLiY7e3bV/0ghcjey1r2Kc7M5KwI+uMhGVvyicAYmDnT/4n87rvxjiZp7InmwmKIUnHh7ojVlmRE2HLYYaS73TQqK2NLp07sat2abe3bs+TII3lj5kz+17cvXh18EJL106cf7F8HEKHF3r1k7t8ffmE1ddn4fBzIzKTvpZdGFF/yJfkTToh3BFVdeaUu/h0CuxcIqXDllbVdK00CVgy7C1Fp48ZMuewyZpx1FqWNG1feZON1OumwZQsHMjMpy8xkx/TpMYknmVUMCW6+dy+DZs3i6tdf57YXXsAd6SyrtZx91dnNU4fk6q7x+aBp03hHcaj16/1vhM+n3Te1qOlWb6s1atSI7CRdBGbPnj04jMFX8Q8d/L0mkYynD97d6QSfj74LFpD3888s7tOHlbm5DH/tNVoUFiJeL05jyDj/fPZefjktg9crVVVsbNSIzhs2cPVbb+Hw+UjzetnbogW+umaYDP4wr2+Rb+CCiy6KOL7kSvIjRvjHsCeqDh0gPz/eUSQkO1eBAn+Cv//++209hl18Ph8vTpjgP+UP/oev658/ysbEGTNn8psff6ycp/6wTZv47Sef+IsO2s5hDM3eew/fQw/hOOqoqI6ZioqLi/GlpXHRtGlV5vyf379/1RWeqgvl/Qt8kJ9++un069cv4hiTJ8lfey3EoDUYlW3boKQELBqnrUJz8sknc2aY07YmCo/Hw6OPPnrorfBRnJ7Xp9m+fQz44QdcQVNLuGrpf5fA1+pLLuHIeA56SFALFyygeWEhmUHL/YF/ecZ654qvEHxWVu0s7rbbbov67DQ5+uRXrUr8BF9BRyUcwq45zkWEoUOHJm2CB3j3zTcP/mMH3S0pxtBl7VpaBF+sNob00lKaRHJ9I+hDpMuGDXgdDjZ37MjiPn3YVTH8rxZiDPk5OXqnbA1+mjkTT1raIYtvd1u7FldZWWiFVCT1oDKat2hhSYKHZGnJX3BBvCMIXXLPy2KLL774wtLyzj//fFq1akWHDh2Seh1XYwxr1q6tscVunE42HH4453z+OZ+fc46/D10En8PBCT/9xI8DBlCSmVlRUP2n/0Gv78/M5JUbbmBfy5ZgDD6nk9yVK/nd1Kk4qt1ub/C35E/88UfW33YbXSZOjO6XTjH7jIHMTLZ07MhhmzbhDCTqXkuXMvfkk9nRpo3/vYNa3ydXeTlH/+9/rBw4kGEXXkjTpk1p166dZUOBE78lv2QJLFsW7yhUFHbsqHWixrC0bduWBx98kP79+9OlS5ekTvAA77/3Xp1dMsbpZOZZZ1XpVvGkp7Pw2GP5zY8/HtwwzGSwrmtXdmVnU56RQXmjRnhcLlbl5vLdiSdWPX7gC8CIkB+j+xySRUlJCQTem6m/+x17s7IoS0+nLD0dn8PBb374oWq/fE3vk8+H+Hwc8Ze/cO/995Obm0v79u0tvdcj8Vvyxx4b7whUFD744AN81SdjilDr1q1JS5FFqgvz89n48884mzTB63AgxmBqSPju9HSk2t/PnZ7OEWvX8t9IuwYdDqq/I570dOYdfzwnffstDsDtdLK2Wzc+GjaME37+mYFff02TkpLIjpeCysrKeOqppyq7WIqaN+eFESM4fONGmhcWsrVDB3bV1dUSdMOU1+GgbY8etsWa2P8xN9yg870nsaKiIhYtWmRZeUuXLuX111/n2muvtazMePnP3XfTtbycxcccQ+b+/ZQ0aUJtPd7ByV+8XrqtXMm2NhEsSFXPBd2iZs1Y2K8fTq+XJX36sCo3FxwOvj79dDZ37MjZOn9TpY8//tj/twweC+9wsKFLl9AKCGqpe10uXnzxRa677jo6d+5sbaAkenfNK6/EO4LIaIsHgClTplhe5rp169ic5Auxb9++nWMXLmTx0UeDCAeaNvWPj6+H+Hw4vV5W9O7Np+eff+gG9V0YFaFJSQmOWhpOPoeDL84+mw8vvphVPXoc/DAQYU1uLm67Z4BNIkuXLvU/CFwriUrgwuu//vUvjA0XtxM3ycdgSlrbaJIHsC0ZV66elKRmz57Nkj59qt7w5HDUnqQDzxuHA4/LRVmjRofMk+IKcb3c9LIyf39vLRNhedLTa01a6444IqRjNBgWTwttfD42bdpkXZkBiZvkCwvjHUHkmjWLdwQpLRZz0tupuLiYdV27HjraIjj5Bg+pq+vinQgOr5chn3+Oo9qSitV/Tisvp6h5c//47VoSlLOO7lHtk/cLp7Xt8Hg46ZtvGDFhAle9+SaOeuYDWmbDIJPE7ZNP5gtsESy2qxoO5759uLxeSmrqGw9eWShERoQ+ixaxtls3lvXuXblv5w0b2Bzo43W53RyxZg3Le/WqtRxXeTlnf/45vZctI83rZVVuLl+cfTaFLVqQ5vGQncz/kxYqKysLbdiqz8dVb79Np40bSfd4aL17Nz2XLWNljx61Lshtxz0lifuuJfL0BXX5/e/jHUFCsHvueJ/Ph8OmO0Lt1varrzCtW1NY1wI4Ph+Ni4v9Y+HrSSYOrxeHMZw3fTrLe/WqHLaX4XZz1/jx+NLSyNy/n7mnnMLS3r1rLsQYrn7zTTpu3kxaYDRPz+XL6bxxI/+45RbO/c9/6KR3vALw97//vf6NjOGwzZvptGkT6UFnVL+bOpV5/fvz1eDBlDZpUrlt45ISSho1YteuXZbHa/t/iYisF5FFIrJQROaFvOPOnTZGZZPjjkueO3NtNmnSJFvL/zF4nHgcRFyvgcyiIgZ+/XWdF0pPnDuXtBCn+h3w3XekezxkFhdz4nffceTy5Vz9+usMmjWL8vR0MkpLcRhD7qpVOGsZzpq9Ywcdtm6tTPDgn7emcXExF73/Pl1fftm2aRaSSUlJCYX79tW9UaCV33HLlkNuLhNjOP6XXzj1m2+CnhTK0tMZNm0a3pISii1eeD1W79ogY0w/Y0zos94PH25jODa55JJ4R5AQysvLbRklEGzGjBm2lh+i8Os1sOeoo+iwdWvtGxhD6927KW3cuN5WfJtt2zht7tzKn8+cOZPLpkyh+9q1tC0ooFlhIV4RVuTmsqt1a5pUm2OlQueNG3HW8KHiNIYumzbR+NRTQ/vlUtwbFQvR1zP5mMPrxYjgrWG64XKXi70tW1Z5zud0MmfQIC7+97/5zOKBBYn70VzPfBoJae3aeEeQECZMmGD7MYwxSTuUcsDDD5Pfvj2u2i5yGsOX55yDu55rO4evW8eNL71UZfZDgSqt9TRjKGvUiClXXMHUSy+lsFpyqThe3wULcNTwwVwxrYHy17kdIV709zkctNq5E09a2iHDY40Ii445puoOIuzNyuKTYcNoZPG0zrFI8gb4UkR+EZGbq78oIjeLyDwRmXfIdLQjR8YgPAvdcUe8I0gIB2ppLVrNjpEIYaizXkPtdbtNu3b8cOKJ5OzYUeMqQIhQHjxMMrjMoNb2WTNmVJnyoDZNDxw4pNsgWJrbzZ467s70ajcN4K9v3oyMKu+LeL21jpj5/Nxzeefyy9nWrh0epxOP00mZy8Ub11zjP0urTgSf01nr2VakYnHh9WRjzFYRaQPMEJHlxpg5FS8aYyYBkwDy8vKq1vjHH4cnnohBiBZwOuHoo+MdRdyVhzhe2wptIrnr0zp11muou24PuuEGPg5usYXQvSVeb5W7X3NCnKM/zeul+8qVLK9lPnjjcOCpYeRMRUTF48ZRxyXiBuPzwHz7wc757DNmDx5MWUZG5XuT5nbjdLspa9KELZ07M/nmm8ncvx9XeTn7srJqvbMZwJOWRmHz5pbGbftHtDFma+D7DmAaEN76faNH2xCVDSy+WJKsYjmGvW/fvjE7VnXR1us2gwfTrLycFvv2gcfjbw1W3BRVU3+vMf5RM0Gv7atldE71JFLmcrGvjsThdTo5olpXowF8IhSeeSYtHngg1F8rpZVUu0+gyf799F+wgJsmTeLIFStILysjc/9+Tpo7l/MqPhAC79mBZs3Y27q1/4OgjjMjp9eLx+KbzmxtyYtIJuAwxhQFHg8BxoZVyAMPwPjxEKM1QsOWlqbz6wRpUdewQAs1i+MNZ1bUaxHh1Mcfx33yyaw48kjmnXDCIZOGHaJacvjvoEFcMG1alSF64O9DN/iT97ouXZhy+eX+O1mrnS00Kywko6wMfD4MsLlDB9rn5+NJS2NXq1YcuO8+cu+7L5xfK6W1T09nU9Ac8S327cOTlkarPXu44r33AP9F1YI2bSjNyKi9oFrG2Ke53TQtLOSi556zNG67u2vaAtMC02amAW8bYz4Pq4T0dCgogFatErO17PHAY4/5P4wUjWO0KlanTp1icpxaRF+vgY6HHcbqjz7C+5e/1HjRs4oaksLSo46i57Jl9Fm8+JCLowb4OS+PL885p+oNVsaQ5nZz8z//Sas9e/A4nf7Vn4wha+9evA4HaW437bZvZ/U//wnnnQd13EDVkPz2mGP4+88/V/5ckJPDTyecQM/ly2lTUMD3AwYw+4wzcPh89V40d3i9+ILmmXd6PDg9Hva0bo0j0gXAa2FrkjfGrAWiP6fOyIC9e6F5cygtjbo4yz34IPzud3DkkfGOJCFkZmbafvH1pJNOsrX8ulhWr4Hu/frR+KWX+OWf/6zpQPUOoVzXtSs9Vqwg3e2mLCOD+ccey4YuXWi5ezebO3WqcRoEMYadOTnk7NpVORKnYgRI8Dj53NWr8fTvT5pOZwBA09NOg59/rjwj8qSn899Bg/jmtNPovmoVa7p3xx3CGgfi89Fv3jzm/+Y3gScEr8uF1+XyX3i3WPJcNne5YMOGeEdRu7ywhkqnNCsXPKhNhw4dbD9GrHTs2JEBp5zib8253WDMIfPO1KZimoL9mZm8eNttzD7jDFb07MnPJ5zAlsMOq3EfE5j5MpgYc8jZgADO0lK8K1eG+yulpEYVdx8HXRupmDRuec+eISX4ig/u/x13XI2v9Q11quIwJE+SB2jTBhK1wiXqNYM42L9/v63lX3bZZTH5IImls4YMoVf37mQWFXH4unV0CXFa35ImTZg5eDCzBw3iQJMmlYnGV8ckZIjQeePGqk/VdYzLLw8pllRXWNekiaEOMxXBOByHLvJtDI1cLs658srIA6xF4s5dU5vcXNi2zT+FwJYt8Y5GxVDXrl05/fTTbVlYIRFcMnw4a089lfyrr8bp8bClQwfKmjSpt8tmfZcuFDVv7k/s9XCVl9NjxQrahLEko7GhCyEZ2TVXktPp5IQTTuCUU06hUaNGlpefXC35Cm3bwubNUDHBTyJIxjt0k8yll16asgm+whFHHEHPwIiZ+8aPZ8B33yF1dd0YQ0GbNjQKGvVR0zbZO3bQYfNmhn76KRd/8EFYMTX58MOwtk9VpRXXA2u6SG6Mf5nGcKbzqFgnwBiGDBlCE5vyWfK15IMdOACnnQbBk/3Eiw2T/Scrp9OJN8TJtcKxfft2utjQZ5loWrdrR8Y33/Diww+TuXcv6R4PZSJVl5oDMIb08nJ8IvgAp8dTtRsgkESydu3i5kmTarw71ieCw5jKsfUV5wyVN0IdfjiZXbta+eslrVatWtX8gjF0X7mSXkuXMv3880M6owIOrh9Q1we0BZKzJR9szhz/FelHHql5DnoRaNTIvjneO3aE8nKI0dDBZHBM9Xk5LPJeYCxyQ9C0aVPufPZZLp8wgdZHHHFw5ajqi4k0a4YnPZ3CwJ2U4vPhdLtxeL00Lilh4H//yyVTpyK1tDB9DgcGKGzalIKsLHz4E7wB9o8bR6YN85snq9oWkf/Nd99xwYcf4vJ6Q5pmogoRBMjPz48+wFokd0u+goj/ztiKu2ON8Sfe4KXM3G745ReYMQP+8Q+oaxbA2qSnw0cfwRln+B+rGmXXtUp9FEpLSzHGpNxF17o0a9aMm266qfJnX6B/PLh/+MCBA+Tn5zN//nxWr1xJ5q5dnPnVV3RbswZESPN4KGzenJZ791YZj+92OnH4fLx15ZV0y83lxCuuqBwTL4Cub3aotLQ0PMGJ3Bh+GjCAH088EafXe+gF1RD4HA7+PWECtz/+uIWRHpQaSb46Ef/Y+mAuFwwY4P96+OGD24Vq1Cj/h4hO1lSvptWG51npnXfe4aqrrrKt/ERX08W/zMxMunfvTvfu3QHYsG4d87Zu5fuTTqLpgQNs6diRNI+H6155hfSyMhw+Hw5jcLtcvH/NNZw9dizt2rWL9a+SlNKdzqpJXgQT6EarnMjNGFxuN16HI6SuGyOCu6wMr9eL0+IboSBVk3yoli2r+26+li3hyy/h+ONjFlIqsPPi6KpVq2wrO1Uc3rUrW4YMYc+8eaw88khM4AaoSTfdRKeNG8ksKcF1/PGc8cADDNelKsPirbi4WlcD0ecja+dOCkL94HQ42N+0Ka8/8ADXPfmkNYEGadhJvmdP/xt2773w7LP+N+6qq+D1161dib2BsXv+mhkzZnDWWWfZeoxkd9Ktt3KC282nd99N4d697G3blmF33EHnww+Pd2hJrXFmJmX1zbTqdLKjXbuwc8gR331nS3dkw07yFZ55xv+lLGF3n/n333+vST4EaS4XwypWMlKWGDhwIB9PnYqpqxvGmPC6dQNnBi327WPZsmX0rm0d3ghpB7NKOnYvLahUbbZt20arXbtqHw8fWPQlLCKIz8f8445j7969UcdYnSZ5lXRycnLiHYJqoETEv4RiLXP+Z0Q4gaIAmzt1ol+/ftGEVyNN8irpDE/GRd5VSujSpUuNq2hV8NXWTRPc8q/hLMDrdOLMyLDlrldN8iqpDB06lMzMzHiHoRqorl27+ld3qqW7xl3T3DM+H+22bvV35VRMf+D1klZxATfQJ3/vvffaErNeeFW2yM7OZufOnZaWOWLECFrrHEEqjtLT00lzu2tcaavWvniHg+3t21dejK0YV+9xOGhaWEiT5s255eGHbZsATVvyyhY9e/a0tLz09HRN8CohZO/dW3VqiWpr79bE1JDAxefjQGYmV915p20JHjTJK5scffTRlpZ38sknW1qeUpHa3bGj/0GUQ4WN04lxOmnRsmX0QdXB9iQvIueIyAoRWS0iI+0+nkoMVg9ztPrMIFparxsujw0zrNrJ1iQvIk7gRWAo0Bu4UkSsHemvElJtM/ZFqqXNrZ1waL1u2Cxb2MMYmtqwSEh1drfkTwBWG2PWGmPKgXeBC2w+pkoAVvefL1q0yNLyoqT1ugG7+OKLD84FHw0R9peU2H5zn91JviMQvJrG5sBzlUTkZhGZJyLzCgoKbA5HxdLlFq4N+u2331pWlgXqrdegdTtVdevWzdKZVnfv3m1ZWTWxO8nXdGWiyseWMWaSMSbPGJOndzKmlp49e5JRfcrnCO3Zs4exY8daUpYF6q3XoHU7ld1zzz3hL/dXXWDfF154gf/85z8WRXYou5P8ZqBT0M+HARGs1qGS1ciR1l2TNMbw0ksvWVZeFLReN3AiQv8ePfw/BK/WFV4hlSN05s+fj9vttjDCg+xO8j8DuSLSVUTSgSuAj20+pkow/fv3t6ysLVu2WFZWFLReK3579dXkbN9e+bN4PHUn+3o+CF5//XWrQqvC1iRvjPEAI4AvgGXAFGPMEjuPqRLP+eefT9++feMdhmW0XqsKf5w4kWYeD/h8mPrGzdfz+vagDwwr2T6tgTHmU+BTu4+jEtuFF17I+eefz5dffslPP/0U73CipvVagb/b5p7/+z/27NnDq6++SlFRUcRl9ajo/rGY3vGqYsbpdDJ06FAGDBgQcRndunWzMCKlrJGVlcXtt98e1Rj6iy++2MKIDtIkr2Lu7LPPZtCgQRHtO2zYMIujUcoaLpeL++67j2bNmvmfqOiDD/HCrF0rqmmSV3Fx2mmncccdd4S9X3p6ug3RKGUNp9PJPffcw9FHH01mURHtt2yhXX5+lWmGHV4v6aWlON1uMAaHx0Pjxo1ti0mTvIqbrKws7rrrrrBm4LNq3L1Sdrr44os5/corabt9u3+REYcDRDAOBz6nk/KMDHxOJ2nl5Ry9aBEdbWy8aJJXcdWiRQsuu+yykLefM2eOjdEoZZ28vDyWn3QSO3NycFSf1CyQ8D0ZGfyvXz+KFy2ybXoDTfIq7sIZVfDf//7XvkCUsljFUpW+wEIhNRJha6dOfDNzpi0xaJJXCeGWW26JdwhKWa5du3a0atUKVwh3sy795BNbYtAkrxJCu3btLJ+eWKlEcNuIETTZv5+0+hK9z2fL8TXJq4Rx++23xzsEpSzncDjot2kT/ebPp/mePYcmc2NwlZfTwabFSDTJq4TRvHnzkEbP2D01q1JWO+3NNyl3OGi1Zw/dVq/2X4itGD8vgjs9na0ejy3H1iSvEkoos1a+9tprMYhEKes4mjRhyMUXs6V9e9bk5vovxAYvAC7C9vbtbWnAaJJXCae+1nxhYWGMIlHKOpkDB+Koq99dhP+8/77lx9UkrxJO27Zt691m3rx5MYhEKQt16wb1dMms32r9sgSa5FXCOfvss+vdZvr06TGIRClrlbVoUfuLge6b+RbfC6JJXiWcDh06xDsEpWxx9dVXVz4Wn49+CxZw/UsvceOkSRz/4484vF4+//JLS4+pA5NVQmrWrFm9c3N7PB4dW6+SSvfu3Wmxcyf7srO55N//JnfVKtID4+dzCgrovXQpr117raXH1Ja8Skj33HNPvdusXLkyBpEoZa1+GRn0WbCgSoIHSHe7aZ+fz3E//4zHwuGUtiV5EXlERLaIyMLA17l2HUs1TO/bMBKhPlqvVbROfvxxLvjkk0MnLQMyysvpkJ/Pj2+9Zdnx7G7JP2uM6Rf40qXSVFhCWWXHrhXu66H1WkXMlZ7OvubN8dUwxbY7LY0dbdvimjDBsuNpd41KWPfff3+92+zatSsGkShlrc//9KcaZ6Y0Iiw65hg2depk2bHsTvIjRORXEXlFRLJq2kBEbhaReSIyr6CgwOZwVDIJZTGROE09XG+9Bq3bqnZXjB3LG1dfzd4WLSh3uShLT2d/ZiZvXX01xZmZrOjVC4qLLTlWVEMTRGQm0K6Glx4E/g6MA0zg+zPA9dU3NMZMAiYB5OXl2TNrvkpaLperzi6ZFStWWH5MK+o1aN1WtXM6nezKzub5O+8kp6AAh8/H9rZt/StIAe6MDH55+mmOGz066mNFleSNMWeGsp2ITAbsmSxZpbSRI0cybty4OrdZsGABxx57rGXH1HqtYqEsLQ1EKKjlDu9vtm2jvzFRL/Bt5+ia9kE/XgQstutYqmH7+OOPY3YsrdfKKucMG1bn6weysliyZEnUx7HzTpKnRKQf/tPa9YAu/aPCtnPnzniHUJ3Wa2WJ+fPn1/6iCJ70dH7+8kv69OkT1XFsS/LGmGvsKls1HD6bVsuJlNZrZZU9IYwM252fH/VxdAilSmjZ2dkhbWfXSvdK2aVJXYt7BxSHsE19NMmrhBbq3DTfffedzZEoZa3L65ujRgRfCDcE1keTvEp4eXl59W4zc+bMGESilHXad+yIq7zcvwRgHdb/+mtUx9EkrxLeeeedF+8QlLLF/Y88gsPtrjPRfz9tWlTH0CSvUob2y6uks3Ytvnq6JPP374/qEJrkVcLbtGlTSNtt3LjR5kiUstZrL75YdUHvGriNiaoBo0leJbxQ+9tnz55tcyRKWWtry5Z1JniARqWllJSURHwMTfIq4R04cCCk7TZs2GBzJEpZy4QwCV/LoiIcUUxtoEleJbycnJx4h6BUfIiw8fDDWTJpUsRFaJJXCa9Xr17xDkEpezgc9Q6hBNi1bVvkh4h4T6Vi5Jdffol3CErZI4RpO7J27yZ3+PCID6FL3auEF+romuuvr3Fad6USV10ja4xBjKHzunV07dkz4kNoS14lvFBWiOrRowedLFwyTamYqaO7pvPq1Zz33ntRFa9JXiU0YwzeGla1r+60006LQTRKWadyNFgdI2f2ZmfjbNo0quNod41KWD6fr95VoSq0b9++/o2UShAb1q/nX6+8ggPwORy1JvpoV4UCbcmrBBZqggdr/hmUioV9+/Yx7dlnAfA5nXW25If96U9RH0+TvEpIP/30U8jb/vnPf7YxEqWs9eb/+3/sa9kS6por3hgyCwvpeuSRUR9Pk7xKOB6Ph88++yykbc844wwaWTDntlKxMOeVVyht2hRHXWPjjcFVUsK9zzxjyTGjSvIicqmILBERn4jkVXvtLyKyWkRWiMjZ0YWpGgqfz8ejjz4a0rYZGRmceuqptsShdVtZ7cMnn6T9Y49R2rixv5umJoHk/8c//tGyLshoL7wuBi4G/hn8pIj0Bq4AjgI6ADNF5EhjTP3DJFSDtW3bNv75z3/Wv2HAyJEjbYxG67ayzru3307jdeuY378/UlsrPvD80SJkdeli2bGjSvLGmGVQ40WvC4B3jTFlwDoRWQ2cAHwfzfFU6nr99ddZt25dyNs/8MADNkajdVtZ55Xhw9nUtStpLVrgcblqHhfv84EI7Xbv5uIJEyw9vl1DKDsCPwT9vDnw3CFE5GbgZoDOnTvbFI5KZGPGjAlr+5tuugmXy2VTNPXSuq1CsvTXX/nwzTdxd+0KIv4ED/7RNMZUjqpxBO4Dab1tG7eEcSYbqnqTvIjMBNrV8NKDxpiPatuthudqPEcxxkwCJgHk5eXp0j4NyHfffceMGTPC2ufBBx8MeXHv+mjdVnZ5bvRo9olAkyY1D5EUIc3tpklxMZ02bmRLly78yYYEDyEkeWPMmRGUuxkIvsf8MGBrBOWoFPXYY4/hdrvD2ufhhx8OaYqDUGndVlbzlJXx6GOPHUzsdVw8zS4o4PdvvMGsIUO4869/tS0mu7prPgbeFpHx+C9O5QKhD3xWKSvcrpkKo0ePtjiSiGndVocoKixk/F//6p86uJ7l/ACcbjfZ27fz2s0386fnnrM1tqiSvIhcBPwNyAGmi8hCY8zZxpglIjIFWAp4gNt09EHDNm/ePKZPnx72fiLCqFGjbIio3uNq3VYhmTBhAnt27z6Y4OtjDGkeD33uvptLBg60Pb5oR9dMA6bV8tqjQGgDnlXKKi0t5cknn4xo365du3LttddaHFFotG6r+syaNYu5c+ceHC1TX4I3Boyh9c6d3PjkkzSKcuKxUOkEZco2kfS7V7j66qvp3r27xREpFb3SkhKefOyxg9MShNh6B8DnY8SLL9oXXA00ySvLffLJJxGv5pSens5f/vIXiyNSyhqfnnceP+fl+RN8OMkduPHSS+l41FE2RlczTfLKUtEk+GuvvZauXbtaHJFS1pjfty8/X3SR/4cwEnxaWhoPPvSQjZHVTZO8slQkCb5v375ceOGF1gejlEXKlyzh68GDw2u9G8PoUaPqnm0yBjTJq7hKoKGRStVq3x/+QOF559W9UVDXzDE9enDRlVfaHFVoNMmruBg5ciQZGRnxDkOpkEjbtqSXl1NeW50NTFPQuXNnrrvuutgGVw9N8iqmBg0apOuxqqTTesoUThw6lK8HDaraZRPUerf6jmyrJF5EKqndeeedNT7vdDoZPXq0JniVlKRJE45q25Zjf/kF8fkqx7w3LSzkqiFDGP3IIwmZ4EFb8spiLVu2ZNSoUbz55pusXbuW9PR07rrrLho3bhzv0JSKSs6UKZyXn89xgwdT7PHgGjiQLpMnxzusemmSV5YTEa655pp4h6GU5Zzt29Nx6dJ4hxGWxDy/UEopZQlN8koplcI0ySulVArTJK+UUilMk7xSSqUwMTWtHB4nIlIAbIiymGxgpwXhRCtR4oDEiSUR4jjcGJMT64Nq3bZFosQB8Y+l1nqdUEneCiIyzxiTp3EclCixJEocySpR/n4ax6ESKZbqtLtGKaVSmCZ5pZRKYamY5CfFO4CARIkDEieWRIkjWSXK30/jOFQixVJFyvXJK6WUOigVW/JKKaUCUjLJi8gjIrJFRBYGvs6N8fHPEZEVIrJaREbG8tjV4lgvIosCf4N5MT72KyKyQ0QWBz3XSkRmiMiqwPesWMaUCuJZtxOlXgdiiUvdTsZ6nZJJPuBZY0y/wNensTqoiDiBF4GhQG/gShHpHavj12BQ4G8Q6+Fd/wLOqfbcSGCWMSYXmBX4WYUv5nU7Aes1xKdu/4skq9epnOTj5QRgtTFmrTGmHHgXuCDOMcWcMWYOsLva0xcArwUevwZcGMuYVFS0XpOc9TqVk/wIEfk1cHoVy9OnjsCmoJ83B56LBwN8KSK/iMjNcYohWFtjTD5A4HubOMeTrOJRtxOpXkNi1e2ErtdJm+RFZKaILK7h6wLg70A3oB+QDzwTy9BqeC5eQ5hONsb0x3+KfZuI6Np7SSBB63Yi1WvQuh2ypF0ZyhhzZijbichk4BObwwm2GegU9PNhwNYYHr+SMWZr4PsOEZmG/5R7TjxiCdguIu2NMfki0h7YEcdYElaC1u2EqdeQcHU7oet10rbk6xL4Q1e4CFhc27Y2+BnIFZGuIpIOXAF8HMPjAyAimSLSrOIxMITY/h1q8jEwPPB4OPBRHGNJSnGs2wlRryEh63ZC1+ukbcnX4ykR6Yf/dHI9cEusDmyM8YjICOALwAm8YoxZEqvjB2kLTBMR8L/PbxtjPo/VwUXkHeB0IFtENgOjgSeAKSJyA7ARuDRW8aSQuNTtBKrXEMe6nYz1Wu94VUqpFJaS3TVKKaX8NMkrpVQK0ySvlFIpTJO8UkqlME3ySimVwjTJK6VUCtMkr5RSKUyTvFJKpbD/D+FZD08jywYIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.565101706370547\n",
      "0.0 0.5681471744596736\n",
      "Iter 5 : Acc 0.0000 , nmi 0.0000 , ari 0.0000 , Test F1 0.000, Test AUC 0.568\n",
      "Epoch: 05 | Batch: 010 | Loss: 4.590 | Rec-Loss: 0.894 | Dist-Loss: 2.884 | Classification-Loss: 0.996\n",
      "Epoch: 05 | Batch: 020 | Loss: 5.454 | Rec-Loss: 0.904 | Dist-Loss: 3.652 | Classification-Loss: 0.939\n",
      "Epoch: 05 | Batch: 030 | Loss: 7.281 | Rec-Loss: 0.878 | Dist-Loss: 5.743 | Classification-Loss: 0.748\n",
      "Epoch: 05 Loss: 334.745 | Rec-Loss: 33.671 | Dist-Loss: 262.271 | Classification-Loss: 30.760\n",
      "Epoch: 06 | NMI: 0.027 | ARI: 0.078\n",
      "Epoch: 06 | NMI: 0.027 | ARI: 0.078\n",
      "Epoch: 06 | Batch: 010 | Loss: 7.531 | Rec-Loss: 0.929 | Dist-Loss: 5.846 | Classification-Loss: 0.896\n",
      "Epoch: 06 | Batch: 020 | Loss: 5.036 | Rec-Loss: 0.889 | Dist-Loss: 3.230 | Classification-Loss: 0.982\n",
      "Epoch: 06 | Batch: 030 | Loss: 4.612 | Rec-Loss: 0.943 | Dist-Loss: 2.963 | Classification-Loss: 0.927\n",
      "Epoch: 06 Loss: 208.140 | Rec-Loss: 32.424 | Dist-Loss: 144.717 | Classification-Loss: 31.294\n",
      "Epoch: 07 | NMI: 0.033 | ARI: 0.097\n",
      "Epoch: 07 | NMI: 0.033 | ARI: 0.097\n",
      "Epoch: 07 | Batch: 010 | Loss: 4.936 | Rec-Loss: 0.734 | Dist-Loss: 3.407 | Classification-Loss: 0.835\n",
      "Epoch: 07 | Batch: 020 | Loss: 4.920 | Rec-Loss: 0.898 | Dist-Loss: 3.094 | Classification-Loss: 1.004\n",
      "Epoch: 07 | Batch: 030 | Loss: 3.975 | Rec-Loss: 0.864 | Dist-Loss: 2.284 | Classification-Loss: 1.002\n",
      "Epoch: 07 Loss: 168.525 | Rec-Loss: 31.460 | Dist-Loss: 110.435 | Classification-Loss: 32.229\n",
      "Epoch: 08 | NMI: 0.052 | ARI: 0.142\n",
      "Epoch: 08 | NMI: 0.052 | ARI: 0.142\n",
      "Epoch: 08 | Batch: 010 | Loss: 3.691 | Rec-Loss: 0.935 | Dist-Loss: 2.039 | Classification-Loss: 0.883\n",
      "Epoch: 08 | Batch: 020 | Loss: 4.630 | Rec-Loss: 0.846 | Dist-Loss: 2.976 | Classification-Loss: 0.870\n",
      "Epoch: 08 | Batch: 030 | Loss: 3.203 | Rec-Loss: 0.829 | Dist-Loss: 1.730 | Classification-Loss: 0.827\n",
      "Epoch: 08 Loss: 164.959 | Rec-Loss: 31.094 | Dist-Loss: 104.976 | Classification-Loss: 32.230\n",
      "Epoch: 09 | NMI: 0.057 | ARI: 0.156\n",
      "Epoch: 09 | NMI: 0.057 | ARI: 0.156\n",
      "Epoch: 09 | Batch: 010 | Loss: 3.409 | Rec-Loss: 0.833 | Dist-Loss: 1.965 | Classification-Loss: 0.806\n",
      "Epoch: 09 | Batch: 020 | Loss: 3.377 | Rec-Loss: 0.869 | Dist-Loss: 1.951 | Classification-Loss: 0.745\n",
      "Epoch: 09 | Batch: 030 | Loss: 2.451 | Rec-Loss: 0.773 | Dist-Loss: 1.062 | Classification-Loss: 0.934\n",
      "Epoch: 09 Loss: 153.789 | Rec-Loss: 30.882 | Dist-Loss: 95.127 | Classification-Loss: 32.541\n",
      "Epoch: 10 | NMI: 0.056 | ARI: 0.166\n",
      "Epoch: 10 | NMI: 0.056 | ARI: 0.166\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.6129421265679278\n",
      "0.0 0.6107623799072688\n",
      "Iter 10 : Acc 0.0000 , nmi 0.0000 , ari 0.0000 , Test F1 0.000, Test AUC 0.611\n",
      "Epoch: 10 | Batch: 010 | Loss: 5.398 | Rec-Loss: 0.783 | Dist-Loss: 3.425 | Classification-Loss: 0.915\n",
      "Epoch: 10 | Batch: 020 | Loss: 3.906 | Rec-Loss: 0.854 | Dist-Loss: 2.121 | Classification-Loss: 0.994\n",
      "Epoch: 10 | Batch: 030 | Loss: 3.698 | Rec-Loss: 0.935 | Dist-Loss: 2.142 | Classification-Loss: 0.947\n",
      "Epoch: 10 Loss: 135.058 | Rec-Loss: 30.411 | Dist-Loss: 76.927 | Classification-Loss: 32.643\n",
      "Epoch: 11 | NMI: 0.062 | ARI: 0.180\n",
      "Epoch: 11 | NMI: 0.062 | ARI: 0.180\n",
      "Epoch: 11 | Batch: 010 | Loss: 3.186 | Rec-Loss: 0.977 | Dist-Loss: 1.342 | Classification-Loss: 0.906\n",
      "Epoch: 11 | Batch: 020 | Loss: 2.872 | Rec-Loss: 0.787 | Dist-Loss: 1.294 | Classification-Loss: 0.932\n",
      "Epoch: 11 | Batch: 030 | Loss: 3.799 | Rec-Loss: 0.768 | Dist-Loss: 2.267 | Classification-Loss: 0.836\n",
      "Epoch: 11 Loss: 122.526 | Rec-Loss: 29.995 | Dist-Loss: 64.375 | Classification-Loss: 33.082\n",
      "Epoch: 12 | NMI: 0.070 | ARI: 0.192\n",
      "Epoch: 12 | NMI: 0.070 | ARI: 0.192\n",
      "Epoch: 12 | Batch: 010 | Loss: 3.576 | Rec-Loss: 0.834 | Dist-Loss: 2.051 | Classification-Loss: 0.890\n",
      "Epoch: 12 | Batch: 020 | Loss: 4.897 | Rec-Loss: 0.839 | Dist-Loss: 3.025 | Classification-Loss: 0.973\n",
      "Epoch: 12 | Batch: 030 | Loss: 2.564 | Rec-Loss: 0.925 | Dist-Loss: 0.997 | Classification-Loss: 0.941\n",
      "Epoch: 12 Loss: 157.556 | Rec-Loss: 30.181 | Dist-Loss: 95.659 | Classification-Loss: 33.658\n",
      "Epoch: 13 | NMI: 0.069 | ARI: 0.199\n",
      "Epoch: 13 | NMI: 0.069 | ARI: 0.199\n",
      "Epoch: 13 | Batch: 010 | Loss: 4.628 | Rec-Loss: 0.889 | Dist-Loss: 2.618 | Classification-Loss: 1.027\n",
      "Epoch: 13 | Batch: 020 | Loss: 2.974 | Rec-Loss: 0.954 | Dist-Loss: 1.175 | Classification-Loss: 1.083\n",
      "Epoch: 13 | Batch: 030 | Loss: 1.966 | Rec-Loss: 0.818 | Dist-Loss: 0.439 | Classification-Loss: 1.103\n",
      "Epoch: 13 Loss: 125.760 | Rec-Loss: 30.234 | Dist-Loss: 66.230 | Classification-Loss: 33.254\n",
      "Epoch: 14 | NMI: 0.066 | ARI: 0.192\n",
      "Epoch: 14 | NMI: 0.066 | ARI: 0.192\n",
      "Epoch: 14 | Batch: 010 | Loss: 3.286 | Rec-Loss: 0.821 | Dist-Loss: 1.690 | Classification-Loss: 1.027\n",
      "Epoch: 14 | Batch: 020 | Loss: 3.186 | Rec-Loss: 0.759 | Dist-Loss: 1.742 | Classification-Loss: 0.808\n",
      "Epoch: 14 | Batch: 030 | Loss: 2.547 | Rec-Loss: 0.874 | Dist-Loss: 0.901 | Classification-Loss: 0.982\n",
      "Epoch: 14 Loss: 123.896 | Rec-Loss: 29.696 | Dist-Loss: 63.015 | Classification-Loss: 33.397\n",
      "Epoch: 15 | NMI: 0.071 | ARI: 0.203\n",
      "Epoch: 15 | NMI: 0.071 | ARI: 0.203\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.6948440974695136\n",
      "0.0 0.6919389798113399\n",
      "Iter 15 : Acc 0.0000 , nmi 0.0000 , ari 0.0000 , Test F1 0.000, Test AUC 0.692\n",
      "Epoch: 15 | Batch: 010 | Loss: 5.492 | Rec-Loss: 0.749 | Dist-Loss: 3.463 | Classification-Loss: 0.883\n",
      "Epoch: 15 | Batch: 020 | Loss: 2.546 | Rec-Loss: 0.725 | Dist-Loss: 1.141 | Classification-Loss: 0.816\n",
      "Epoch: 15 | Batch: 030 | Loss: 2.750 | Rec-Loss: 0.809 | Dist-Loss: 1.285 | Classification-Loss: 0.902\n",
      "Epoch: 15 Loss: 118.367 | Rec-Loss: 29.347 | Dist-Loss: 59.035 | Classification-Loss: 33.451\n",
      "Epoch: 16 | NMI: 0.069 | ARI: 0.203\n",
      "Epoch: 16 | NMI: 0.069 | ARI: 0.203\n",
      "Epoch: 16 | Batch: 010 | Loss: 2.846 | Rec-Loss: 0.799 | Dist-Loss: 1.048 | Classification-Loss: 1.047\n",
      "Epoch: 16 | Batch: 020 | Loss: 2.380 | Rec-Loss: 0.740 | Dist-Loss: 0.920 | Classification-Loss: 0.863\n",
      "Epoch: 16 | Batch: 030 | Loss: 2.922 | Rec-Loss: 0.776 | Dist-Loss: 1.437 | Classification-Loss: 0.833\n",
      "Epoch: 16 Loss: 122.553 | Rec-Loss: 29.290 | Dist-Loss: 61.570 | Classification-Loss: 33.679\n",
      "Epoch: 17 | NMI: 0.067 | ARI: 0.199\n",
      "Epoch: 17 | NMI: 0.067 | ARI: 0.199\n",
      "Epoch: 17 | Batch: 010 | Loss: 2.993 | Rec-Loss: 0.927 | Dist-Loss: 1.137 | Classification-Loss: 0.933\n",
      "Epoch: 17 | Batch: 020 | Loss: 2.241 | Rec-Loss: 0.815 | Dist-Loss: 0.668 | Classification-Loss: 0.970\n",
      "Epoch: 17 | Batch: 030 | Loss: 2.292 | Rec-Loss: 0.787 | Dist-Loss: 0.870 | Classification-Loss: 0.827\n",
      "Epoch: 17 Loss: 127.651 | Rec-Loss: 29.775 | Dist-Loss: 62.517 | Classification-Loss: 33.989\n",
      "Epoch: 18 | NMI: 0.070 | ARI: 0.200\n",
      "Epoch: 18 | NMI: 0.070 | ARI: 0.200\n",
      "Epoch: 18 | Batch: 010 | Loss: 3.729 | Rec-Loss: 0.713 | Dist-Loss: 2.048 | Classification-Loss: 0.995\n",
      "Epoch: 18 | Batch: 020 | Loss: 4.937 | Rec-Loss: 0.799 | Dist-Loss: 3.197 | Classification-Loss: 0.911\n",
      "Epoch: 18 | Batch: 030 | Loss: 4.227 | Rec-Loss: 0.712 | Dist-Loss: 2.722 | Classification-Loss: 0.864\n",
      "Epoch: 18 Loss: 113.874 | Rec-Loss: 28.825 | Dist-Loss: 54.912 | Classification-Loss: 33.579\n",
      "Epoch: 19 | NMI: 0.076 | ARI: 0.216\n",
      "Epoch: 19 | NMI: 0.076 | ARI: 0.216\n",
      "Epoch: 19 | Batch: 010 | Loss: 3.467 | Rec-Loss: 0.753 | Dist-Loss: 1.687 | Classification-Loss: 0.873\n",
      "Epoch: 19 | Batch: 020 | Loss: 1.823 | Rec-Loss: 0.763 | Dist-Loss: 0.284 | Classification-Loss: 0.965\n",
      "Epoch: 19 | Batch: 030 | Loss: 2.939 | Rec-Loss: 0.884 | Dist-Loss: 1.175 | Classification-Loss: 1.035\n",
      "Epoch: 19 Loss: 101.087 | Rec-Loss: 28.358 | Dist-Loss: 41.306 | Classification-Loss: 34.335\n",
      "Epoch: 20 | NMI: 0.078 | ARI: 0.220\n",
      "Epoch: 20 | NMI: 0.078 | ARI: 0.220\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.7218149990552464\n",
      "0.0 0.7203910553625674\n",
      "Iter 20 : Acc 0.0000 , nmi 0.0000 , ari 0.0000 , Test F1 0.000, Test AUC 0.720\n",
      "Epoch: 20 | Batch: 010 | Loss: 1.868 | Rec-Loss: 0.960 | Dist-Loss: 0.150 | Classification-Loss: 1.007\n",
      "Epoch: 20 | Batch: 020 | Loss: 3.239 | Rec-Loss: 0.782 | Dist-Loss: 1.696 | Classification-Loss: 0.813\n",
      "Epoch: 20 | Batch: 030 | Loss: 3.211 | Rec-Loss: 0.744 | Dist-Loss: 1.515 | Classification-Loss: 0.939\n",
      "Epoch: 20 Loss: 96.359 | Rec-Loss: 27.917 | Dist-Loss: 37.377 | Classification-Loss: 34.408\n",
      "Epoch: 21 | NMI: 0.072 | ARI: 0.208\n",
      "Epoch: 21 | NMI: 0.072 | ARI: 0.208\n",
      "Epoch: 21 | Batch: 010 | Loss: 4.280 | Rec-Loss: 0.762 | Dist-Loss: 2.408 | Classification-Loss: 0.944\n",
      "Epoch: 21 | Batch: 020 | Loss: 3.062 | Rec-Loss: 0.700 | Dist-Loss: 1.481 | Classification-Loss: 0.857\n",
      "Epoch: 21 | Batch: 030 | Loss: 2.369 | Rec-Loss: 0.727 | Dist-Loss: 0.712 | Classification-Loss: 0.950\n",
      "Epoch: 21 Loss: 95.570 | Rec-Loss: 27.689 | Dist-Loss: 35.757 | Classification-Loss: 34.247\n",
      "Epoch: 22 | NMI: 0.066 | ARI: 0.195\n",
      "Epoch: 22 | NMI: 0.066 | ARI: 0.195\n",
      "Epoch: 22 | Batch: 010 | Loss: 2.842 | Rec-Loss: 0.765 | Dist-Loss: 1.164 | Classification-Loss: 0.892\n",
      "Epoch: 22 | Batch: 020 | Loss: 2.490 | Rec-Loss: 0.745 | Dist-Loss: 1.041 | Classification-Loss: 0.798\n",
      "Epoch: 22 | Batch: 030 | Loss: 4.977 | Rec-Loss: 0.780 | Dist-Loss: 2.917 | Classification-Loss: 0.859\n",
      "Epoch: 22 Loss: 110.887 | Rec-Loss: 27.467 | Dist-Loss: 49.088 | Classification-Loss: 34.210\n",
      "Epoch: 23 | NMI: 0.066 | ARI: 0.192\n",
      "Epoch: 23 | NMI: 0.066 | ARI: 0.192\n",
      "Epoch: 23 | Batch: 010 | Loss: 1.847 | Rec-Loss: 0.815 | Dist-Loss: 0.202 | Classification-Loss: 0.993\n",
      "Epoch: 23 | Batch: 020 | Loss: 2.050 | Rec-Loss: 0.756 | Dist-Loss: 0.448 | Classification-Loss: 1.000\n",
      "Epoch: 23 | Batch: 030 | Loss: 2.829 | Rec-Loss: 0.744 | Dist-Loss: 1.115 | Classification-Loss: 0.769\n",
      "Epoch: 23 Loss: 108.476 | Rec-Loss: 27.448 | Dist-Loss: 46.567 | Classification-Loss: 32.688\n",
      "Epoch: 24 | NMI: 0.070 | ARI: 0.204\n",
      "Epoch: 24 | NMI: 0.070 | ARI: 0.204\n",
      "Epoch: 24 | Batch: 010 | Loss: 2.340 | Rec-Loss: 0.775 | Dist-Loss: 0.536 | Classification-Loss: 1.081\n",
      "Epoch: 24 | Batch: 020 | Loss: 2.563 | Rec-Loss: 0.868 | Dist-Loss: 0.864 | Classification-Loss: 0.900\n",
      "Epoch: 24 | Batch: 030 | Loss: 2.786 | Rec-Loss: 0.789 | Dist-Loss: 0.913 | Classification-Loss: 1.032\n",
      "Epoch: 24 Loss: 134.650 | Rec-Loss: 28.232 | Dist-Loss: 68.484 | Classification-Loss: 33.017\n",
      "Epoch: 25 | NMI: 0.069 | ARI: 0.201\n",
      "Epoch: 25 | NMI: 0.069 | ARI: 0.201\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04405286343612334 0.742906171421927\n",
      "0.052401746724890834 0.7092861113937299\n",
      "Iter 25 : Acc 0.0000 , nmi 0.0000 , ari 0.0000 , Test F1 0.052, Test AUC 0.709\n",
      "Epoch: 25 | Batch: 010 | Loss: 3.285 | Rec-Loss: 0.807 | Dist-Loss: 1.317 | Classification-Loss: 0.968\n",
      "Epoch: 25 | Batch: 020 | Loss: 3.746 | Rec-Loss: 0.903 | Dist-Loss: 1.962 | Classification-Loss: 0.888\n",
      "Epoch: 25 | Batch: 030 | Loss: 1.987 | Rec-Loss: 0.678 | Dist-Loss: 0.449 | Classification-Loss: 0.876\n",
      "Epoch: 25 Loss: 141.321 | Rec-Loss: 28.008 | Dist-Loss: 72.780 | Classification-Loss: 32.960\n",
      "Epoch: 26 | NMI: 0.056 | ARI: 0.166\n",
      "Epoch: 26 | NMI: 0.056 | ARI: 0.166\n",
      "Epoch: 26 | Batch: 010 | Loss: 2.989 | Rec-Loss: 0.723 | Dist-Loss: 1.260 | Classification-Loss: 0.859\n",
      "Epoch: 26 | Batch: 020 | Loss: 2.646 | Rec-Loss: 0.707 | Dist-Loss: 1.077 | Classification-Loss: 0.900\n",
      "Epoch: 26 | Batch: 030 | Loss: 2.694 | Rec-Loss: 0.684 | Dist-Loss: 1.007 | Classification-Loss: 1.007\n",
      "Epoch: 26 Loss: 117.207 | Rec-Loss: 27.279 | Dist-Loss: 53.786 | Classification-Loss: 32.255\n",
      "Epoch: 27 | NMI: 0.048 | ARI: 0.140\n",
      "Epoch: 27 | NMI: 0.048 | ARI: 0.140\n",
      "Epoch: 27 | Batch: 010 | Loss: 3.203 | Rec-Loss: 0.713 | Dist-Loss: 1.418 | Classification-Loss: 0.924\n",
      "Epoch: 27 | Batch: 020 | Loss: 2.450 | Rec-Loss: 0.883 | Dist-Loss: 0.671 | Classification-Loss: 0.845\n",
      "Epoch: 27 | Batch: 030 | Loss: 3.963 | Rec-Loss: 0.772 | Dist-Loss: 2.137 | Classification-Loss: 0.817\n",
      "Epoch: 27 Loss: 121.241 | Rec-Loss: 27.765 | Dist-Loss: 55.902 | Classification-Loss: 32.039\n",
      "Epoch: 28 | NMI: 0.060 | ARI: 0.152\n",
      "Epoch: 28 | NMI: 0.060 | ARI: 0.152\n",
      "Epoch: 28 | Batch: 010 | Loss: 2.457 | Rec-Loss: 0.778 | Dist-Loss: 0.705 | Classification-Loss: 1.043\n",
      "Epoch: 28 | Batch: 020 | Loss: 4.704 | Rec-Loss: 0.804 | Dist-Loss: 2.475 | Classification-Loss: 0.871\n",
      "Epoch: 28 | Batch: 030 | Loss: 1.875 | Rec-Loss: 0.732 | Dist-Loss: 0.356 | Classification-Loss: 0.826\n",
      "Epoch: 28 Loss: 109.557 | Rec-Loss: 27.120 | Dist-Loss: 46.898 | Classification-Loss: 30.512\n",
      "Epoch: 29 | NMI: 0.058 | ARI: 0.141\n",
      "Epoch: 29 | NMI: 0.058 | ARI: 0.141\n",
      "Epoch: 29 | Batch: 010 | Loss: 12.906 | Rec-Loss: 0.682 | Dist-Loss: 10.313 | Classification-Loss: 0.719\n",
      "Epoch: 29 | Batch: 020 | Loss: 8.901 | Rec-Loss: 0.814 | Dist-Loss: 6.476 | Classification-Loss: 0.711\n",
      "Epoch: 29 | Batch: 030 | Loss: 5.331 | Rec-Loss: 0.727 | Dist-Loss: 3.304 | Classification-Loss: 0.629\n",
      "Epoch: 29 Loss: 202.422 | Rec-Loss: 27.123 | Dist-Loss: 124.155 | Classification-Loss: 26.775\n",
      "Epoch: 30 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 30 | NMI: 0.000 | ARI: 0.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19961612284069097 0.5826241624394994\n",
      "0.19961612284069097 0.5890721065100798\n",
      "Iter 30 : Acc 0.0000 , nmi 0.0000 , ari 0.0000 , Test F1 0.200, Test AUC 0.589\n",
      "Epoch: 30 | Batch: 010 | Loss: 4.483 | Rec-Loss: 0.754 | Dist-Loss: 2.299 | Classification-Loss: 1.064\n",
      "Epoch: 30 | Batch: 020 | Loss: 5.174 | Rec-Loss: 0.637 | Dist-Loss: 3.525 | Classification-Loss: 0.722\n",
      "Epoch: 30 | Batch: 030 | Loss: 2.904 | Rec-Loss: 0.843 | Dist-Loss: 0.996 | Classification-Loss: 0.917\n",
      "Epoch: 30 Loss: nan | Rec-Loss: 26.857 | Dist-Loss: 55.254 | Classification-Loss: nan\n",
      "Epoch: 31 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 31 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 31 | Batch: 010 | Loss: 2.099 | Rec-Loss: 0.859 | Dist-Loss: 0.510 | Classification-Loss: 0.655\n",
      "Epoch: 31 | Batch: 020 | Loss: 1.454 | Rec-Loss: 0.693 | Dist-Loss: 0.230 | Classification-Loss: 0.477\n",
      "Epoch: 31 | Batch: 030 | Loss: 1.740 | Rec-Loss: 0.625 | Dist-Loss: 0.614 | Classification-Loss: 0.478\n",
      "Epoch: 31 Loss: 70.037 | Rec-Loss: 25.909 | Dist-Loss: 18.384 | Classification-Loss: 24.191\n",
      "Epoch: 32 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 32 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 32 | Batch: 010 | Loss: 1.622 | Rec-Loss: 0.710 | Dist-Loss: 0.162 | Classification-Loss: 0.795\n",
      "Epoch: 32 | Batch: 020 | Loss: 1.502 | Rec-Loss: 0.697 | Dist-Loss: 0.109 | Classification-Loss: 0.750\n",
      "Epoch: 32 | Batch: 030 | Loss: 1.273 | Rec-Loss: 0.686 | Dist-Loss: 0.081 | Classification-Loss: 0.543\n",
      "Epoch: 32 Loss: 54.741 | Rec-Loss: 25.426 | Dist-Loss: 6.064 | Classification-Loss: 24.756\n",
      "Epoch: 33 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 33 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 33 | Batch: 010 | Loss: 1.278 | Rec-Loss: 0.640 | Dist-Loss: 0.088 | Classification-Loss: 0.600\n",
      "Epoch: 33 | Batch: 020 | Loss: 1.582 | Rec-Loss: 0.714 | Dist-Loss: 0.050 | Classification-Loss: 0.906\n",
      "Epoch: 33 | Batch: 030 | Loss: 1.538 | Rec-Loss: 0.646 | Dist-Loss: 0.075 | Classification-Loss: 0.907\n",
      "Epoch: 33 Loss: nan | Rec-Loss: 25.104 | Dist-Loss: 2.236 | Classification-Loss: nan\n",
      "Epoch: 34 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 34 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 34 | Batch: 010 | Loss: 1.451 | Rec-Loss: 0.636 | Dist-Loss: 0.023 | Classification-Loss: 0.877\n",
      "Epoch: 34 | Batch: 020 | Loss: 1.033 | Rec-Loss: 0.652 | Dist-Loss: 0.048 | Classification-Loss: 0.366\n",
      "Epoch: 34 | Batch: 030 | Loss: 1.337 | Rec-Loss: 0.725 | Dist-Loss: 0.027 | Classification-Loss: 0.649\n",
      "Epoch: 34 Loss: 50.400 | Rec-Loss: 24.798 | Dist-Loss: 1.239 | Classification-Loss: 26.947\n",
      "Epoch: 35 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 35 | NMI: 0.000 | ARI: 0.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16901408450704225 0.7486028546096714\n",
      "0.16901408450704225 0.7486301071205361\n",
      "Iter 35 : Acc 0.0000 , nmi 0.0000 , ari 0.0000 , Test F1 0.169, Test AUC 0.749\n",
      "Epoch: 35 | Batch: 010 | Loss: 1.371 | Rec-Loss: 0.642 | Dist-Loss: 0.012 | Classification-Loss: 0.791\n",
      "Epoch: 35 | Batch: 020 | Loss: 1.355 | Rec-Loss: 0.624 | Dist-Loss: 0.017 | Classification-Loss: 0.788\n",
      "Epoch: 35 | Batch: 030 | Loss: 1.220 | Rec-Loss: 0.683 | Dist-Loss: 0.012 | Classification-Loss: 0.578\n",
      "Epoch: 35 Loss: 48.922 | Rec-Loss: 24.623 | Dist-Loss: 0.928 | Classification-Loss: 25.757\n",
      "Epoch: 36 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 36 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 36 | Batch: 010 | Loss: 1.900 | Rec-Loss: 0.793 | Dist-Loss: 0.013 | Classification-Loss: 1.201\n",
      "Epoch: 36 | Batch: 020 | Loss: 1.659 | Rec-Loss: 0.688 | Dist-Loss: 0.022 | Classification-Loss: 1.041\n",
      "Epoch: 36 | Batch: 030 | Loss: 1.368 | Rec-Loss: 0.742 | Dist-Loss: 0.022 | Classification-Loss: 0.662\n",
      "Epoch: 36 Loss: 54.244 | Rec-Loss: 24.640 | Dist-Loss: 0.938 | Classification-Loss: 31.426\n",
      "Epoch: 37 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 37 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 37 | Batch: 010 | Loss: 1.656 | Rec-Loss: 0.651 | Dist-Loss: 0.016 | Classification-Loss: 1.078\n",
      "Epoch: 37 | Batch: 020 | Loss: 1.155 | Rec-Loss: 0.610 | Dist-Loss: 0.017 | Classification-Loss: 0.575\n",
      "Epoch: 37 | Batch: 030 | Loss: 1.327 | Rec-Loss: 0.653 | Dist-Loss: 0.011 | Classification-Loss: 0.723\n",
      "Epoch: 37 Loss: nan | Rec-Loss: 24.098 | Dist-Loss: 0.815 | Classification-Loss: nan\n",
      "Epoch: 38 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 38 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 38 | Batch: 010 | Loss: 1.702 | Rec-Loss: 0.612 | Dist-Loss: 0.012 | Classification-Loss: 1.170\n",
      "Epoch: 38 | Batch: 020 | Loss: 1.416 | Rec-Loss: 0.633 | Dist-Loss: 0.012 | Classification-Loss: 0.837\n",
      "Epoch: 38 | Batch: 030 | Loss: 1.433 | Rec-Loss: 0.632 | Dist-Loss: 0.012 | Classification-Loss: 0.856\n",
      "Epoch: 38 Loss: 54.297 | Rec-Loss: 24.159 | Dist-Loss: 0.822 | Classification-Loss: 31.774\n",
      "Epoch: 39 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 39 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 39 | Batch: 010 | Loss: 1.635 | Rec-Loss: 0.602 | Dist-Loss: 0.019 | Classification-Loss: 1.093\n",
      "Epoch: 39 | Batch: 020 | Loss: 1.409 | Rec-Loss: 0.691 | Dist-Loss: 0.015 | Classification-Loss: 0.757\n",
      "Epoch: 39 | Batch: 030 | Loss: 1.509 | Rec-Loss: 0.792 | Dist-Loss: 0.013 | Classification-Loss: 0.758\n",
      "Epoch: 39 Loss: nan | Rec-Loss: 24.013 | Dist-Loss: 0.790 | Classification-Loss: nan\n",
      "Epoch: 40 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 40 | NMI: 0.000 | ARI: 0.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18503937007874016 0.753615953983227\n",
      "0.18503937007874016 0.7535973314341361\n",
      "Iter 40 : Acc 0.0000 , nmi 0.0000 , ari 0.0000 , Test F1 0.185, Test AUC 0.754\n",
      "Epoch: 40 | Batch: 010 | Loss: 1.128 | Rec-Loss: 0.653 | Dist-Loss: 0.012 | Classification-Loss: 0.498\n",
      "Epoch: 40 | Batch: 020 | Loss: 1.395 | Rec-Loss: 0.667 | Dist-Loss: 0.013 | Classification-Loss: 0.767\n",
      "Epoch: 40 | Batch: 030 | Loss: 1.237 | Rec-Loss: 0.603 | Dist-Loss: 0.013 | Classification-Loss: 0.666\n",
      "Epoch: 40 Loss: 51.394 | Rec-Loss: 23.421 | Dist-Loss: 0.951 | Classification-Loss: 28.970\n",
      "Epoch: 41 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 41 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 41 | Batch: 010 | Loss: 1.420 | Rec-Loss: 0.634 | Dist-Loss: 0.010 | Classification-Loss: 0.828\n",
      "Epoch: 41 | Batch: 020 | Loss: 1.251 | Rec-Loss: 0.584 | Dist-Loss: 0.016 | Classification-Loss: 0.695\n",
      "Epoch: 41 | Batch: 030 | Loss: 1.782 | Rec-Loss: 0.612 | Dist-Loss: 0.010 | Classification-Loss: 1.239\n",
      "Epoch: 41 Loss: 51.414 | Rec-Loss: 23.360 | Dist-Loss: 0.782 | Classification-Loss: 29.107\n",
      "Epoch: 42 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 42 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 42 | Batch: 010 | Loss: 1.449 | Rec-Loss: 0.644 | Dist-Loss: 0.012 | Classification-Loss: 0.844\n",
      "Epoch: 42 | Batch: 020 | Loss: 1.844 | Rec-Loss: 0.657 | Dist-Loss: 0.012 | Classification-Loss: 1.250\n",
      "Epoch: 42 | Batch: 030 | Loss: 2.194 | Rec-Loss: 0.653 | Dist-Loss: 0.011 | Classification-Loss: 1.628\n",
      "Epoch: 42 Loss: 50.804 | Rec-Loss: 23.146 | Dist-Loss: 0.909 | Classification-Loss: 28.418\n",
      "Epoch: 43 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 43 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 43 | Batch: 010 | Loss: 1.516 | Rec-Loss: 0.603 | Dist-Loss: 0.015 | Classification-Loss: 0.951\n",
      "Epoch: 43 | Batch: 020 | Loss: 1.857 | Rec-Loss: 0.647 | Dist-Loss: 0.017 | Classification-Loss: 1.264\n",
      "Epoch: 43 | Batch: 030 | Loss: 1.107 | Rec-Loss: 0.625 | Dist-Loss: 0.015 | Classification-Loss: 0.494\n",
      "Epoch: 43 Loss: 53.029 | Rec-Loss: 23.299 | Dist-Loss: 0.873 | Classification-Loss: 30.527\n",
      "Epoch: 44 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 44 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 44 | Batch: 010 | Loss: 1.352 | Rec-Loss: 0.631 | Dist-Loss: 0.012 | Classification-Loss: 0.747\n",
      "Epoch: 44 | Batch: 020 | Loss: 1.661 | Rec-Loss: 0.643 | Dist-Loss: 0.012 | Classification-Loss: 1.061\n",
      "Epoch: 44 | Batch: 030 | Loss: 1.403 | Rec-Loss: 0.616 | Dist-Loss: 0.012 | Classification-Loss: 0.817\n",
      "Epoch: 44 Loss: 51.968 | Rec-Loss: 22.878 | Dist-Loss: 0.752 | Classification-Loss: 29.869\n",
      "Epoch: 45 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 45 | NMI: 0.000 | ARI: 0.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.4241894194851819\n",
      "0.0 0.5\n",
      "Iter 45 : Acc 0.0000 , nmi 0.0000 , ari 0.0000 , Test F1 0.000, Test AUC 0.500\n",
      "Epoch: 45 | Batch: 010 | Loss: 1.613 | Rec-Loss: 0.723 | Dist-Loss: 0.011 | Classification-Loss: 0.923\n",
      "Epoch: 45 | Batch: 020 | Loss: 1.906 | Rec-Loss: 0.704 | Dist-Loss: 0.010 | Classification-Loss: 1.253\n",
      "Epoch: 45 | Batch: 030 | Loss: 1.124 | Rec-Loss: 0.582 | Dist-Loss: 0.012 | Classification-Loss: 0.557\n",
      "Epoch: 45 Loss: nan | Rec-Loss: 22.570 | Dist-Loss: 0.908 | Classification-Loss: nan\n",
      "Epoch: 46 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 46 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 46 | Batch: 010 | Loss: 1.570 | Rec-Loss: 0.671 | Dist-Loss: 0.009 | Classification-Loss: 0.931\n",
      "Epoch: 46 | Batch: 020 | Loss: 1.953 | Rec-Loss: 0.618 | Dist-Loss: 0.014 | Classification-Loss: 1.384\n",
      "Epoch: 46 | Batch: 030 | Loss: 1.457 | Rec-Loss: 0.629 | Dist-Loss: 0.009 | Classification-Loss: 0.858\n",
      "Epoch: 46 Loss: 50.639 | Rec-Loss: 22.560 | Dist-Loss: 0.716 | Classification-Loss: 28.645\n",
      "Epoch: 47 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 47 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 47 | Batch: 010 | Loss: 1.481 | Rec-Loss: 0.670 | Dist-Loss: 0.012 | Classification-Loss: 0.835\n",
      "Epoch: 47 | Batch: 020 | Loss: 1.662 | Rec-Loss: 0.627 | Dist-Loss: 0.010 | Classification-Loss: 1.070\n",
      "Epoch: 47 | Batch: 030 | Loss: 1.539 | Rec-Loss: 0.663 | Dist-Loss: 0.012 | Classification-Loss: 0.903\n",
      "Epoch: 47 Loss: nan | Rec-Loss: 22.506 | Dist-Loss: 0.669 | Classification-Loss: nan\n",
      "Epoch: 48 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 48 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 48 | Batch: 010 | Loss: 1.093 | Rec-Loss: 0.584 | Dist-Loss: 0.012 | Classification-Loss: 0.516\n",
      "Epoch: 48 | Batch: 020 | Loss: 1.035 | Rec-Loss: 0.605 | Dist-Loss: 0.020 | Classification-Loss: 0.427\n",
      "Epoch: 48 | Batch: 030 | Loss: 1.380 | Rec-Loss: 0.638 | Dist-Loss: 0.013 | Classification-Loss: 0.759\n",
      "Epoch: 48 Loss: nan | Rec-Loss: 22.505 | Dist-Loss: 0.703 | Classification-Loss: nan\n",
      "Epoch: 49 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 49 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 49 | Batch: 010 | Loss: 1.408 | Rec-Loss: 0.563 | Dist-Loss: 0.013 | Classification-Loss: 0.863\n",
      "Epoch: 49 | Batch: 020 | Loss: 1.189 | Rec-Loss: 0.636 | Dist-Loss: 0.012 | Classification-Loss: 0.561\n",
      "Epoch: 49 | Batch: 030 | Loss: 1.478 | Rec-Loss: 0.548 | Dist-Loss: 0.010 | Classification-Loss: 0.955\n",
      "Epoch: 49 Loss: nan | Rec-Loss: 22.492 | Dist-Loss: 0.748 | Classification-Loss: nan\n",
      "Epoch: 50 | NMI: 0.000 | ARI: 0.000\n",
      "Epoch: 50 | NMI: 0.000 | ARI: 0.000\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model.args = args\n",
    "# model.clustering = initial_clustering\n",
    "reducer = umap.UMAP()\n",
    "\n",
    "X_latents, _, tmp_q = model(torch.FloatTensor(X_train).to(args.device))\n",
    "tmp_q, tmp_q_p, tmp_q_n = tmp_q\n",
    "# update target distribution p\n",
    "\n",
    "# evaluate clustering performance\n",
    "y_pred = tmp_q.detach().numpy().argmax(1)\n",
    "y_pred_last = y_pred\n",
    "\n",
    "for epoch in range(args.epoch):\n",
    "    # Print training set\n",
    "    if epoch%5 == 0:\n",
    "        out = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "        cluster_id = model.clustering.update_assign(out.cpu().detach().numpy())\n",
    "        X2 = reducer.fit_transform(out.cpu().detach().numpy())\n",
    "\n",
    "        c_clusters = [color[int(cluster_id[i])] for i in range(len(cluster_id))]\n",
    "        c_labels = [color[int(y_train[i])] for i in range(len(cluster_id))]\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.suptitle('Clusters vs Labels')\n",
    "        ax1.scatter(X2[:,0], X2[:,1], color=c_clusters)\n",
    "        ax2.scatter(X2[:,0], X2[:,1], color=c_labels)\n",
    "        plt.show()\n",
    "        \n",
    "#         plot(model, torch.FloatTensor(X_train).to(args.device), y_train)\n",
    "        X_latents, _, tmp_q = model(torch.FloatTensor(X_train).to(args.device))\n",
    "        tmp_q, tmp_q_p, tmp_q_n = tmp_q\n",
    "        # update target distribution p\n",
    "\n",
    "        # evaluate clustering performance\n",
    "        y_pred = tmp_q.detach().numpy().argmax(1)\n",
    "        delta_label = np.sum(y_pred != y_pred_last).astype(\n",
    "            np.float32) / y_pred.shape[0]\n",
    "        y_pred_last = y_pred\n",
    "\n",
    "        # acc = cluster_acc(y, y_pred)\n",
    "        # nmi = nmi_score(y, y_pred)\n",
    "        # ari = ari_score(y, y_pred)\n",
    "        acc, nmi, ari = 0, 0, 0\n",
    "        \n",
    "        X_test_latents, X_test_bar, qs = model(torch.FloatTensor(X_test).to(args.device))\n",
    "        q_test = qs[0]\n",
    "        cluster_ids = torch.argmax(q_test, axis=1)\n",
    "        preds = torch.zeros((len(X_test_bar), 2))\n",
    "\n",
    "        # Weighted predictions\n",
    "        for j in range(model.n_clusters):\n",
    "            cluster_id = np.where(cluster_ids == j)[0]\n",
    "            cluster_preds = model.classifiers[j][0](X_test_latents)\n",
    "#             print(q_test, cluster_preds[:,0])\n",
    "            preds[:,0] += q_test[:,j]*cluster_preds[:,0]\n",
    "            preds[:,1] += q_test[:,j]*cluster_preds[:,1]\n",
    "\n",
    "        test_f1 = f1_score(y_test, np.argmax(preds.detach().numpy(), axis=1))\n",
    "        test_auc = roc_auc_score(y_test, preds[:,1].detach().numpy())\n",
    "        print(test_f1, test_auc)\n",
    "        # Hard local predictions\n",
    "        for j in range(model.n_clusters):\n",
    "            cluster_id = np.where(cluster_ids == j)[0]\n",
    "            X_cluster = X_test_latents[cluster_id]\n",
    "            cluster_preds = model.classifiers[j][0](X_cluster)\n",
    "            preds[cluster_id,:] = cluster_preds\n",
    "\n",
    "        test_f1 = f1_score(y_test, np.argmax(preds.detach().numpy(), axis=1))\n",
    "        test_auc = roc_auc_score(y_test, preds[:,1].detach().numpy())\n",
    "        print(test_f1, test_auc)\n",
    "        \n",
    "        print('Iter {}'.format(epoch), ': Acc {:.4f}'.format(acc),\n",
    "              ', nmi {:.4f}'.format(nmi), ', ari {:.4f}'.format(ari), \n",
    "              ', Test F1 {:.3f}, Test AUC {:.3f}'.format(test_f1, test_auc))\n",
    "\n",
    "    model.train()\n",
    "    model.fit(epoch, train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    out = evaluate(model, test_loader)  # evaluation on the test_loader\n",
    "    if len(out) > 2:\n",
    "        NMI, ARI, base_f1, base_mcc, base_auc, cac_f1, cac_mcc, cac_auc = out\n",
    "        print('Epoch: {:02d} | NMI: {:.3f} | ARI: {:.3f} | Base_F1: {:.3f} | Base_MCC: {:.3f} | Base_AUC: {:.3f} | CAC_F1: {:.3f} | CAC_MCC: {:.3f} | CAC_AUC: {:.3f}'.format(\n",
    "                epoch+1, NMI, ARI, base_f1, base_mcc, base_auc, cac_f1, cac_mcc, cac_auc))\n",
    "    else:\n",
    "        NMI, ARI = out\n",
    "        print('Epoch: {:02d} | NMI: {:.3f} | ARI: {:.3f}'.format(\n",
    "                epoch+1, NMI, ARI))\n",
    "    nmi_list.append(NMI)\n",
    "    ari_list.append(ARI)\n",
    "    print('Epoch: {:02d} | NMI: {:.3f} | ARI: {:.3f}'.format(\n",
    "            epoch+1, NMI, ARI))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(NNClassifier, self).__init__()\n",
    "        self.args = args\n",
    "        self.input_dim = args.latent_dim\n",
    "        self.n_classes = args.n_classes\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "#         self.train_loss = None\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, args.n_classes)\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.classifier.parameters(), lr=args.lr)\n",
    "#         self.dropout = nn.Dropout(p=0.1)\n",
    "#         self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "#         self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.classifier(inputs)\n",
    "\n",
    "    def fit(self, X_batch, y_batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.classifier.train()\n",
    "        y_pred = self.forward(X_batch.detach())\n",
    "        train_loss = self.criterion(y_pred, y_batch)\n",
    "        train_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return y_pred.detach().numpy(), train_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def plot(model, X_train, y_train, X_test=None, y_test=None):\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    qs, latents_X = model(X_train, output=\"latent\")\n",
    "    q_train = qs[0]\n",
    "    print(q_train)\n",
    "    cluster_id_train = torch.argmax(q_train, axis=1)\n",
    "    X2 = reducer.fit_transform(latents_X.cpu().detach().numpy())\n",
    "\n",
    "    print(\"Training data\")\n",
    "\n",
    "    c_clusters = [color[int(cluster_id_train[i])] for i in range(len(cluster_id_train))]\n",
    "    c_labels = [color[int(y_train[i])] for i in range(len(cluster_id_train))]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle('Clusters vs Labels')\n",
    "    ax1.scatter(X2[:,0], X2[:,1], color=c_clusters)\n",
    "    ax2.scatter(X2[:,0], X2[:,1], color=c_labels)\n",
    "    plt.show()\n",
    "    if X_test is not None:\n",
    "        qs, latents_test = model(X_test, output=\"latent\")\n",
    "        q_test = qs[0]\n",
    "        X2 = reducer.transform(latents_test.cpu().detach().numpy())\n",
    "        cluster_id_test = torch.argmax(q_test, axis=1)\n",
    "        c_clusters = [color[int(cluster_id_test[i])] for i in range(len(cluster_id_test))]\n",
    "        c_labels = [color[int(y_test[i])] for i in range(len(cluster_id_test))]\n",
    "\n",
    "        print(\"Test data\")\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.suptitle('Clusters vs Labels')\n",
    "        ax1.scatter(X2[:,0], X2[:,1], color=c_clusters)\n",
    "        ax2.scatter(X2[:,0], X2[:,1], color=c_labels)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch wise local network training (Soft clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NNClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-9f8539cd9686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training separate models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclassifiers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mNNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# optimizers = [torch.optim.Adam(classifiers[i].classifier.parameters(), lr=args.lr) for i in range(args.n_clusters)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-9f8539cd9686>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training separate models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclassifiers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mNNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# optimizers = [torch.optim.Adam(classifiers[i].classifier.parameters(), lr=args.lr) for i in range(args.n_clusters)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NNClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Training separate models\n",
    "args.lr = 0.02\n",
    "classifiers = [NNClassifier(args) for _ in range(args.n_clusters)]\n",
    "\n",
    "# optimizers = [torch.optim.Adam(classifiers[i].classifier.parameters(), lr=args.lr) for i in range(args.n_clusters)]\n",
    "EPOCHS = 200\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "\n",
    "\n",
    "latents_X = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id_train = model.clustering.update_assign(latents_X.cpu().detach().numpy())\n",
    "X_latents_data_loader = list(zip(latents_X, cluster_id_train, y_train))\n",
    "train_loader_latents = torch.utils.data.DataLoader(X_latents_data_loader,\n",
    "    batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "latents_test = model.autoencoder(torch.FloatTensor(np.array(X_test)).to(args.device), latent=True)\n",
    "cluster_id_test = model.clustering.update_assign(latents_test.cpu().detach().numpy())\n",
    "# plot(latents_X, y_train, latents_test, y_test)\n",
    "\n",
    "\n",
    "for e in range(1, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    alpha = (1-e/EPOCHS)\n",
    "    acc = 0\n",
    "#     plot(latents_X, y_train, latents_test, y_test)\n",
    "\n",
    "    for X_batch, cluster_batch, y_batch in train_loader_latents:\n",
    "        q = 1.0 / (1.0 + torch.sum(\n",
    "            torch.pow(X_batch.detach().unsqueeze(1) - model.clustering.clusters, 2), 2) / args.alpha)\n",
    "        q = q.pow((args.alpha + 1.0) / 2.0)\n",
    "        q = (q.t() / torch.sum(q, 1)).t()\n",
    "\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        new_y_batch = []\n",
    "        y_pred = np.zeros((len(X_batch), 2))\n",
    "        for k in range(args.n_clusters):\n",
    "#             idx = np.where(cluster_batch == k)[0]\n",
    "            y_pred_idx, loss = classifiers[k].fit(X_batch, y_batch)\n",
    "            y_pred += q[:,k].numpy().reshape(-1,1)*y_pred_idx\n",
    "#             new_y_batch.append(y_batch.detach().numpy())\n",
    "            epoch_loss += loss\n",
    "        y_pred = y_pred/args.n_clusters\n",
    "#         print(y_pred)\n",
    "#         y_pred = np.hstack(y_pred)\n",
    "#         new_y_batch = np.hstack(new_y_batch)\n",
    "        \n",
    "        f1 = f1_score(np.argmax(y_pred, axis=1), y_batch)\n",
    "        acc = roc_auc_score(y_batch, y_pred[:,1])\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_f1 += f1.item()\n",
    "\n",
    "    test_preds = []\n",
    "    test_loss = 0.0\n",
    "    new_y_test = []\n",
    "    \n",
    "    # Epoch Testing\n",
    "#     for k in range(args.n_clusters):\n",
    "#         classifiers[k].classifier.eval()\n",
    "#         idx = np.where(cluster_id_test == k)[0]\n",
    "#         latents_idx = latents_test[idx]\n",
    "#         y_pred_idx = classifiers[k](latents_idx)\n",
    "#         test_loss += nn.CrossEntropyLoss(reduction='mean')(y_pred_idx, torch.tensor(y_test[idx]).to(device))\n",
    "#         test_preds.append(y_pred_idx.detach().numpy())\n",
    "#         new_y_test.append(y_test[idx])\n",
    "    q = 1.0 / (1.0 + torch.sum(\n",
    "        torch.pow(latents_test.detach().unsqueeze(1) - model.clustering.clusters, 2), 2) / args.alpha)\n",
    "    q = q.pow((args.alpha + 1.0) / 2.0)\n",
    "    q = (q.t() / torch.sum(q, 1)).t()\n",
    "    \n",
    "    y_pred = np.zeros((len(X_test), 2))\n",
    "    for k in range(args.n_clusters):\n",
    "        classifiers[k].classifier.eval()\n",
    "#         idx = np.where(cluster_id_test == k)[0]\n",
    "        latents_idx = latents_test\n",
    "        y_pred_idx = classifiers[k](latents_idx)\n",
    "#         print(q.shape, y_pred_idx.shape)\n",
    "        y_pred += q[:,k].numpy().reshape(-1,1)*y_pred_idx.detach().numpy()\n",
    "\n",
    "        test_loss += nn.CrossEntropyLoss(reduction='mean')(torch.tensor(y_pred).to(device), torch.tensor(y_test).to(device))\n",
    "#         test_preds.append(y_pred)\n",
    "#         new_y_test.append(y_test[idx])\n",
    "#     test_preds = np.vstack(test_preds)\n",
    "#     new_y_test = np.hstack(new_y_test).reshape(-1)\n",
    "    test_f1 = f1_score(np.argmax(y_pred, axis=1), y_test)\n",
    "    y_pred = y_pred/args.n_clusters\n",
    "    test_acc = roc_auc_score(y_test, y_pred[:,1])\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {epoch_loss/len(train_loader):.5f} | Train F1: {epoch_f1/len(train_loader):.3f} | Train Acc: {epoch_acc/len(train_loader):.3f}| Test F1: {test_f1:.3f} | Test Acc: {test_acc:.3f} | Test Loss: {test_loss:.3f}')\n",
    "\n",
    "out = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id = model.clustering.update_assign(out.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Local Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 1.35373 | Train F1: 0.551 | Train Acc: 0.498| Test F1: 0.576 | Test Acc: 0.584 | Test Loss: 1.358\n",
      "Epoch 002: | Train Loss: 1.35362 | Train F1: 0.561 | Train Acc: 0.499| Test F1: 0.576 | Test Acc: 0.584 | Test Loss: 1.359\n",
      "Epoch 003: | Train Loss: 1.35416 | Train F1: 0.561 | Train Acc: 0.498| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.359\n",
      "Epoch 004: | Train Loss: 1.35444 | Train F1: 0.561 | Train Acc: 0.498| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.359\n",
      "Epoch 005: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.498| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.359\n",
      "Epoch 006: | Train Loss: 1.35454 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.359\n",
      "Epoch 007: | Train Loss: 1.35454 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.359\n",
      "Epoch 008: | Train Loss: 1.35454 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.359\n",
      "Epoch 009: | Train Loss: 1.35454 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.359\n",
      "Epoch 010: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.359\n",
      "Epoch 011: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.359\n",
      "Epoch 012: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.359\n",
      "Epoch 013: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.359\n",
      "Epoch 014: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 015: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 016: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 017: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 018: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 019: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 020: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 021: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 022: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 023: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 024: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 025: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 026: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 027: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 028: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 029: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 030: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 031: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 032: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 033: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 034: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 035: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 036: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 037: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 038: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n",
      "Epoch 039: | Train Loss: 1.35453 | Train F1: 0.561 | Train Acc: 0.497| Test F1: 0.576 | Test Acc: 0.585 | Test Loss: 1.358\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-f20cbeb8bd78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_batch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0my_pred_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;31m#             print(\"F1 Cluster: \", k, f1_score(np.argmax(y_pred_idx, axis=1), y_batch[idx]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#             print(\"Cluster: \", k, len(idx))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-607683e30d5c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_batch, y_batch)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    108\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training separate models\n",
    "args.lr = 0.02\n",
    "classifiers = [NNClassifier(args) for _ in range(args.n_clusters)]\n",
    "\n",
    "# optimizers = [torch.optim.Adam(classifiers[i].classifier.parameters(), lr=args.lr) for i in range(args.n_clusters)]\n",
    "EPOCHS = 100\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "\n",
    "\n",
    "latents_X = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id_train = model.clustering.update_assign(latents_X.cpu().detach().numpy())\n",
    "\n",
    "X_latents_data_loader = list(zip(latents_X, cluster_id_train, y_train))\n",
    "train_loader_latents = torch.utils.data.DataLoader(X_latents_data_loader,\n",
    "    batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "latents_test = model.autoencoder(torch.FloatTensor(np.array(X_test)).to(args.device), latent=True)\n",
    "cluster_id_test = model.clustering.update_assign(latents_test.cpu().detach().numpy())\n",
    "# plot(latents_X, y_train, latents_test, y_test)\n",
    "\n",
    "for e in range(1, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    alpha = (1-e/EPOCHS)\n",
    "    acc = 0\n",
    "#     plot(latents_X, y_train, latents_test, y_test)\n",
    "\n",
    "    for X_batch, cluster_batch, y_batch in train_loader_latents:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        new_y_batch = []\n",
    "        y_pred = []\n",
    "        for k in range(args.n_clusters):\n",
    "            idx = np.where(cluster_batch == k)[0]\n",
    "            y_pred_idx, loss = classifiers[k].fit(X_batch[idx], y_batch[idx])\n",
    "#             print(\"F1 Cluster: \", k, f1_score(np.argmax(y_pred_idx, axis=1), y_batch[idx]))\n",
    "#             print(\"Cluster: \", k, len(idx))\n",
    "            new_y_batch.append(y_batch[idx])\n",
    "            y_pred.append(y_pred_idx)\n",
    "            epoch_loss += loss\n",
    "\n",
    "        y_pred = np.vstack(y_pred)\n",
    "        new_y_batch = np.hstack(new_y_batch)\n",
    "#         print(y_pred.shape, new_y_batch, len(X_batch))\n",
    "        f1 = f1_score(np.argmax(y_pred, axis=1), new_y_batch)\n",
    "        acc = roc_auc_score(y_batch, y_pred[:,1])\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_f1 += f1.item()\n",
    "\n",
    "    test_preds = []\n",
    "    test_loss = 0.0\n",
    "    new_y_test = []\n",
    "    \n",
    "    # Epoch Testing\n",
    "    for k in range(args.n_clusters):\n",
    "        classifiers[k].classifier.eval()\n",
    "        idx = np.where(cluster_id_test == k)[0]\n",
    "        latents_idx = latents_test[idx]\n",
    "        y_pred_idx = classifiers[k](latents_idx)\n",
    "        test_loss += nn.CrossEntropyLoss(reduction='mean')(y_pred_idx, torch.tensor(y_test[idx]).to(device))\n",
    "        test_preds.append(y_pred_idx.detach().numpy())\n",
    "        new_y_test.append(y_test[idx])\n",
    "\n",
    "    test_preds = np.vstack(test_preds)\n",
    "    new_y_test = np.hstack(new_y_test).reshape(-1)\n",
    "    test_f1 = f1_score(np.argmax(test_preds, axis=1), new_y_test)\n",
    "    test_acc = roc_auc_score(new_y_test, test_preds[:,1])\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {epoch_loss/len(train_loader):.5f} | Train F1: {epoch_f1/len(train_loader):.3f} | Train Acc: {epoch_acc/len(train_loader):.3f}| Test F1: {test_f1:.3f} | Test Acc: {test_acc:.3f} | Test Loss: {test_loss:.3f}')\n",
    "\n",
    "out = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id = model.clustering.update_assign(out.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Clustering No Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 0.02288 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.189 | Test Acc: 0.752 | Test Loss: 2.181\n",
      "Epoch 002: | Train Loss: 0.02270 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.189 | Test Acc: 0.768 | Test Loss: 2.165\n",
      "Epoch 003: | Train Loss: 0.02254 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.189 | Test Acc: 0.755 | Test Loss: 2.150\n",
      "Epoch 004: | Train Loss: 0.02238 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.253 | Test Acc: 0.755 | Test Loss: 2.134\n",
      "Epoch 005: | Train Loss: 0.02222 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.253 | Test Acc: 0.756 | Test Loss: 2.119\n",
      "Epoch 006: | Train Loss: 0.02206 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.757 | Test Loss: 2.104\n",
      "Epoch 007: | Train Loss: 0.02191 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.756 | Test Loss: 2.089\n",
      "Epoch 008: | Train Loss: 0.02175 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.756 | Test Loss: 2.074\n",
      "Epoch 009: | Train Loss: 0.02159 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.762 | Test Loss: 2.059\n",
      "Epoch 010: | Train Loss: 0.02144 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.762 | Test Loss: 2.044\n",
      "Epoch 011: | Train Loss: 0.02128 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.762 | Test Loss: 2.029\n",
      "Epoch 012: | Train Loss: 0.02113 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.771 | Test Loss: 2.015\n",
      "Epoch 013: | Train Loss: 0.02099 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.765 | Test Loss: 2.001\n",
      "Epoch 014: | Train Loss: 0.02084 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.762 | Test Loss: 1.987\n",
      "Epoch 015: | Train Loss: 0.02070 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.008 | Test Acc: 0.762 | Test Loss: 1.973\n",
      "Epoch 016: | Train Loss: 0.02055 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.004 | Test Acc: 0.763 | Test Loss: 1.958\n",
      "Epoch 017: | Train Loss: 0.02040 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.001 | Test Acc: 0.763 | Test Loss: 1.944\n",
      "Epoch 018: | Train Loss: 0.02026 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.762 | Test Loss: 1.930\n",
      "Epoch 019: | Train Loss: 0.02011 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.761 | Test Loss: 1.915\n",
      "Epoch 020: | Train Loss: 0.01996 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.761 | Test Loss: 1.901\n",
      "Epoch 021: | Train Loss: 0.01981 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.762 | Test Loss: 1.886\n",
      "Epoch 022: | Train Loss: 0.01966 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.763 | Test Loss: 1.871\n",
      "Epoch 023: | Train Loss: 0.01950 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.764 | Test Loss: 1.856\n",
      "Epoch 024: | Train Loss: 0.01935 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.765 | Test Loss: 1.841\n",
      "Epoch 025: | Train Loss: 0.01920 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.765 | Test Loss: 1.827\n",
      "Epoch 026: | Train Loss: 0.01905 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.766 | Test Loss: 1.812\n",
      "Epoch 027: | Train Loss: 0.01889 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.766 | Test Loss: 1.797\n",
      "Epoch 028: | Train Loss: 0.01874 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.767 | Test Loss: 1.782\n",
      "Epoch 029: | Train Loss: 0.01859 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.767 | Test Loss: 1.767\n",
      "Epoch 030: | Train Loss: 0.01844 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.767 | Test Loss: 1.753\n",
      "Epoch 031: | Train Loss: 0.01829 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.768 | Test Loss: 1.739\n",
      "Epoch 032: | Train Loss: 0.01814 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.768 | Test Loss: 1.724\n",
      "Epoch 033: | Train Loss: 0.01800 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.768 | Test Loss: 1.711\n",
      "Epoch 034: | Train Loss: 0.01785 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.769 | Test Loss: 1.697\n",
      "Epoch 035: | Train Loss: 0.01771 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.769 | Test Loss: 1.684\n",
      "Epoch 036: | Train Loss: 0.01758 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.769 | Test Loss: 1.671\n",
      "Epoch 037: | Train Loss: 0.01745 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.769 | Test Loss: 1.659\n",
      "Epoch 038: | Train Loss: 0.01732 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.770 | Test Loss: 1.647\n",
      "Epoch 039: | Train Loss: 0.01719 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.770 | Test Loss: 1.635\n",
      "Epoch 040: | Train Loss: 0.01708 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.770 | Test Loss: 1.624\n",
      "Epoch 041: | Train Loss: 0.01696 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.770 | Test Loss: 1.613\n",
      "Epoch 042: | Train Loss: 0.01685 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.770 | Test Loss: 1.603\n",
      "Epoch 043: | Train Loss: 0.01674 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.771 | Test Loss: 1.593\n",
      "Epoch 044: | Train Loss: 0.01664 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.771 | Test Loss: 1.583\n",
      "Epoch 045: | Train Loss: 0.01654 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.771 | Test Loss: 1.574\n",
      "Epoch 046: | Train Loss: 0.01645 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.771 | Test Loss: 1.565\n",
      "Epoch 047: | Train Loss: 0.01636 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.557\n",
      "Epoch 048: | Train Loss: 0.01627 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.549\n",
      "Epoch 049: | Train Loss: 0.01619 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.541\n",
      "Epoch 050: | Train Loss: 0.01611 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.534\n",
      "Epoch 051: | Train Loss: 0.01603 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.527\n",
      "Epoch 052: | Train Loss: 0.01596 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.520\n",
      "Epoch 053: | Train Loss: 0.01589 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.513\n",
      "Epoch 054: | Train Loss: 0.01582 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.507\n",
      "Epoch 055: | Train Loss: 0.01575 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.501\n",
      "Epoch 056: | Train Loss: 0.01569 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.496\n",
      "Epoch 057: | Train Loss: 0.01563 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.491\n",
      "Epoch 058: | Train Loss: 0.01558 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.486\n",
      "Epoch 059: | Train Loss: 0.01553 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.482\n",
      "Epoch 060: | Train Loss: 0.01548 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.477\n",
      "Epoch 061: | Train Loss: 0.01544 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.474\n",
      "Epoch 062: | Train Loss: 0.01540 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.470\n",
      "Epoch 063: | Train Loss: 0.01536 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.467\n",
      "Epoch 064: | Train Loss: 0.01533 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.465\n",
      "Epoch 065: | Train Loss: 0.01530 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.462\n",
      "Epoch 066: | Train Loss: 0.01527 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.460\n",
      "Epoch 067: | Train Loss: 0.01525 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 068: | Train Loss: 0.01522 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.457\n",
      "Epoch 069: | Train Loss: 0.01521 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.455\n",
      "Epoch 070: | Train Loss: 0.01519 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.454\n",
      "Epoch 071: | Train Loss: 0.01518 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.453\n",
      "Epoch 072: | Train Loss: 0.01517 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.453\n",
      "Epoch 073: | Train Loss: 0.01516 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.452\n",
      "Epoch 074: | Train Loss: 0.01515 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.451\n",
      "Epoch 075: | Train Loss: 0.01514 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.451\n",
      "Epoch 076: | Train Loss: 0.01514 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.451\n",
      "Epoch 077: | Train Loss: 0.01513 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 078: | Train Loss: 0.01513 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 079: | Train Loss: 0.01513 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 080: | Train Loss: 0.01513 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 081: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 082: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 083: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 084: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 085: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 086: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 087: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 088: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 089: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 090: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 091: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.776 | Test Loss: 1.450\n",
      "Epoch 092: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 093: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 094: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 095: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.776 | Test Loss: 1.450\n",
      "Epoch 096: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.776 | Test Loss: 1.450\n",
      "Epoch 097: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 098: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 099: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 100: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 101: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 102: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 103: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 104: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 105: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 106: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 107: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 108: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 109: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 110: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 111: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 112: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 113: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 114: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 115: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 116: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 117: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 118: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 119: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 120: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 121: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 122: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 123: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 124: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 125: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 126: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 127: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 128: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 129: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 130: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 131: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 132: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 133: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 134: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.449\n",
      "Epoch 135: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.449\n",
      "Epoch 136: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 138: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 139: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 140: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.771 | Test Loss: 1.448\n",
      "Epoch 141: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.771 | Test Loss: 1.448\n",
      "Epoch 142: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.771 | Test Loss: 1.448\n",
      "Epoch 143: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 144: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 145: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 146: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 147: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 148: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 149: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 150: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 151: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 152: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 153: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 154: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 155: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 156: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 157: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 158: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 159: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 160: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 161: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 162: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 163: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 164: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 165: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 166: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.446\n",
      "Epoch 167: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.446\n",
      "Epoch 168: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.446\n",
      "Epoch 169: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.446\n",
      "Epoch 170: | Train Loss: 0.01508 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.446\n",
      "Epoch 171: | Train Loss: 0.01508 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.446\n",
      "Epoch 172: | Train Loss: 0.01508 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.446\n",
      "Epoch 173: | Train Loss: 0.01508 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.446\n",
      "Epoch 174: | Train Loss: 0.01508 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.446\n",
      "Epoch 175: | Train Loss: 0.01508 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.445\n",
      "Epoch 176: | Train Loss: 0.01508 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.445\n",
      "Epoch 177: | Train Loss: 0.01507 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.445\n",
      "Epoch 178: | Train Loss: 0.01507 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.445\n",
      "Epoch 179: | Train Loss: 0.01507 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.445\n",
      "Epoch 180: | Train Loss: 0.01507 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.445\n",
      "Epoch 181: | Train Loss: 0.01507 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.444\n",
      "Epoch 182: | Train Loss: 0.01507 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.444\n",
      "Epoch 183: | Train Loss: 0.01506 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.444\n",
      "Epoch 184: | Train Loss: 0.01506 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.444\n",
      "Epoch 185: | Train Loss: 0.01506 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.444\n",
      "Epoch 186: | Train Loss: 0.01506 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.443\n",
      "Epoch 187: | Train Loss: 0.01505 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.443\n",
      "Epoch 188: | Train Loss: 0.01505 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.443\n",
      "Epoch 189: | Train Loss: 0.01505 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.443\n",
      "Epoch 190: | Train Loss: 0.01505 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.442\n",
      "Epoch 191: | Train Loss: 0.01505 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.442\n",
      "Epoch 192: | Train Loss: 0.01504 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.442\n",
      "Epoch 193: | Train Loss: 0.01504 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.442\n",
      "Epoch 194: | Train Loss: 0.01504 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.441\n",
      "Epoch 195: | Train Loss: 0.01503 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.441\n",
      "Epoch 196: | Train Loss: 0.01503 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.441\n",
      "Epoch 197: | Train Loss: 0.01503 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.441\n",
      "Epoch 198: | Train Loss: 0.01503 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.440\n",
      "Epoch 199: | Train Loss: 0.01502 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.440\n",
      "Epoch 200: | Train Loss: 0.01502 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.440\n",
      "Epoch 201: | Train Loss: 0.01502 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.439\n",
      "Epoch 202: | Train Loss: 0.01501 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.439\n",
      "Epoch 203: | Train Loss: 0.01501 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204: | Train Loss: 0.01500 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.438\n",
      "Epoch 205: | Train Loss: 0.01500 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.438\n",
      "Epoch 206: | Train Loss: 0.01500 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.437\n",
      "Epoch 207: | Train Loss: 0.01499 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.437\n",
      "Epoch 208: | Train Loss: 0.01499 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.437\n",
      "Epoch 209: | Train Loss: 0.01498 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.436\n",
      "Epoch 210: | Train Loss: 0.01498 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.436\n",
      "Epoch 211: | Train Loss: 0.01498 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.435\n",
      "Epoch 212: | Train Loss: 0.01497 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.434\n",
      "Epoch 213: | Train Loss: 0.01496 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.434\n",
      "Epoch 214: | Train Loss: 0.01495 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.433\n",
      "Epoch 215: | Train Loss: 0.01495 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.433\n",
      "Epoch 216: | Train Loss: 0.01494 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.432\n",
      "Epoch 217: | Train Loss: 0.01494 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.431\n",
      "Epoch 218: | Train Loss: 0.01493 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.008 | Test Acc: 0.774 | Test Loss: 1.431\n",
      "Epoch 219: | Train Loss: 0.01492 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.006 | Test Acc: 0.774 | Test Loss: 1.430\n",
      "Epoch 220: | Train Loss: 0.01492 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.010 | Test Acc: 0.774 | Test Loss: 1.429\n",
      "Epoch 221: | Train Loss: 0.01491 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.022 | Test Acc: 0.774 | Test Loss: 1.429\n",
      "Epoch 222: | Train Loss: 0.01490 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.016 | Test Acc: 0.774 | Test Loss: 1.428\n",
      "Epoch 223: | Train Loss: 0.01490 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.043 | Test Acc: 0.774 | Test Loss: 1.427\n",
      "Epoch 224: | Train Loss: 0.01489 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.052 | Test Acc: 0.774 | Test Loss: 1.427\n",
      "Epoch 225: | Train Loss: 0.01488 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.048 | Test Acc: 0.774 | Test Loss: 1.426\n",
      "Epoch 226: | Train Loss: 0.01487 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.080 | Test Acc: 0.774 | Test Loss: 1.425\n",
      "Epoch 227: | Train Loss: 0.01487 | Train F1: 0.001 | Train Acc: 0.008| Test F1: 0.071 | Test Acc: 0.774 | Test Loss: 1.424\n",
      "Epoch 228: | Train Loss: 0.01486 | Train F1: 0.001 | Train Acc: 0.008| Test F1: 0.108 | Test Acc: 0.774 | Test Loss: 1.424\n",
      "Epoch 229: | Train Loss: 0.01485 | Train F1: 0.001 | Train Acc: 0.008| Test F1: 0.115 | Test Acc: 0.774 | Test Loss: 1.423\n",
      "Epoch 230: | Train Loss: 0.01484 | Train F1: 0.001 | Train Acc: 0.008| Test F1: 0.114 | Test Acc: 0.774 | Test Loss: 1.422\n",
      "Epoch 231: | Train Loss: 0.01483 | Train F1: 0.001 | Train Acc: 0.008| Test F1: 0.162 | Test Acc: 0.774 | Test Loss: 1.421\n",
      "Epoch 232: | Train Loss: 0.01483 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.129 | Test Acc: 0.774 | Test Loss: 1.421\n",
      "Epoch 233: | Train Loss: 0.01482 | Train F1: 0.001 | Train Acc: 0.008| Test F1: 0.197 | Test Acc: 0.774 | Test Loss: 1.420\n",
      "Epoch 234: | Train Loss: 0.01481 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.162 | Test Acc: 0.775 | Test Loss: 1.419\n",
      "Epoch 235: | Train Loss: 0.01480 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.200 | Test Acc: 0.775 | Test Loss: 1.418\n",
      "Epoch 236: | Train Loss: 0.01479 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.218 | Test Acc: 0.775 | Test Loss: 1.417\n",
      "Epoch 237: | Train Loss: 0.01478 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.197 | Test Acc: 0.775 | Test Loss: 1.417\n",
      "Epoch 238: | Train Loss: 0.01478 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.237 | Test Acc: 0.775 | Test Loss: 1.416\n",
      "Epoch 239: | Train Loss: 0.01477 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.220 | Test Acc: 0.775 | Test Loss: 1.415\n",
      "Epoch 240: | Train Loss: 0.01476 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.238 | Test Acc: 0.775 | Test Loss: 1.414\n",
      "Epoch 241: | Train Loss: 0.01475 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.238 | Test Acc: 0.775 | Test Loss: 1.414\n",
      "Epoch 242: | Train Loss: 0.01474 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.234 | Test Acc: 0.775 | Test Loss: 1.413\n",
      "Epoch 243: | Train Loss: 0.01473 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.245 | Test Acc: 0.775 | Test Loss: 1.412\n",
      "Epoch 244: | Train Loss: 0.01473 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.238 | Test Acc: 0.775 | Test Loss: 1.411\n",
      "Epoch 245: | Train Loss: 0.01472 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.248 | Test Acc: 0.775 | Test Loss: 1.410\n",
      "Epoch 246: | Train Loss: 0.01471 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.244 | Test Acc: 0.775 | Test Loss: 1.410\n",
      "Epoch 247: | Train Loss: 0.01470 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.247 | Test Acc: 0.775 | Test Loss: 1.409\n",
      "Epoch 248: | Train Loss: 0.01469 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.252 | Test Acc: 0.775 | Test Loss: 1.408\n",
      "Epoch 249: | Train Loss: 0.01469 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.248 | Test Acc: 0.775 | Test Loss: 1.407\n",
      "Epoch 250: | Train Loss: 0.01468 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.256 | Test Acc: 0.775 | Test Loss: 1.407\n",
      "Epoch 251: | Train Loss: 0.01467 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.250 | Test Acc: 0.775 | Test Loss: 1.406\n",
      "Epoch 252: | Train Loss: 0.01466 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.255 | Test Acc: 0.775 | Test Loss: 1.405\n",
      "Epoch 253: | Train Loss: 0.01466 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.257 | Test Acc: 0.775 | Test Loss: 1.404\n",
      "Epoch 254: | Train Loss: 0.01465 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.253 | Test Acc: 0.775 | Test Loss: 1.404\n",
      "Epoch 255: | Train Loss: 0.01464 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.266 | Test Acc: 0.775 | Test Loss: 1.403\n",
      "Epoch 256: | Train Loss: 0.01463 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.253 | Test Acc: 0.775 | Test Loss: 1.402\n",
      "Epoch 257: | Train Loss: 0.01463 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.268 | Test Acc: 0.775 | Test Loss: 1.402\n",
      "Epoch 258: | Train Loss: 0.01462 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.260 | Test Acc: 0.775 | Test Loss: 1.401\n",
      "Epoch 259: | Train Loss: 0.01461 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.266 | Test Acc: 0.775 | Test Loss: 1.400\n",
      "Epoch 260: | Train Loss: 0.01460 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.272 | Test Acc: 0.776 | Test Loss: 1.399\n",
      "Epoch 261: | Train Loss: 0.01460 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.263 | Test Acc: 0.776 | Test Loss: 1.399\n",
      "Epoch 262: | Train Loss: 0.01459 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.281 | Test Acc: 0.776 | Test Loss: 1.398\n",
      "Epoch 263: | Train Loss: 0.01458 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.264 | Test Acc: 0.776 | Test Loss: 1.397\n",
      "Epoch 264: | Train Loss: 0.01458 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.281 | Test Acc: 0.776 | Test Loss: 1.397\n",
      "Epoch 265: | Train Loss: 0.01457 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.276 | Test Acc: 0.776 | Test Loss: 1.396\n",
      "Epoch 266: | Train Loss: 0.01456 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.276 | Test Acc: 0.776 | Test Loss: 1.395\n",
      "Epoch 267: | Train Loss: 0.01456 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.286 | Test Acc: 0.776 | Test Loss: 1.395\n",
      "Epoch 268: | Train Loss: 0.01455 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.272 | Test Acc: 0.776 | Test Loss: 1.394\n",
      "Epoch 269: | Train Loss: 0.01454 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.291 | Test Acc: 0.776 | Test Loss: 1.394\n",
      "Epoch 270: | Train Loss: 0.01454 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.279 | Test Acc: 0.776 | Test Loss: 1.393\n",
      "Epoch 271: | Train Loss: 0.01453 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.287 | Test Acc: 0.776 | Test Loss: 1.392\n",
      "Epoch 272: | Train Loss: 0.01453 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.290 | Test Acc: 0.776 | Test Loss: 1.392\n",
      "Epoch 273: | Train Loss: 0.01452 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.281 | Test Acc: 0.776 | Test Loss: 1.391\n",
      "Epoch 274: | Train Loss: 0.01451 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.295 | Test Acc: 0.776 | Test Loss: 1.391\n",
      "Epoch 275: | Train Loss: 0.01451 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.281 | Test Acc: 0.776 | Test Loss: 1.390\n",
      "Epoch 276: | Train Loss: 0.01450 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.297 | Test Acc: 0.776 | Test Loss: 1.390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277: | Train Loss: 0.01450 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.288 | Test Acc: 0.776 | Test Loss: 1.389\n",
      "Epoch 278: | Train Loss: 0.01449 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.293 | Test Acc: 0.776 | Test Loss: 1.389\n",
      "Epoch 279: | Train Loss: 0.01449 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.295 | Test Acc: 0.776 | Test Loss: 1.388\n",
      "Epoch 280: | Train Loss: 0.01448 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.290 | Test Acc: 0.776 | Test Loss: 1.388\n",
      "Epoch 281: | Train Loss: 0.01448 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.304 | Test Acc: 0.776 | Test Loss: 1.387\n",
      "Epoch 282: | Train Loss: 0.01447 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.289 | Test Acc: 0.776 | Test Loss: 1.387\n",
      "Epoch 283: | Train Loss: 0.01447 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.312 | Test Acc: 0.776 | Test Loss: 1.386\n",
      "Epoch 284: | Train Loss: 0.01446 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.293 | Test Acc: 0.777 | Test Loss: 1.386\n",
      "Epoch 285: | Train Loss: 0.01446 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.303 | Test Acc: 0.777 | Test Loss: 1.385\n",
      "Epoch 286: | Train Loss: 0.01445 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.299 | Test Acc: 0.777 | Test Loss: 1.385\n",
      "Epoch 287: | Train Loss: 0.01445 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.299 | Test Acc: 0.777 | Test Loss: 1.384\n",
      "Epoch 288: | Train Loss: 0.01444 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.313 | Test Acc: 0.777 | Test Loss: 1.384\n",
      "Epoch 289: | Train Loss: 0.01444 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.296 | Test Acc: 0.777 | Test Loss: 1.384\n",
      "Epoch 290: | Train Loss: 0.01444 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.322 | Test Acc: 0.777 | Test Loss: 1.383\n",
      "Epoch 291: | Train Loss: 0.01443 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.295 | Test Acc: 0.777 | Test Loss: 1.383\n",
      "Epoch 292: | Train Loss: 0.01443 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.324 | Test Acc: 0.777 | Test Loss: 1.383\n",
      "Epoch 293: | Train Loss: 0.01442 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.299 | Test Acc: 0.777 | Test Loss: 1.382\n",
      "Epoch 294: | Train Loss: 0.01442 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.317 | Test Acc: 0.777 | Test Loss: 1.382\n",
      "Epoch 295: | Train Loss: 0.01442 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.318 | Test Acc: 0.777 | Test Loss: 1.381\n",
      "Epoch 296: | Train Loss: 0.01441 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.304 | Test Acc: 0.777 | Test Loss: 1.381\n",
      "Epoch 297: | Train Loss: 0.01441 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.332 | Test Acc: 0.777 | Test Loss: 1.381\n",
      "Epoch 298: | Train Loss: 0.01441 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.302 | Test Acc: 0.777 | Test Loss: 1.380\n",
      "Epoch 299: | Train Loss: 0.01440 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.332 | Test Acc: 0.777 | Test Loss: 1.380\n",
      "Epoch 300: | Train Loss: 0.01440 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.312 | Test Acc: 0.777 | Test Loss: 1.380\n",
      "Epoch 301: | Train Loss: 0.01440 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.323 | Test Acc: 0.777 | Test Loss: 1.380\n",
      "Epoch 302: | Train Loss: 0.01439 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.327 | Test Acc: 0.777 | Test Loss: 1.379\n",
      "Epoch 303: | Train Loss: 0.01439 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.313 | Test Acc: 0.777 | Test Loss: 1.379\n",
      "Epoch 304: | Train Loss: 0.01439 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.338 | Test Acc: 0.777 | Test Loss: 1.379\n",
      "Epoch 305: | Train Loss: 0.01439 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.311 | Test Acc: 0.777 | Test Loss: 1.378\n",
      "Epoch 306: | Train Loss: 0.01438 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.340 | Test Acc: 0.777 | Test Loss: 1.378\n",
      "Epoch 307: | Train Loss: 0.01438 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.319 | Test Acc: 0.777 | Test Loss: 1.378\n",
      "Epoch 308: | Train Loss: 0.01438 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.334 | Test Acc: 0.777 | Test Loss: 1.378\n",
      "Epoch 309: | Train Loss: 0.01437 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.329 | Test Acc: 0.777 | Test Loss: 1.378\n",
      "Epoch 310: | Train Loss: 0.01437 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.322 | Test Acc: 0.777 | Test Loss: 1.377\n",
      "Epoch 311: | Train Loss: 0.01437 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.340 | Test Acc: 0.777 | Test Loss: 1.377\n",
      "Epoch 312: | Train Loss: 0.01437 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.320 | Test Acc: 0.777 | Test Loss: 1.377\n",
      "Epoch 313: | Train Loss: 0.01437 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.347 | Test Acc: 0.777 | Test Loss: 1.377\n",
      "Epoch 314: | Train Loss: 0.01436 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.320 | Test Acc: 0.777 | Test Loss: 1.377\n",
      "Epoch 315: | Train Loss: 0.01436 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.347 | Test Acc: 0.777 | Test Loss: 1.377\n",
      "Epoch 316: | Train Loss: 0.01436 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.322 | Test Acc: 0.777 | Test Loss: 1.376\n",
      "Epoch 317: | Train Loss: 0.01436 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.343 | Test Acc: 0.777 | Test Loss: 1.376\n",
      "Epoch 318: | Train Loss: 0.01436 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.336 | Test Acc: 0.777 | Test Loss: 1.376\n",
      "Epoch 319: | Train Loss: 0.01435 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.336 | Test Acc: 0.777 | Test Loss: 1.376\n",
      "Epoch 320: | Train Loss: 0.01435 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.345 | Test Acc: 0.777 | Test Loss: 1.376\n",
      "Epoch 321: | Train Loss: 0.01435 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.328 | Test Acc: 0.778 | Test Loss: 1.375\n",
      "Epoch 322: | Train Loss: 0.01435 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.351 | Test Acc: 0.778 | Test Loss: 1.375\n",
      "Epoch 323: | Train Loss: 0.01435 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.324 | Test Acc: 0.778 | Test Loss: 1.375\n",
      "Epoch 324: | Train Loss: 0.01435 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.356 | Test Acc: 0.778 | Test Loss: 1.375\n",
      "Epoch 325: | Train Loss: 0.01435 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.326 | Test Acc: 0.778 | Test Loss: 1.375\n",
      "Epoch 326: | Train Loss: 0.01434 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.352 | Test Acc: 0.778 | Test Loss: 1.375\n",
      "Epoch 327: | Train Loss: 0.01434 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.336 | Test Acc: 0.778 | Test Loss: 1.375\n",
      "Epoch 328: | Train Loss: 0.01434 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.345 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 329: | Train Loss: 0.01434 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.345 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 330: | Train Loss: 0.01434 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.336 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 331: | Train Loss: 0.01434 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 332: | Train Loss: 0.01434 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.333 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 333: | Train Loss: 0.01433 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 334: | Train Loss: 0.01433 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.332 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 335: | Train Loss: 0.01433 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 336: | Train Loss: 0.01433 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.336 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 337: | Train Loss: 0.01433 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.355 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 338: | Train Loss: 0.01433 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.347 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 339: | Train Loss: 0.01433 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.347 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 340: | Train Loss: 0.01433 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 341: | Train Loss: 0.01433 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.342 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 342: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 343: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.337 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 344: | Train Loss: 0.01432 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 345: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.338 | Test Acc: 0.778 | Test Loss: 1.373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 346: | Train Loss: 0.01432 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 347: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.346 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 348: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 349: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.353 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 350: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.353 | Test Acc: 0.778 | Test Loss: 1.372\n",
      "Epoch 351: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.778 | Test Loss: 1.372\n",
      "Epoch 352: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.350 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 353: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 354: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.345 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 355: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.364 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 356: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.344 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 357: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.363 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 358: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.351 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 359: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 360: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.353 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 361: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 362: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.357 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 363: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 364: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 365: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.353 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 366: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.363 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 367: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.351 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 368: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 369: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.348 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 370: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 371: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.345 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 372: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 373: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.348 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 374: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 375: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 376: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.361 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 377: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 378: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.357 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 379: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 380: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.352 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 381: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 382: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.351 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 383: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 384: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 385: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.364 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 386: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.357 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 387: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 388: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.363 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 389: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 390: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 391: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 392: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 393: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.352 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 394: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 395: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.352 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 396: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 397: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.355 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 398: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 399: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.357 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 400: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.365 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 401: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 402: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.363 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 403: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.362 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 404: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 405: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.365 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 406: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.779 | Test Loss: 1.370\n",
      "Epoch 407: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 408: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.355 | Test Acc: 0.779 | Test Loss: 1.370\n",
      "Epoch 409: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 410: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.355 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 411: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 412: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.351 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 413: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.779 | Test Loss: 1.371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.350 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 415: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 416: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.348 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 417: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.780 | Test Loss: 1.371\n",
      "Epoch 418: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.352 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 419: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 420: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.362 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 421: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 422: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 423: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 424: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 425: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.355 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 426: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 427: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 428: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 429: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 430: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 431: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 432: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.356 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 433: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 434: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 435: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 436: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.365 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 437: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.365 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 438: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 439: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 440: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 441: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.357 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 442: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 443: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 444: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 445: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 446: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 447: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.365 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 448: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 449: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 450: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.365 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 451: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 452: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.362 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 453: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 454: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 455: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 456: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 457: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 458: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 459: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 460: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.370\n",
      "Epoch 461: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 462: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.370\n",
      "Epoch 463: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 464: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 465: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 466: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 467: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.781 | Test Loss: 1.370\n",
      "Epoch 468: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 469: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.370\n",
      "Epoch 470: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 471: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.370\n",
      "Epoch 472: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 473: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 474: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.357 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 475: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 476: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.357 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 477: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 478: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 479: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 480: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 481: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 482: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 483: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 484: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.363 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 485: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 486: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.364 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 487: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 488: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 489: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 490: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 491: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 492: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 493: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 494: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 495: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 496: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.364 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 497: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 498: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 499: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 500: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.356 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 501: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 502: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 503: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.380 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 504: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 505: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.380 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 506: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 507: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 508: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 509: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 510: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 511: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 512: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 513: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 514: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 515: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 516: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 517: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.364 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 518: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 519: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 520: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 521: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 522: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 523: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 524: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 525: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 526: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 527: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 528: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.781 | Test Loss: 1.368\n",
      "Epoch 529: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 530: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 531: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 532: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 533: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 534: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 535: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 536: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 537: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 538: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 539: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 540: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 541: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 542: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 543: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 544: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 545: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 546: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 547: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 548: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 549: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 550: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 551: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 552: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 553: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 554: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 555: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 556: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 557: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.365 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 558: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 559: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.362 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 560: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 561: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.362 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 562: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 563: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.362 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 564: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 565: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 566: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 567: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 568: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 569: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 570: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 571: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 572: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 573: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 574: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 575: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 576: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 577: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 578: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 579: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 580: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 581: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 582: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 583: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 584: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 585: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 586: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 587: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.380 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 588: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.355 | Test Acc: 0.783 | Test Loss: 1.368\n",
      "Epoch 589: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.387 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 590: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.351 | Test Acc: 0.783 | Test Loss: 1.368\n",
      "Epoch 591: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.388 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 592: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.353 | Test Acc: 0.783 | Test Loss: 1.368\n",
      "Epoch 593: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.381 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 594: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.782 | Test Loss: 1.367\n",
      "Epoch 595: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.782 | Test Loss: 1.367\n",
      "Epoch 596: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.379 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 597: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.356 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 598: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.382 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 599: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.361 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 600: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.367\n",
      "Epoch 601: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.782 | Test Loss: 1.367\n",
      "Epoch 602: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 603: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.379 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 604: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.362 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 605: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 606: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 607: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 608: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 609: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 610: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.378 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 611: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 612: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 613: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 614: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 615: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 616: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 617: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 618: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 619: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 620: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 621: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 622: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 623: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.783 | Test Loss: 1.367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 624: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 625: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 626: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 627: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 628: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 629: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 630: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 631: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 632: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 633: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 634: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 635: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 636: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 637: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 638: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 639: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 640: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 641: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 642: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 643: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 644: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 645: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 646: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 647: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 648: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 649: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 650: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 651: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 652: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 653: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 654: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 655: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 656: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 657: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 658: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 659: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 660: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 661: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 662: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 663: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 664: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 665: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 666: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 667: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 668: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 669: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 670: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.378 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 671: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 672: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.378 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 673: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 674: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.379 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 675: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 676: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.382 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 677: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.362 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 678: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.387 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 679: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.356 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 680: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.393 | Test Acc: 0.784 | Test Loss: 1.367\n",
      "Epoch 681: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.351 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 682: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.391 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 683: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 684: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.380 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 685: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 686: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 687: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.383 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 688: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 689: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.383 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 690: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 691: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 692: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.380 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 693: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.784 | Test Loss: 1.366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 694: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.382 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 695: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 696: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.380 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 697: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 698: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 699: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.380 | Test Acc: 0.784 | Test Loss: 1.366\n"
     ]
    }
   ],
   "source": [
    "# Training separate models\n",
    "args.lr = 0.002\n",
    "classifiers = [NNClassifier(args) for _ in range(args.n_clusters)]\n",
    "# optimizers = [torch.optim.Adam(classifiers[i].classifier.parameters(), lr=args.lr) for i in range(args.n_clusters)]\n",
    "EPOCHS = 700\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "\n",
    "latents_X = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id_train = model.clustering.update_assign(latents_X.cpu().detach().numpy())\n",
    "\n",
    "X_latents_data_loader = list(zip(latents_X, cluster_id_train, y_train))\n",
    "train_loader_latents = torch.utils.data.DataLoader(X_latents_data_loader,\n",
    "    batch_size=1024, shuffle=False)\n",
    "\n",
    "latents_test = model.autoencoder(torch.FloatTensor(np.array(X_test)).to(args.device), latent=True)\n",
    "\n",
    "cluster_id_test = model.clustering.update_assign(latents_test.cpu().detach().numpy())\n",
    "\n",
    "# plot(latents_X, y_train, latents_test, y_test)\n",
    "\n",
    "for e in range(1, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    alpha = (1-e/EPOCHS)\n",
    "    acc = 0\n",
    "\n",
    "    new_y = []\n",
    "    y_pred = []\n",
    "    for k in range(args.n_clusters):\n",
    "#         print(\"Training on cluster: \", k)\n",
    "        idx = np.where(cluster_id_train == k)[0]\n",
    "        y_pred_idx, loss = classifiers[k].fit(latents_X[idx], torch.tensor(y_train[idx]).to(device))\n",
    "        y_pred.append(y_pred_idx)\n",
    "        new_y.append(y_train[idx])\n",
    "        epoch_loss += loss\n",
    "#         print(\"Cluster: \", k, \"AUC: \", roc_auc_score(y_train[idx], y_pred_idx[:,1])\\\n",
    "#               , \"loss: \", loss, \"N =\", len(idx))\n",
    "\n",
    "    y_pred = np.vstack(y_pred)\n",
    "    new_y = np.hstack(new_y)\n",
    "\n",
    "    f1 = f1_score(np.argmax(y_pred, axis=1), new_y)\n",
    "    acc = roc_auc_score(new_y, y_pred[:,1])\n",
    "    epoch_f1 += f1.item()\n",
    "    epoch_acc += acc.item()\n",
    "\n",
    "    test_preds = []\n",
    "    test_loss = 0.0\n",
    "    new_y_test = []\n",
    "    for k in range(args.n_clusters):\n",
    "        classifiers[k].classifier.eval()\n",
    "        idx = np.where(cluster_id_test == k)[0]\n",
    "        latents_idx = latents_test[idx]\n",
    "        y_pred_idx = classifiers[k](latents_idx)\n",
    "        test_loss += nn.CrossEntropyLoss(reduction='mean')(y_pred_idx, torch.tensor(y_test[idx]).to(device))\n",
    "        test_preds.append(y_pred_idx.detach().numpy())\n",
    "        new_y_test.append(y_test[idx])\n",
    "#         print(\"Cluster: \", k, \"F1: \", roc_auc_score(y_test[idx], y_pred_idx[:,1].detach().numpy()),\\\n",
    "#               \"loss: \", loss, \"N =\", len(idx))\n",
    "\n",
    "\n",
    "    test_preds = np.vstack(test_preds)\n",
    "    new_y_test = np.hstack(new_y_test).reshape(-1)\n",
    "    test_f1 = f1_score(np.argmax(test_preds, axis=1), new_y_test)\n",
    "    test_acc = roc_auc_score(new_y_test, test_preds[:,1])\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {epoch_loss/len(train_loader):.5f} | Train F1: {epoch_f1/len(train_loader):.3f} | Train Acc: {epoch_acc/len(train_loader):.3f}| Test F1: {test_f1:.3f} | Test Acc: {test_acc:.3f} | Test Loss: {test_loss:.3f}')\n",
    "\n",
    "out = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id = model.clustering.update_assign(out.cpu().detach().numpy())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 0.01571 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.506 | Test Loss: 0.492\n",
      "Epoch 002: | Train Loss: 0.01367 | Train F1: 0.000 | Train Acc: 0.013| Test F1: 0.000 | Test Acc: 0.472 | Test Loss: 0.444\n",
      "Epoch 003: | Train Loss: 0.01234 | Train F1: 0.000 | Train Acc: 0.012| Test F1: 0.000 | Test Acc: 0.510 | Test Loss: 0.414\n",
      "Epoch 004: | Train Loss: 0.01150 | Train F1: 0.000 | Train Acc: 0.013| Test F1: 0.000 | Test Acc: 0.540 | Test Loss: 0.415\n",
      "Epoch 005: | Train Loss: 0.01153 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.524 | Test Loss: 0.432\n",
      "Epoch 006: | Train Loss: 0.01197 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.493 | Test Loss: 0.437\n",
      "Epoch 007: | Train Loss: 0.01206 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.479 | Test Loss: 0.430\n",
      "Epoch 008: | Train Loss: 0.01185 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.478 | Test Loss: 0.421\n",
      "Epoch 009: | Train Loss: 0.01160 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.479 | Test Loss: 0.415\n",
      "Epoch 010: | Train Loss: 0.01143 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.478 | Test Loss: 0.413\n",
      "Epoch 011: | Train Loss: 0.01138 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.479 | Test Loss: 0.414\n",
      "Epoch 012: | Train Loss: 0.01140 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.480 | Test Loss: 0.415\n",
      "Epoch 013: | Train Loss: 0.01145 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.480 | Test Loss: 0.417\n",
      "Epoch 014: | Train Loss: 0.01150 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.479 | Test Loss: 0.417\n",
      "Epoch 015: | Train Loss: 0.01152 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.479 | Test Loss: 0.417\n",
      "Epoch 016: | Train Loss: 0.01152 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.480 | Test Loss: 0.416\n",
      "Epoch 017: | Train Loss: 0.01150 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.481 | Test Loss: 0.415\n",
      "Epoch 018: | Train Loss: 0.01146 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.486 | Test Loss: 0.413\n",
      "Epoch 019: | Train Loss: 0.01142 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.498 | Test Loss: 0.412\n",
      "Epoch 020: | Train Loss: 0.01138 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.516 | Test Loss: 0.411\n",
      "Epoch 021: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.527 | Test Loss: 0.410\n",
      "Epoch 022: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.534 | Test Loss: 0.410\n",
      "Epoch 023: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.538 | Test Loss: 0.411\n",
      "Epoch 024: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.540 | Test Loss: 0.411\n",
      "Epoch 025: | Train Loss: 0.01136 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.542 | Test Loss: 0.412\n",
      "Epoch 026: | Train Loss: 0.01138 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.544 | Test Loss: 0.412\n",
      "Epoch 027: | Train Loss: 0.01139 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.545 | Test Loss: 0.412\n",
      "Epoch 028: | Train Loss: 0.01139 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.545 | Test Loss: 0.412\n",
      "Epoch 029: | Train Loss: 0.01138 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.546 | Test Loss: 0.411\n",
      "Epoch 030: | Train Loss: 0.01137 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.547 | Test Loss: 0.411\n",
      "Epoch 031: | Train Loss: 0.01136 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.548 | Test Loss: 0.410\n",
      "Epoch 032: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.548 | Test Loss: 0.410\n",
      "Epoch 033: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.549 | Test Loss: 0.410\n",
      "Epoch 034: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.550 | Test Loss: 0.410\n",
      "Epoch 035: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.550 | Test Loss: 0.410\n",
      "Epoch 036: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.551 | Test Loss: 0.410\n",
      "Epoch 037: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.552 | Test Loss: 0.410\n",
      "Epoch 038: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.552 | Test Loss: 0.410\n",
      "Epoch 039: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.553 | Test Loss: 0.410\n",
      "Epoch 040: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.553 | Test Loss: 0.410\n",
      "Epoch 041: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.554 | Test Loss: 0.410\n",
      "Epoch 042: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.555 | Test Loss: 0.410\n",
      "Epoch 043: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.555 | Test Loss: 0.410\n",
      "Epoch 044: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.556 | Test Loss: 0.410\n",
      "Epoch 045: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.556 | Test Loss: 0.410\n",
      "Epoch 046: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.557 | Test Loss: 0.410\n",
      "Epoch 047: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.557 | Test Loss: 0.410\n",
      "Epoch 048: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.558 | Test Loss: 0.410\n",
      "Epoch 049: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.558 | Test Loss: 0.410\n",
      "Epoch 050: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.559 | Test Loss: 0.410\n",
      "Epoch 051: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.559 | Test Loss: 0.410\n",
      "Epoch 052: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.559 | Test Loss: 0.410\n",
      "Epoch 053: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.560 | Test Loss: 0.410\n",
      "Epoch 054: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.560 | Test Loss: 0.410\n",
      "Epoch 055: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.561 | Test Loss: 0.410\n",
      "Epoch 056: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.561 | Test Loss: 0.410\n",
      "Epoch 057: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.562 | Test Loss: 0.410\n",
      "Epoch 058: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.562 | Test Loss: 0.410\n",
      "Epoch 059: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.562 | Test Loss: 0.410\n",
      "Epoch 060: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.563 | Test Loss: 0.410\n",
      "Epoch 061: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.563 | Test Loss: 0.410\n",
      "Epoch 062: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.564 | Test Loss: 0.410\n",
      "Epoch 063: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.564 | Test Loss: 0.410\n",
      "Epoch 064: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.564 | Test Loss: 0.410\n",
      "Epoch 065: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.565 | Test Loss: 0.410\n",
      "Epoch 066: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.565 | Test Loss: 0.410\n",
      "Epoch 067: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.566 | Test Loss: 0.410\n",
      "Epoch 068: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.566 | Test Loss: 0.410\n",
      "Epoch 069: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.566 | Test Loss: 0.410\n",
      "Epoch 070: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.567 | Test Loss: 0.410\n",
      "Epoch 071: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.567 | Test Loss: 0.410\n",
      "Epoch 072: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.567 | Test Loss: 0.410\n",
      "Epoch 073: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.567 | Test Loss: 0.410\n",
      "Epoch 074: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.567 | Test Loss: 0.410\n",
      "Epoch 075: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.567 | Test Loss: 0.410\n",
      "Epoch 076: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.567 | Test Loss: 0.410\n",
      "Epoch 077: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.568 | Test Loss: 0.410\n",
      "Epoch 078: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.568 | Test Loss: 0.410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 079: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.568 | Test Loss: 0.410\n",
      "Epoch 080: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.568 | Test Loss: 0.410\n",
      "Epoch 081: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.569 | Test Loss: 0.410\n",
      "Epoch 082: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.570 | Test Loss: 0.410\n",
      "Epoch 083: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.570 | Test Loss: 0.410\n",
      "Epoch 084: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.571 | Test Loss: 0.410\n",
      "Epoch 085: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.571 | Test Loss: 0.410\n",
      "Epoch 086: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.571 | Test Loss: 0.410\n",
      "Epoch 087: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.572 | Test Loss: 0.409\n",
      "Epoch 088: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.572 | Test Loss: 0.409\n",
      "Epoch 089: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.572 | Test Loss: 0.409\n",
      "Epoch 090: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.572 | Test Loss: 0.409\n",
      "Epoch 091: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.572 | Test Loss: 0.409\n",
      "Epoch 092: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.572 | Test Loss: 0.409\n",
      "Epoch 093: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.573 | Test Loss: 0.409\n",
      "Epoch 094: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.573 | Test Loss: 0.409\n",
      "Epoch 095: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.573 | Test Loss: 0.409\n",
      "Epoch 096: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.573 | Test Loss: 0.409\n",
      "Epoch 097: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.573 | Test Loss: 0.409\n",
      "Epoch 098: | Train Loss: 0.01131 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.573 | Test Loss: 0.409\n",
      "Epoch 099: | Train Loss: 0.01131 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.574 | Test Loss: 0.409\n",
      "Epoch 100: | Train Loss: 0.01131 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.574 | Test Loss: 0.409\n",
      "Epoch 101: | Train Loss: 0.01131 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.574 | Test Loss: 0.409\n",
      "Epoch 102: | Train Loss: 0.01131 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.574 | Test Loss: 0.409\n",
      "Epoch 103: | Train Loss: 0.01131 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.574 | Test Loss: 0.409\n",
      "Epoch 104: | Train Loss: 0.01131 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.575 | Test Loss: 0.409\n",
      "Epoch 105: | Train Loss: 0.01130 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.575 | Test Loss: 0.409\n",
      "Epoch 106: | Train Loss: 0.01130 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.575 | Test Loss: 0.409\n",
      "Epoch 107: | Train Loss: 0.01130 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.575 | Test Loss: 0.409\n",
      "Epoch 108: | Train Loss: 0.01130 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.575 | Test Loss: 0.409\n",
      "Epoch 109: | Train Loss: 0.01130 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.575 | Test Loss: 0.409\n",
      "Epoch 110: | Train Loss: 0.01129 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.576 | Test Loss: 0.408\n",
      "Epoch 111: | Train Loss: 0.01129 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.576 | Test Loss: 0.408\n",
      "Epoch 112: | Train Loss: 0.01129 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.576 | Test Loss: 0.408\n",
      "Epoch 113: | Train Loss: 0.01129 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.576 | Test Loss: 0.408\n",
      "Epoch 114: | Train Loss: 0.01128 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.577 | Test Loss: 0.408\n",
      "Epoch 115: | Train Loss: 0.01128 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.577 | Test Loss: 0.408\n",
      "Epoch 116: | Train Loss: 0.01128 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.577 | Test Loss: 0.408\n",
      "Epoch 117: | Train Loss: 0.01127 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.577 | Test Loss: 0.408\n",
      "Epoch 118: | Train Loss: 0.01127 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.578 | Test Loss: 0.407\n",
      "Epoch 119: | Train Loss: 0.01127 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.578 | Test Loss: 0.407\n",
      "Epoch 120: | Train Loss: 0.01126 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.578 | Test Loss: 0.407\n",
      "Epoch 121: | Train Loss: 0.01126 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.579 | Test Loss: 0.407\n",
      "Epoch 122: | Train Loss: 0.01125 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.579 | Test Loss: 0.407\n",
      "Epoch 123: | Train Loss: 0.01125 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.580 | Test Loss: 0.407\n",
      "Epoch 124: | Train Loss: 0.01124 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.580 | Test Loss: 0.406\n",
      "Epoch 125: | Train Loss: 0.01124 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.582 | Test Loss: 0.406\n",
      "Epoch 126: | Train Loss: 0.01123 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.584 | Test Loss: 0.406\n",
      "Epoch 127: | Train Loss: 0.01123 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.588 | Test Loss: 0.406\n",
      "Epoch 128: | Train Loss: 0.01122 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.593 | Test Loss: 0.405\n",
      "Epoch 129: | Train Loss: 0.01121 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.598 | Test Loss: 0.405\n",
      "Epoch 130: | Train Loss: 0.01121 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.603 | Test Loss: 0.405\n",
      "Epoch 131: | Train Loss: 0.01120 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.610 | Test Loss: 0.404\n",
      "Epoch 132: | Train Loss: 0.01119 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.617 | Test Loss: 0.404\n",
      "Epoch 133: | Train Loss: 0.01118 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.622 | Test Loss: 0.404\n",
      "Epoch 134: | Train Loss: 0.01118 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.625 | Test Loss: 0.403\n",
      "Epoch 135: | Train Loss: 0.01117 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.626 | Test Loss: 0.403\n",
      "Epoch 136: | Train Loss: 0.01116 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.628 | Test Loss: 0.403\n",
      "Epoch 137: | Train Loss: 0.01115 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.631 | Test Loss: 0.402\n",
      "Epoch 138: | Train Loss: 0.01114 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.636 | Test Loss: 0.402\n",
      "Epoch 139: | Train Loss: 0.01113 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.637 | Test Loss: 0.401\n",
      "Epoch 140: | Train Loss: 0.01112 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.637 | Test Loss: 0.401\n",
      "Epoch 141: | Train Loss: 0.01111 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.640 | Test Loss: 0.401\n",
      "Epoch 142: | Train Loss: 0.01109 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.643 | Test Loss: 0.400\n",
      "Epoch 143: | Train Loss: 0.01108 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.644 | Test Loss: 0.400\n",
      "Epoch 144: | Train Loss: 0.01107 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.646 | Test Loss: 0.399\n",
      "Epoch 145: | Train Loss: 0.01106 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.647 | Test Loss: 0.399\n",
      "Epoch 146: | Train Loss: 0.01104 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.649 | Test Loss: 0.398\n",
      "Epoch 147: | Train Loss: 0.01103 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.651 | Test Loss: 0.398\n",
      "Epoch 148: | Train Loss: 0.01102 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.652 | Test Loss: 0.397\n",
      "Epoch 149: | Train Loss: 0.01100 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.654 | Test Loss: 0.396\n",
      "Epoch 150: | Train Loss: 0.01099 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.659 | Test Loss: 0.396\n",
      "Epoch 151: | Train Loss: 0.01097 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.653 | Test Loss: 0.395\n",
      "Epoch 152: | Train Loss: 0.01096 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.663 | Test Loss: 0.395\n",
      "Epoch 153: | Train Loss: 0.01094 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.657 | Test Loss: 0.394\n",
      "Epoch 154: | Train Loss: 0.01093 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.662 | Test Loss: 0.394\n",
      "Epoch 155: | Train Loss: 0.01091 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.667 | Test Loss: 0.393\n",
      "Epoch 156: | Train Loss: 0.01089 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.654 | Test Loss: 0.393\n",
      "Epoch 157: | Train Loss: 0.01088 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.672 | Test Loss: 0.392\n",
      "Epoch 158: | Train Loss: 0.01087 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.638 | Test Loss: 0.394\n",
      "Epoch 159: | Train Loss: 0.01089 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.660 | Test Loss: 0.392\n",
      "Epoch 160: | Train Loss: 0.01090 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.648 | Test Loss: 0.391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161: | Train Loss: 0.01083 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.645 | Test Loss: 0.391\n",
      "Epoch 162: | Train Loss: 0.01083 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.661 | Test Loss: 0.390\n",
      "Epoch 163: | Train Loss: 0.01085 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.662 | Test Loss: 0.389\n",
      "Epoch 164: | Train Loss: 0.01079 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.644 | Test Loss: 0.391\n",
      "Epoch 165: | Train Loss: 0.01081 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.667 | Test Loss: 0.389\n",
      "Epoch 166: | Train Loss: 0.01080 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.674 | Test Loss: 0.388\n",
      "Epoch 167: | Train Loss: 0.01076 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.646 | Test Loss: 0.390\n",
      "Epoch 168: | Train Loss: 0.01079 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.675 | Test Loss: 0.388\n",
      "Epoch 169: | Train Loss: 0.01076 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.673 | Test Loss: 0.387\n",
      "Epoch 170: | Train Loss: 0.01076 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.649 | Test Loss: 0.389\n",
      "Epoch 171: | Train Loss: 0.01077 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.673 | Test Loss: 0.387\n",
      "Epoch 172: | Train Loss: 0.01073 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.670 | Test Loss: 0.387\n",
      "Epoch 173: | Train Loss: 0.01075 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.658 | Test Loss: 0.388\n",
      "Epoch 174: | Train Loss: 0.01074 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.666 | Test Loss: 0.387\n",
      "Epoch 175: | Train Loss: 0.01073 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.672 | Test Loss: 0.387\n",
      "Epoch 176: | Train Loss: 0.01074 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.667 | Test Loss: 0.387\n",
      "Epoch 177: | Train Loss: 0.01072 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.662 | Test Loss: 0.387\n",
      "Epoch 178: | Train Loss: 0.01072 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.677 | Test Loss: 0.386\n",
      "Epoch 179: | Train Loss: 0.01072 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.674 | Test Loss: 0.386\n",
      "Epoch 180: | Train Loss: 0.01070 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.663 | Test Loss: 0.387\n",
      "Epoch 181: | Train Loss: 0.01071 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.678 | Test Loss: 0.386\n",
      "Epoch 182: | Train Loss: 0.01071 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.677 | Test Loss: 0.386\n",
      "Epoch 183: | Train Loss: 0.01070 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.666 | Test Loss: 0.386\n",
      "Epoch 184: | Train Loss: 0.01070 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.678 | Test Loss: 0.385\n",
      "Epoch 185: | Train Loss: 0.01069 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.678 | Test Loss: 0.385\n",
      "Epoch 186: | Train Loss: 0.01069 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.668 | Test Loss: 0.386\n",
      "Epoch 187: | Train Loss: 0.01069 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.677 | Test Loss: 0.385\n",
      "Epoch 188: | Train Loss: 0.01068 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.678 | Test Loss: 0.385\n",
      "Epoch 189: | Train Loss: 0.01068 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.669 | Test Loss: 0.385\n",
      "Epoch 190: | Train Loss: 0.01068 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.678 | Test Loss: 0.384\n",
      "Epoch 191: | Train Loss: 0.01067 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.678 | Test Loss: 0.384\n",
      "Epoch 192: | Train Loss: 0.01067 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.670 | Test Loss: 0.385\n",
      "Epoch 193: | Train Loss: 0.01067 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.678 | Test Loss: 0.384\n",
      "Epoch 194: | Train Loss: 0.01067 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.679 | Test Loss: 0.384\n",
      "Epoch 195: | Train Loss: 0.01066 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.673 | Test Loss: 0.384\n",
      "Epoch 196: | Train Loss: 0.01066 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.678 | Test Loss: 0.384\n",
      "Epoch 197: | Train Loss: 0.01066 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.680 | Test Loss: 0.384\n",
      "Epoch 198: | Train Loss: 0.01065 | Train F1: 0.000 | Train Acc: 0.019| Test F1: 0.000 | Test Acc: 0.677 | Test Loss: 0.384\n",
      "Epoch 199: | Train Loss: 0.01065 | Train F1: 0.000 | Train Acc: 0.019| Test F1: 0.000 | Test Acc: 0.679 | Test Loss: 0.384\n",
      "Epoch 200: | Train Loss: 0.01065 | Train F1: 0.000 | Train Acc: 0.019| Test F1: 0.123 | Test Acc: 0.680 | Test Loss: 0.384\n",
      "Epoch 201: | Train Loss: 0.01065 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.678 | Test Loss: 0.384\n",
      "Epoch 202: | Train Loss: 0.01064 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.680 | Test Loss: 0.384\n",
      "Epoch 203: | Train Loss: 0.01064 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.679 | Test Loss: 0.384\n",
      "Epoch 204: | Train Loss: 0.01064 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.679 | Test Loss: 0.383\n",
      "Epoch 205: | Train Loss: 0.01063 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.681 | Test Loss: 0.383\n",
      "Epoch 206: | Train Loss: 0.01063 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.680 | Test Loss: 0.383\n",
      "Epoch 207: | Train Loss: 0.01063 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.679 | Test Loss: 0.383\n",
      "Epoch 208: | Train Loss: 0.01062 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.681 | Test Loss: 0.383\n",
      "Epoch 209: | Train Loss: 0.01062 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.680 | Test Loss: 0.383\n",
      "Epoch 210: | Train Loss: 0.01062 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.123 | Test Acc: 0.679 | Test Loss: 0.383\n",
      "Epoch 211: | Train Loss: 0.01062 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.683 | Test Loss: 0.383\n",
      "Epoch 212: | Train Loss: 0.01061 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.679 | Test Loss: 0.383\n",
      "Epoch 213: | Train Loss: 0.01061 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.682 | Test Loss: 0.383\n",
      "Epoch 214: | Train Loss: 0.01061 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.681 | Test Loss: 0.383\n",
      "Epoch 215: | Train Loss: 0.01061 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.681 | Test Loss: 0.383\n",
      "Epoch 216: | Train Loss: 0.01060 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.682 | Test Loss: 0.382\n",
      "Epoch 217: | Train Loss: 0.01060 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.123 | Test Acc: 0.683 | Test Loss: 0.382\n",
      "Epoch 218: | Train Loss: 0.01060 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.681 | Test Loss: 0.382\n",
      "Epoch 219: | Train Loss: 0.01059 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.685 | Test Loss: 0.382\n",
      "Epoch 220: | Train Loss: 0.01059 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.679 | Test Loss: 0.382\n",
      "Epoch 221: | Train Loss: 0.01059 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.687 | Test Loss: 0.382\n",
      "Epoch 222: | Train Loss: 0.01061 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.669 | Test Loss: 0.386\n",
      "Epoch 223: | Train Loss: 0.01066 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.687 | Test Loss: 0.382\n",
      "Epoch 224: | Train Loss: 0.01062 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.111 | Test Acc: 0.687 | Test Loss: 0.381\n",
      "Epoch 225: | Train Loss: 0.01058 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.672 | Test Loss: 0.384\n",
      "Epoch 226: | Train Loss: 0.01061 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.686 | Test Loss: 0.382\n",
      "Epoch 227: | Train Loss: 0.01058 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.115 | Test Acc: 0.688 | Test Loss: 0.381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228: | Train Loss: 0.01059 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.674 | Test Loss: 0.383\n",
      "Epoch 229: | Train Loss: 0.01059 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.683 | Test Loss: 0.382\n",
      "Epoch 230: | Train Loss: 0.01057 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.114 | Test Acc: 0.689 | Test Loss: 0.381\n",
      "Epoch 231: | Train Loss: 0.01059 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.677 | Test Loss: 0.382\n",
      "Epoch 232: | Train Loss: 0.01058 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.683 | Test Loss: 0.382\n",
      "Epoch 233: | Train Loss: 0.01057 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.689 | Test Loss: 0.381\n",
      "Epoch 234: | Train Loss: 0.01059 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.682 | Test Loss: 0.381\n",
      "Epoch 235: | Train Loss: 0.01056 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.682 | Test Loss: 0.382\n",
      "Epoch 236: | Train Loss: 0.01057 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.689 | Test Loss: 0.381\n",
      "Epoch 237: | Train Loss: 0.01057 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.688 | Test Loss: 0.381\n",
      "Epoch 238: | Train Loss: 0.01056 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.681 | Test Loss: 0.382\n",
      "Epoch 239: | Train Loss: 0.01057 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.688 | Test Loss: 0.381\n",
      "Epoch 240: | Train Loss: 0.01055 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.690 | Test Loss: 0.380\n",
      "Epoch 241: | Train Loss: 0.01055 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.679 | Test Loss: 0.382\n",
      "Epoch 242: | Train Loss: 0.01056 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.689 | Test Loss: 0.380\n",
      "Epoch 243: | Train Loss: 0.01055 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.690 | Test Loss: 0.380\n",
      "Epoch 244: | Train Loss: 0.01055 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.683 | Test Loss: 0.381\n",
      "Epoch 245: | Train Loss: 0.01055 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.689 | Test Loss: 0.380\n",
      "Epoch 246: | Train Loss: 0.01054 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.690 | Test Loss: 0.380\n",
      "Epoch 247: | Train Loss: 0.01054 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.685 | Test Loss: 0.381\n",
      "Epoch 248: | Train Loss: 0.01055 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.689 | Test Loss: 0.380\n",
      "Epoch 249: | Train Loss: 0.01054 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.692 | Test Loss: 0.380\n",
      "Epoch 250: | Train Loss: 0.01054 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.136 | Test Acc: 0.683 | Test Loss: 0.381\n",
      "Epoch 251: | Train Loss: 0.01054 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.692 | Test Loss: 0.380\n",
      "Epoch 252: | Train Loss: 0.01053 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.691 | Test Loss: 0.380\n",
      "Epoch 253: | Train Loss: 0.01053 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.686 | Test Loss: 0.381\n",
      "Epoch 254: | Train Loss: 0.01053 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.693 | Test Loss: 0.379\n",
      "Epoch 255: | Train Loss: 0.01053 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.689 | Test Loss: 0.380\n",
      "Epoch 256: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.691 | Test Loss: 0.380\n",
      "Epoch 257: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.136 | Test Acc: 0.693 | Test Loss: 0.379\n",
      "Epoch 258: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.690 | Test Loss: 0.380\n",
      "Epoch 259: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.136 | Test Acc: 0.692 | Test Loss: 0.379\n",
      "Epoch 260: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.694 | Test Loss: 0.379\n",
      "Epoch 261: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.144 | Test Acc: 0.687 | Test Loss: 0.380\n",
      "Epoch 262: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.697 | Test Loss: 0.379\n",
      "Epoch 263: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.144 | Test Acc: 0.689 | Test Loss: 0.380\n",
      "Epoch 264: | Train Loss: 0.01051 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.695 | Test Loss: 0.379\n",
      "Epoch 265: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.695 | Test Loss: 0.379\n",
      "Epoch 266: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.140 | Test Acc: 0.690 | Test Loss: 0.380\n",
      "Epoch 267: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.698 | Test Loss: 0.378\n",
      "Epoch 268: | Train Loss: 0.01051 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.144 | Test Acc: 0.688 | Test Loss: 0.380\n",
      "Epoch 269: | Train Loss: 0.01051 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.699 | Test Loss: 0.378\n",
      "Epoch 270: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.132 | Test Acc: 0.692 | Test Loss: 0.379\n",
      "Epoch 271: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.144 | Test Acc: 0.697 | Test Loss: 0.378\n",
      "Epoch 272: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.696 | Test Loss: 0.380\n",
      "Epoch 273: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.147 | Test Acc: 0.697 | Test Loss: 0.380\n",
      "Epoch 274: | Train Loss: 0.01053 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.118 | Test Acc: 0.695 | Test Loss: 0.382\n",
      "Epoch 275: | Train Loss: 0.01056 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.147 | Test Acc: 0.699 | Test Loss: 0.379\n",
      "Epoch 276: | Train Loss: 0.01054 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.694 | Test Loss: 0.380\n",
      "Epoch 277: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.698 | Test Loss: 0.378\n",
      "Epoch 278: | Train Loss: 0.01048 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.144 | Test Acc: 0.700 | Test Loss: 0.378\n",
      "Epoch 279: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.694 | Test Loss: 0.381\n",
      "Epoch 280: | Train Loss: 0.01053 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.144 | Test Acc: 0.700 | Test Loss: 0.378\n",
      "Epoch 281: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.699 | Test Loss: 0.378\n",
      "Epoch 282: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.695 | Test Loss: 0.379\n",
      "Epoch 283: | Train Loss: 0.01048 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.139 | Test Acc: 0.702 | Test Loss: 0.378\n",
      "Epoch 284: | Train Loss: 0.01049 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.699 | Test Loss: 0.379\n",
      "Epoch 285: | Train Loss: 0.01049 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.147 | Test Acc: 0.698 | Test Loss: 0.378\n",
      "Epoch 286: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.703 | Test Loss: 0.377\n",
      "Epoch 287: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.136 | Test Acc: 0.695 | Test Loss: 0.379\n",
      "Epoch 288: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.143 | Test Acc: 0.702 | Test Loss: 0.377\n",
      "Epoch 289: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.704 | Test Loss: 0.378\n",
      "Epoch 290: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.147 | Test Acc: 0.697 | Test Loss: 0.378\n",
      "Epoch 291: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.704 | Test Loss: 0.377\n",
      "Epoch 292: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.702 | Test Loss: 0.377\n",
      "Epoch 293: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.146 | Test Acc: 0.700 | Test Loss: 0.378\n",
      "Epoch 294: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.705 | Test Loss: 0.377\n",
      "Epoch 295: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.143 | Test Acc: 0.703 | Test Loss: 0.377\n",
      "Epoch 296: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.144 | Test Acc: 0.701 | Test Loss: 0.377\n",
      "Epoch 297: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.706 | Test Loss: 0.376\n",
      "Epoch 298: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.147 | Test Acc: 0.700 | Test Loss: 0.377\n",
      "Epoch 299: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.132 | Test Acc: 0.707 | Test Loss: 0.376\n",
      "Epoch 300: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.128 | Test Acc: 0.703 | Test Loss: 0.378\n",
      "Epoch 301: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.147 | Test Acc: 0.709 | Test Loss: 0.377\n",
      "Epoch 302: | Train Loss: 0.01049 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.691 | Test Loss: 0.386\n",
      "Epoch 303: | Train Loss: 0.01062 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.148 | Test Acc: 0.708 | Test Loss: 0.381\n",
      "Epoch 304: | Train Loss: 0.01064 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.130 | Test Acc: 0.694 | Test Loss: 0.383\n",
      "Epoch 305: | Train Loss: 0.01059 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.160 | Test Acc: 0.698 | Test Loss: 0.377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 306: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.111 | Test Acc: 0.691 | Test Loss: 0.381\n",
      "Epoch 307: | Train Loss: 0.01070 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.189 | Test Acc: 0.655 | Test Loss: 0.391\n",
      "Epoch 308: | Train Loss: 0.01076 | Train F1: 0.005 | Train Acc: 0.018| Test F1: 0.100 | Test Acc: 0.708 | Test Loss: 0.380\n",
      "Epoch 309: | Train Loss: 0.01053 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.114 | Test Acc: 0.699 | Test Loss: 0.383\n",
      "Epoch 310: | Train Loss: 0.01069 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.132 | Test Acc: 0.691 | Test Loss: 0.380\n",
      "Epoch 311: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.128 | Test Acc: 0.672 | Test Loss: 0.386\n",
      "Epoch 312: | Train Loss: 0.01060 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.105 | Test Acc: 0.710 | Test Loss: 0.378\n",
      "Epoch 313: | Train Loss: 0.01046 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.109 | Test Acc: 0.707 | Test Loss: 0.380\n",
      "Epoch 314: | Train Loss: 0.01057 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.131 | Test Acc: 0.692 | Test Loss: 0.381\n",
      "Epoch 315: | Train Loss: 0.01048 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.681 | Test Loss: 0.384\n",
      "Epoch 316: | Train Loss: 0.01053 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.127 | Test Acc: 0.709 | Test Loss: 0.378\n",
      "Epoch 317: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.124 | Test Acc: 0.708 | Test Loss: 0.378\n",
      "Epoch 318: | Train Loss: 0.01050 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.128 | Test Acc: 0.699 | Test Loss: 0.379\n",
      "Epoch 319: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.143 | Test Acc: 0.688 | Test Loss: 0.381\n",
      "Epoch 320: | Train Loss: 0.01048 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.710 | Test Loss: 0.376\n",
      "Epoch 321: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.709 | Test Loss: 0.376\n",
      "Epoch 322: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.140 | Test Acc: 0.701 | Test Loss: 0.377\n",
      "Epoch 323: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.145 | Test Acc: 0.698 | Test Loss: 0.378\n",
      "Epoch 324: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.130 | Test Acc: 0.710 | Test Loss: 0.375\n",
      "Epoch 325: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.149 | Test Acc: 0.710 | Test Loss: 0.375\n",
      "Epoch 326: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.149 | Test Acc: 0.703 | Test Loss: 0.376\n",
      "Epoch 327: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.138 | Test Acc: 0.706 | Test Loss: 0.376\n",
      "Epoch 328: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.153 | Test Acc: 0.709 | Test Loss: 0.375\n",
      "Epoch 329: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.141 | Test Acc: 0.710 | Test Loss: 0.374\n",
      "Epoch 330: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.142 | Test Acc: 0.706 | Test Loss: 0.376\n",
      "Epoch 331: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.149 | Test Acc: 0.704 | Test Loss: 0.376\n",
      "Epoch 332: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.711 | Test Loss: 0.374\n",
      "Epoch 333: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.138 | Test Acc: 0.709 | Test Loss: 0.374\n",
      "Epoch 334: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.141 | Test Acc: 0.703 | Test Loss: 0.376\n",
      "Epoch 335: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.709 | Test Loss: 0.375\n",
      "Epoch 336: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.710 | Test Loss: 0.374\n",
      "Epoch 337: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.706 | Test Loss: 0.375\n",
      "Epoch 338: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.707 | Test Loss: 0.375\n",
      "Epoch 339: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.130 | Test Acc: 0.710 | Test Loss: 0.374\n",
      "Epoch 340: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.130 | Test Acc: 0.709 | Test Loss: 0.374\n",
      "Epoch 341: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.707 | Test Loss: 0.375\n",
      "Epoch 342: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.130 | Test Acc: 0.710 | Test Loss: 0.374\n",
      "Epoch 343: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.711 | Test Loss: 0.374\n",
      "Epoch 344: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.708 | Test Loss: 0.374\n",
      "Epoch 345: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.710 | Test Loss: 0.374\n",
      "Epoch 346: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.712 | Test Loss: 0.373\n",
      "Epoch 347: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.130 | Test Acc: 0.708 | Test Loss: 0.374\n",
      "Epoch 348: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.711 | Test Loss: 0.373\n",
      "Epoch 349: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.712 | Test Loss: 0.373\n",
      "Epoch 350: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.138 | Test Acc: 0.709 | Test Loss: 0.373\n",
      "Epoch 351: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.713 | Test Loss: 0.372\n",
      "Epoch 352: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.712 | Test Loss: 0.373\n",
      "Epoch 353: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.712 | Test Loss: 0.372\n",
      "Epoch 354: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.713 | Test Loss: 0.372\n",
      "Epoch 355: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.713 | Test Loss: 0.372\n",
      "Epoch 356: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.712 | Test Loss: 0.372\n",
      "Epoch 357: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.715 | Test Loss: 0.371\n",
      "Epoch 358: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.708 | Test Loss: 0.373\n",
      "Epoch 359: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.717 | Test Loss: 0.372\n",
      "Epoch 360: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.130 | Test Acc: 0.693 | Test Loss: 0.382\n",
      "Epoch 361: | Train Loss: 0.01055 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.145 | Test Acc: 0.717 | Test Loss: 0.379\n",
      "Epoch 362: | Train Loss: 0.01063 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.107 | Test Acc: 0.690 | Test Loss: 0.393\n",
      "Epoch 363: | Train Loss: 0.01085 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.163 | Test Acc: 0.708 | Test Loss: 0.374\n",
      "Epoch 364: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.095 | Test Acc: 0.698 | Test Loss: 0.381\n",
      "Epoch 365: | Train Loss: 0.01073 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.130 | Test Acc: 0.666 | Test Loss: 0.395\n",
      "Epoch 366: | Train Loss: 0.01087 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.160 | Test Acc: 0.679 | Test Loss: 0.381\n",
      "Epoch 367: | Train Loss: 0.01051 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.084 | Test Acc: 0.681 | Test Loss: 0.394\n",
      "Epoch 368: | Train Loss: 0.01112 | Train F1: 0.003 | Train Acc: 0.018| Test F1: 0.103 | Test Acc: 0.714 | Test Loss: 0.373\n",
      "Epoch 369: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.638 | Test Loss: 0.398\n",
      "Epoch 370: | Train Loss: 0.01091 | Train F1: 0.004 | Train Acc: 0.018| Test F1: 0.088 | Test Acc: 0.715 | Test Loss: 0.374\n",
      "Epoch 371: | Train Loss: 0.01039 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.079 | Test Acc: 0.696 | Test Loss: 0.384\n",
      "Epoch 372: | Train Loss: 0.01077 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.123 | Test Acc: 0.715 | Test Loss: 0.375\n",
      "Epoch 373: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.662 | Test Loss: 0.387\n",
      "Epoch 374: | Train Loss: 0.01063 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.084 | Test Acc: 0.693 | Test Loss: 0.382\n",
      "Epoch 375: | Train Loss: 0.01053 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.083 | Test Acc: 0.714 | Test Loss: 0.375\n",
      "Epoch 376: | Train Loss: 0.01046 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.107 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 377: | Train Loss: 0.01057 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.111 | Test Acc: 0.702 | Test Loss: 0.376\n",
      "Epoch 378: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.108 | Test Acc: 0.689 | Test Loss: 0.383\n",
      "Epoch 379: | Train Loss: 0.01054 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.111 | Test Acc: 0.705 | Test Loss: 0.376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.122 | Test Acc: 0.715 | Test Loss: 0.377\n",
      "Epoch 381: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.107 | Test Acc: 0.714 | Test Loss: 0.375\n",
      "Epoch 382: | Train Loss: 0.01042 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.112 | Test Acc: 0.706 | Test Loss: 0.377\n",
      "Epoch 383: | Train Loss: 0.01040 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.690 | Test Loss: 0.380\n",
      "Epoch 384: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.707 | Test Loss: 0.375\n",
      "Epoch 385: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.111 | Test Acc: 0.712 | Test Loss: 0.375\n",
      "Epoch 386: | Train Loss: 0.01043 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.123 | Test Acc: 0.714 | Test Loss: 0.374\n",
      "Epoch 387: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.142 | Test Acc: 0.702 | Test Loss: 0.376\n",
      "Epoch 388: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.701 | Test Loss: 0.377\n",
      "Epoch 389: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.123 | Test Acc: 0.712 | Test Loss: 0.374\n",
      "Epoch 390: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.714 | Test Loss: 0.375\n",
      "Epoch 391: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.713 | Test Loss: 0.374\n",
      "Epoch 392: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.709 | Test Loss: 0.375\n",
      "Epoch 393: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.705 | Test Loss: 0.376\n",
      "Epoch 394: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.141 | Test Acc: 0.711 | Test Loss: 0.374\n",
      "Epoch 395: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.714 | Test Loss: 0.373\n",
      "Epoch 396: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.713 | Test Loss: 0.373\n",
      "Epoch 397: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.705 | Test Loss: 0.375\n",
      "Epoch 398: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.709 | Test Loss: 0.374\n",
      "Epoch 399: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.133 | Test Acc: 0.714 | Test Loss: 0.372\n",
      "Epoch 400: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.714 | Test Loss: 0.373\n",
      "Epoch 401: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.711 | Test Loss: 0.373\n",
      "Epoch 402: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.709 | Test Loss: 0.374\n",
      "Epoch 403: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.711 | Test Loss: 0.373\n",
      "Epoch 404: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.715 | Test Loss: 0.372\n",
      "Epoch 405: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.714 | Test Loss: 0.372\n",
      "Epoch 406: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.709 | Test Loss: 0.373\n",
      "Epoch 407: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.711 | Test Loss: 0.373\n",
      "Epoch 408: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.715 | Test Loss: 0.372\n",
      "Epoch 409: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.715 | Test Loss: 0.372\n",
      "Epoch 410: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.712 | Test Loss: 0.373\n",
      "Epoch 411: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.712 | Test Loss: 0.373\n",
      "Epoch 412: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.715 | Test Loss: 0.372\n",
      "Epoch 413: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.716 | Test Loss: 0.372\n",
      "Epoch 414: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.714 | Test Loss: 0.372\n",
      "Epoch 415: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.712 | Test Loss: 0.372\n",
      "Epoch 416: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.716 | Test Loss: 0.371\n",
      "Epoch 417: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.717 | Test Loss: 0.371\n",
      "Epoch 418: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.714 | Test Loss: 0.372\n",
      "Epoch 419: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.714 | Test Loss: 0.371\n",
      "Epoch 420: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.717 | Test Loss: 0.371\n",
      "Epoch 421: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.717 | Test Loss: 0.371\n",
      "Epoch 422: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.715 | Test Loss: 0.371\n",
      "Epoch 423: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.716 | Test Loss: 0.371\n",
      "Epoch 424: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.718 | Test Loss: 0.370\n",
      "Epoch 425: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.717 | Test Loss: 0.370\n",
      "Epoch 426: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.716 | Test Loss: 0.370\n",
      "Epoch 427: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.718 | Test Loss: 0.370\n",
      "Epoch 428: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.718 | Test Loss: 0.370\n",
      "Epoch 429: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.717 | Test Loss: 0.370\n",
      "Epoch 430: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.718 | Test Loss: 0.370\n",
      "Epoch 431: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.719 | Test Loss: 0.370\n",
      "Epoch 432: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.718 | Test Loss: 0.370\n",
      "Epoch 433: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.719 | Test Loss: 0.370\n",
      "Epoch 434: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.720 | Test Loss: 0.369\n",
      "Epoch 435: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.718 | Test Loss: 0.370\n",
      "Epoch 436: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.719 | Test Loss: 0.369\n",
      "Epoch 437: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.721 | Test Loss: 0.369\n",
      "Epoch 438: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.719 | Test Loss: 0.369\n",
      "Epoch 439: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.720 | Test Loss: 0.369\n",
      "Epoch 440: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.721 | Test Loss: 0.369\n",
      "Epoch 441: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.720 | Test Loss: 0.369\n",
      "Epoch 442: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.722 | Test Loss: 0.369\n",
      "Epoch 443: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.721 | Test Loss: 0.369\n",
      "Epoch 444: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.721 | Test Loss: 0.369\n",
      "Epoch 445: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.722 | Test Loss: 0.368\n",
      "Epoch 446: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.721 | Test Loss: 0.369\n",
      "Epoch 447: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.723 | Test Loss: 0.368\n",
      "Epoch 448: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.722 | Test Loss: 0.368\n",
      "Epoch 449: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.723 | Test Loss: 0.368\n",
      "Epoch 450: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.723 | Test Loss: 0.368\n",
      "Epoch 451: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.723 | Test Loss: 0.368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 452: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.723 | Test Loss: 0.368\n",
      "Epoch 453: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.724 | Test Loss: 0.368\n",
      "Epoch 454: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.723 | Test Loss: 0.368\n",
      "Epoch 455: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.724 | Test Loss: 0.368\n",
      "Epoch 456: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.723 | Test Loss: 0.369\n",
      "Epoch 457: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.724 | Test Loss: 0.369\n",
      "Epoch 458: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.724 | Test Loss: 0.370\n",
      "Epoch 459: | Train Loss: 0.01029 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.725 | Test Loss: 0.370\n",
      "Epoch 460: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.722 | Test Loss: 0.373\n",
      "Epoch 461: | Train Loss: 0.01036 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.726 | Test Loss: 0.371\n",
      "Epoch 462: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.718 | Test Loss: 0.375\n",
      "Epoch 463: | Train Loss: 0.01041 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.727 | Test Loss: 0.370\n",
      "Epoch 464: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.718 | Test Loss: 0.371\n",
      "Epoch 465: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 466: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.725 | Test Loss: 0.368\n",
      "Epoch 467: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.722 | Test Loss: 0.370\n",
      "Epoch 468: | Train Loss: 0.01028 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.728 | Test Loss: 0.369\n",
      "Epoch 469: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.719 | Test Loss: 0.371\n",
      "Epoch 470: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.728 | Test Loss: 0.368\n",
      "Epoch 471: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.726 | Test Loss: 0.368\n",
      "Epoch 472: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.723 | Test Loss: 0.368\n",
      "Epoch 473: | Train Loss: 0.01024 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.728 | Test Loss: 0.368\n",
      "Epoch 474: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.722 | Test Loss: 0.370\n",
      "Epoch 475: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.727 | Test Loss: 0.368\n",
      "Epoch 476: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.726 | Test Loss: 0.368\n",
      "Epoch 477: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.724 | Test Loss: 0.368\n",
      "Epoch 478: | Train Loss: 0.01024 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 479: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.723 | Test Loss: 0.368\n",
      "Epoch 480: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 481: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.725 | Test Loss: 0.368\n",
      "Epoch 482: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.726 | Test Loss: 0.368\n",
      "Epoch 483: | Train Loss: 0.01024 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.727 | Test Loss: 0.368\n",
      "Epoch 484: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.724 | Test Loss: 0.368\n",
      "Epoch 485: | Train Loss: 0.01024 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.728 | Test Loss: 0.368\n",
      "Epoch 486: | Train Loss: 0.01025 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.724 | Test Loss: 0.368\n",
      "Epoch 487: | Train Loss: 0.01024 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.728 | Test Loss: 0.367\n",
      "Epoch 488: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.726 | Test Loss: 0.368\n",
      "Epoch 489: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.727 | Test Loss: 0.368\n",
      "Epoch 490: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.729 | Test Loss: 0.367\n",
      "Epoch 491: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.724 | Test Loss: 0.368\n",
      "Epoch 492: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.730 | Test Loss: 0.367\n",
      "Epoch 493: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.721 | Test Loss: 0.369\n",
      "Epoch 494: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.730 | Test Loss: 0.367\n",
      "Epoch 495: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.719 | Test Loss: 0.371\n",
      "Epoch 496: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.731 | Test Loss: 0.368\n",
      "Epoch 497: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.720 | Test Loss: 0.374\n",
      "Epoch 498: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.731 | Test Loss: 0.371\n",
      "Epoch 499: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.728 | Test Loss: 0.375\n",
      "Epoch 500: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.170 | Test Acc: 0.725 | Test Loss: 0.373\n",
      "Epoch 501: | Train Loss: 0.01037 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.729 | Test Loss: 0.371\n",
      "Epoch 502: | Train Loss: 0.01034 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 503: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.726 | Test Loss: 0.368\n",
      "Epoch 504: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.730 | Test Loss: 0.370\n",
      "Epoch 505: | Train Loss: 0.01031 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.729 | Test Loss: 0.370\n",
      "Epoch 506: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.722 | Test Loss: 0.370\n",
      "Epoch 507: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.730 | Test Loss: 0.367\n",
      "Epoch 508: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.720 | Test Loss: 0.369\n",
      "Epoch 509: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.730 | Test Loss: 0.367\n",
      "Epoch 510: | Train Loss: 0.01021 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.731 | Test Loss: 0.367\n",
      "Epoch 511: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.715 | Test Loss: 0.371\n",
      "Epoch 512: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.731 | Test Loss: 0.367\n",
      "Epoch 513: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.726 | Test Loss: 0.368\n",
      "Epoch 514: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 515: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.731 | Test Loss: 0.366\n",
      "Epoch 516: | Train Loss: 0.01022 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.723 | Test Loss: 0.368\n",
      "Epoch 517: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.732 | Test Loss: 0.367\n",
      "Epoch 518: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.730 | Test Loss: 0.368\n",
      "Epoch 519: | Train Loss: 0.01022 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.730 | Test Loss: 0.367\n",
      "Epoch 520: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.730 | Test Loss: 0.367\n",
      "Epoch 521: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.732 | Test Loss: 0.366\n",
      "Epoch 522: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.726 | Test Loss: 0.367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 523: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.732 | Test Loss: 0.366\n",
      "Epoch 524: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.729 | Test Loss: 0.366\n",
      "Epoch 525: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.730 | Test Loss: 0.366\n",
      "Epoch 526: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.732 | Test Loss: 0.365\n",
      "Epoch 527: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.725 | Test Loss: 0.367\n",
      "Epoch 528: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.733 | Test Loss: 0.365\n",
      "Epoch 529: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.724 | Test Loss: 0.368\n",
      "Epoch 530: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.733 | Test Loss: 0.366\n",
      "Epoch 531: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.721 | Test Loss: 0.372\n",
      "Epoch 532: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.734 | Test Loss: 0.370\n",
      "Epoch 533: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.717 | Test Loss: 0.380\n",
      "Epoch 534: | Train Loss: 0.01050 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.734 | Test Loss: 0.371\n",
      "Epoch 535: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.728 | Test Loss: 0.372\n",
      "Epoch 536: | Train Loss: 0.01033 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.732 | Test Loss: 0.367\n",
      "Epoch 537: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.734 | Test Loss: 0.365\n",
      "Epoch 538: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.726 | Test Loss: 0.369\n",
      "Epoch 539: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.734 | Test Loss: 0.368\n",
      "Epoch 540: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.729 | Test Loss: 0.370\n",
      "Epoch 541: | Train Loss: 0.01027 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.733 | Test Loss: 0.367\n",
      "Epoch 542: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.734 | Test Loss: 0.365\n",
      "Epoch 543: | Train Loss: 0.01017 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.729 | Test Loss: 0.367\n",
      "Epoch 544: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.735 | Test Loss: 0.367\n",
      "Epoch 545: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.731 | Test Loss: 0.368\n",
      "Epoch 546: | Train Loss: 0.01022 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.733 | Test Loss: 0.366\n",
      "Epoch 547: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.734 | Test Loss: 0.365\n",
      "Epoch 548: | Train Loss: 0.01016 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.731 | Test Loss: 0.366\n",
      "Epoch 549: | Train Loss: 0.01017 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.734 | Test Loss: 0.366\n",
      "Epoch 550: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.731 | Test Loss: 0.367\n",
      "Epoch 551: | Train Loss: 0.01021 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.734 | Test Loss: 0.366\n",
      "Epoch 552: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.733 | Test Loss: 0.365\n",
      "Epoch 553: | Train Loss: 0.01016 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.733 | Test Loss: 0.365\n",
      "Epoch 554: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.734 | Test Loss: 0.365\n",
      "Epoch 555: | Train Loss: 0.01016 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.733 | Test Loss: 0.366\n",
      "Epoch 556: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.735 | Test Loss: 0.366\n",
      "Epoch 557: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.732 | Test Loss: 0.366\n",
      "Epoch 558: | Train Loss: 0.01017 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.137 | Test Acc: 0.736 | Test Loss: 0.365\n",
      "Epoch 559: | Train Loss: 0.01016 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.732 | Test Loss: 0.366\n",
      "Epoch 560: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.736 | Test Loss: 0.364\n",
      "Epoch 561: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.733 | Test Loss: 0.365\n",
      "Epoch 562: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.735 | Test Loss: 0.365\n",
      "Epoch 563: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.735 | Test Loss: 0.365\n",
      "Epoch 564: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.734 | Test Loss: 0.365\n",
      "Epoch 565: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.736 | Test Loss: 0.364\n",
      "Epoch 566: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.730 | Test Loss: 0.366\n",
      "Epoch 567: | Train Loss: 0.01017 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.737 | Test Loss: 0.365\n",
      "Epoch 568: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.722 | Test Loss: 0.371\n",
      "Epoch 569: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.736 | Test Loss: 0.367\n",
      "Epoch 570: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.710 | Test Loss: 0.377\n",
      "Epoch 571: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.146 | Test Acc: 0.736 | Test Loss: 0.368\n",
      "Epoch 572: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.723 | Test Loss: 0.372\n",
      "Epoch 573: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.737 | Test Loss: 0.365\n",
      "Epoch 574: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.737 | Test Loss: 0.364\n",
      "Epoch 575: | Train Loss: 0.01016 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.721 | Test Loss: 0.368\n",
      "Epoch 576: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.096 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 577: | Train Loss: 0.01030 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.170 | Test Acc: 0.704 | Test Loss: 0.374\n",
      "Epoch 578: | Train Loss: 0.01033 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.730 | Test Loss: 0.366\n",
      "Epoch 579: | Train Loss: 0.01027 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.730 | Test Loss: 0.366\n",
      "Epoch 580: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.731 | Test Loss: 0.365\n",
      "Epoch 581: | Train Loss: 0.01014 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.735 | Test Loss: 0.365\n",
      "Epoch 582: | Train Loss: 0.01021 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.714 | Test Loss: 0.371\n",
      "Epoch 583: | Train Loss: 0.01024 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.737 | Test Loss: 0.364\n",
      "Epoch 584: | Train Loss: 0.01017 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.736 | Test Loss: 0.364\n",
      "Epoch 585: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 586: | Train Loss: 0.01016 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.735 | Test Loss: 0.365\n",
      "Epoch 587: | Train Loss: 0.01020 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.722 | Test Loss: 0.368\n",
      "Epoch 588: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.739 | Test Loss: 0.364\n",
      "Epoch 589: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.738 | Test Loss: 0.364\n",
      "Epoch 590: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.729 | Test Loss: 0.366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.111 | Test Acc: 0.738 | Test Loss: 0.364\n",
      "Epoch 592: | Train Loss: 0.01016 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.145 | Test Acc: 0.729 | Test Loss: 0.366\n",
      "Epoch 593: | Train Loss: 0.01014 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.739 | Test Loss: 0.364\n",
      "Epoch 594: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.738 | Test Loss: 0.364\n",
      "Epoch 595: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.733 | Test Loss: 0.365\n",
      "Epoch 596: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.738 | Test Loss: 0.364\n",
      "Epoch 597: | Train Loss: 0.01016 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.729 | Test Loss: 0.366\n",
      "Epoch 598: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.740 | Test Loss: 0.363\n",
      "Epoch 599: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.737 | Test Loss: 0.364\n",
      "Epoch 600: | Train Loss: 0.01011 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.737 | Test Loss: 0.364\n",
      "Epoch 601: | Train Loss: 0.01011 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.740 | Test Loss: 0.363\n",
      "Epoch 602: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.732 | Test Loss: 0.365\n",
      "Epoch 603: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.740 | Test Loss: 0.363\n",
      "Epoch 604: | Train Loss: 0.01014 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.734 | Test Loss: 0.364\n",
      "Epoch 605: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.740 | Test Loss: 0.363\n",
      "Epoch 606: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.738 | Test Loss: 0.363\n",
      "Epoch 607: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.739 | Test Loss: 0.363\n",
      "Epoch 608: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.740 | Test Loss: 0.364\n",
      "Epoch 609: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.740 | Test Loss: 0.365\n",
      "Epoch 610: | Train Loss: 0.01015 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.736 | Test Loss: 0.369\n",
      "Epoch 611: | Train Loss: 0.01025 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.741 | Test Loss: 0.373\n",
      "Epoch 612: | Train Loss: 0.01041 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.704 | Test Loss: 0.403\n",
      "Epoch 613: | Train Loss: 0.01111 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.742 | Test Loss: 0.374\n",
      "Epoch 614: | Train Loss: 0.01042 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.740 | Test Loss: 0.367\n",
      "Epoch 615: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.731 | Test Loss: 0.365\n",
      "Epoch 616: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.742 | Test Loss: 0.365\n",
      "Epoch 617: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.729 | Test Loss: 0.377\n",
      "Epoch 618: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.741 | Test Loss: 0.368\n",
      "Epoch 619: | Train Loss: 0.01025 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.742 | Test Loss: 0.363\n",
      "Epoch 620: | Train Loss: 0.01013 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.735 | Test Loss: 0.365\n",
      "Epoch 621: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.742 | Test Loss: 0.367\n",
      "Epoch 622: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.739 | Test Loss: 0.370\n",
      "Epoch 623: | Train Loss: 0.01027 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.145 | Test Acc: 0.741 | Test Loss: 0.364\n",
      "Epoch 624: | Train Loss: 0.01011 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.740 | Test Loss: 0.364\n",
      "Epoch 625: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.739 | Test Loss: 0.368\n",
      "Epoch 626: | Train Loss: 0.01023 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.742 | Test Loss: 0.365\n",
      "Epoch 627: | Train Loss: 0.01016 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.736 | Test Loss: 0.364\n",
      "Epoch 628: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.741 | Test Loss: 0.364\n",
      "Epoch 629: | Train Loss: 0.01012 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.741 | Test Loss: 0.365\n",
      "Epoch 630: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.734 | Test Loss: 0.366\n",
      "Epoch 631: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.122 | Test Acc: 0.742 | Test Loss: 0.363\n",
      "Epoch 632: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.738 | Test Loss: 0.364\n",
      "Epoch 633: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.739 | Test Loss: 0.365\n",
      "Epoch 634: | Train Loss: 0.01013 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.144 | Test Acc: 0.742 | Test Loss: 0.364\n",
      "Epoch 635: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.734 | Test Loss: 0.365\n",
      "Epoch 636: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.742 | Test Loss: 0.362\n",
      "Epoch 637: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.740 | Test Loss: 0.363\n",
      "Epoch 638: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.738 | Test Loss: 0.364\n",
      "Epoch 639: | Train Loss: 0.01011 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.145 | Test Acc: 0.743 | Test Loss: 0.363\n",
      "Epoch 640: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.736 | Test Loss: 0.364\n",
      "Epoch 641: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.742 | Test Loss: 0.362\n",
      "Epoch 642: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.740 | Test Loss: 0.363\n",
      "Epoch 643: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.740 | Test Loss: 0.363\n",
      "Epoch 644: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 645: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.737 | Test Loss: 0.364\n",
      "Epoch 646: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 647: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.737 | Test Loss: 0.364\n",
      "Epoch 648: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 649: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.739 | Test Loss: 0.363\n",
      "Epoch 650: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 651: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.742 | Test Loss: 0.362\n",
      "Epoch 652: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.742 | Test Loss: 0.362\n",
      "Epoch 653: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.744 | Test Loss: 0.362\n",
      "Epoch 654: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.740 | Test Loss: 0.363\n",
      "Epoch 655: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.744 | Test Loss: 0.362\n",
      "Epoch 656: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.733 | Test Loss: 0.367\n",
      "Epoch 657: | Train Loss: 0.01016 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.743 | Test Loss: 0.366\n",
      "Epoch 658: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.710 | Test Loss: 0.382\n",
      "Epoch 659: | Train Loss: 0.01053 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.743 | Test Loss: 0.369\n",
      "Epoch 660: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.726 | Test Loss: 0.374\n",
      "Epoch 661: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.746 | Test Loss: 0.364\n",
      "Epoch 662: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.745 | Test Loss: 0.362\n",
      "Epoch 663: | Train Loss: 0.01008 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.728 | Test Loss: 0.366\n",
      "Epoch 664: | Train Loss: 0.01012 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.738 | Test Loss: 0.364\n",
      "Epoch 665: | Train Loss: 0.01020 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.710 | Test Loss: 0.373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 666: | Train Loss: 0.01028 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.742 | Test Loss: 0.363\n",
      "Epoch 667: | Train Loss: 0.01015 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.738 | Test Loss: 0.363\n",
      "Epoch 668: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 669: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.746 | Test Loss: 0.362\n",
      "Epoch 670: | Train Loss: 0.01011 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.726 | Test Loss: 0.368\n",
      "Epoch 671: | Train Loss: 0.01017 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.745 | Test Loss: 0.362\n",
      "Epoch 672: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.739 | Test Loss: 0.363\n",
      "Epoch 673: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.744 | Test Loss: 0.362\n",
      "Epoch 674: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.148 | Test Acc: 0.747 | Test Loss: 0.363\n",
      "Epoch 675: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.736 | Test Loss: 0.366\n",
      "Epoch 676: | Train Loss: 0.01014 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.148 | Test Acc: 0.746 | Test Loss: 0.362\n",
      "Epoch 677: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 678: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 679: | Train Loss: 0.01004 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.746 | Test Loss: 0.361\n",
      "Epoch 680: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.735 | Test Loss: 0.364\n",
      "Epoch 681: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.745 | Test Loss: 0.361\n",
      "Epoch 682: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.734 | Test Loss: 0.364\n",
      "Epoch 683: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.746 | Test Loss: 0.361\n",
      "Epoch 684: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.737 | Test Loss: 0.363\n",
      "Epoch 685: | Train Loss: 0.01006 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.746 | Test Loss: 0.362\n",
      "Epoch 686: | Train Loss: 0.01009 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.739 | Test Loss: 0.365\n",
      "Epoch 687: | Train Loss: 0.01012 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.079 | Test Acc: 0.744 | Test Loss: 0.367\n",
      "Epoch 688: | Train Loss: 0.01023 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.186 | Test Acc: 0.741 | Test Loss: 0.369\n",
      "Epoch 689: | Train Loss: 0.01022 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.079 | Test Acc: 0.745 | Test Loss: 0.371\n",
      "Epoch 690: | Train Loss: 0.01033 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.165 | Test Acc: 0.747 | Test Loss: 0.366\n",
      "Epoch 691: | Train Loss: 0.01019 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.737 | Test Loss: 0.367\n",
      "Epoch 692: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.747 | Test Loss: 0.361\n",
      "Epoch 693: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.729 | Test Loss: 0.366\n",
      "Epoch 694: | Train Loss: 0.01010 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.096 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 695: | Train Loss: 0.01012 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.737 | Test Loss: 0.364\n",
      "Epoch 696: | Train Loss: 0.01007 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.748 | Test Loss: 0.361\n",
      "Epoch 697: | Train Loss: 0.01003 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.748 | Test Loss: 0.360\n",
      "Epoch 698: | Train Loss: 0.01002 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.734 | Test Loss: 0.364\n",
      "Epoch 699: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.744 | Test Loss: 0.362\n",
      "Epoch 700: | Train Loss: 0.01011 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.725 | Test Loss: 0.367\n",
      "Epoch 701: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.747 | Test Loss: 0.362\n",
      "Epoch 702: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.736 | Test Loss: 0.365\n",
      "Epoch 703: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.749 | Test Loss: 0.363\n",
      "Epoch 704: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.745 | Test Loss: 0.365\n",
      "Epoch 705: | Train Loss: 0.01012 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.748 | Test Loss: 0.363\n",
      "Epoch 706: | Train Loss: 0.01008 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.745 | Test Loss: 0.363\n",
      "Epoch 707: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.148 | Test Acc: 0.749 | Test Loss: 0.361\n",
      "Epoch 708: | Train Loss: 0.01004 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.739 | Test Loss: 0.363\n",
      "Epoch 709: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.748 | Test Loss: 0.360\n",
      "Epoch 710: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.734 | Test Loss: 0.364\n",
      "Epoch 711: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.747 | Test Loss: 0.361\n",
      "Epoch 712: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.733 | Test Loss: 0.365\n",
      "Epoch 713: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.748 | Test Loss: 0.361\n",
      "Epoch 714: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.146 | Test Acc: 0.733 | Test Loss: 0.367\n",
      "Epoch 715: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.749 | Test Loss: 0.363\n",
      "Epoch 716: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.735 | Test Loss: 0.370\n",
      "Epoch 717: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.750 | Test Loss: 0.364\n",
      "Epoch 718: | Train Loss: 0.01016 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.744 | Test Loss: 0.367\n",
      "Epoch 719: | Train Loss: 0.01017 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.165 | Test Acc: 0.749 | Test Loss: 0.363\n",
      "Epoch 720: | Train Loss: 0.01009 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.112 | Test Acc: 0.749 | Test Loss: 0.361\n",
      "Epoch 721: | Train Loss: 0.01006 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.745 | Test Loss: 0.361\n",
      "Epoch 722: | Train Loss: 0.01002 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.750 | Test Loss: 0.359\n",
      "Epoch 723: | Train Loss: 0.01001 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.742 | Test Loss: 0.361\n",
      "Epoch 724: | Train Loss: 0.01001 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.750 | Test Loss: 0.360\n",
      "Epoch 725: | Train Loss: 0.01002 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.743 | Test Loss: 0.363\n",
      "Epoch 726: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.751 | Test Loss: 0.361\n",
      "Epoch 727: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.746 | Test Loss: 0.363\n",
      "Epoch 728: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.162 | Test Acc: 0.750 | Test Loss: 0.362\n",
      "Epoch 729: | Train Loss: 0.01007 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.116 | Test Acc: 0.748 | Test Loss: 0.363\n",
      "Epoch 730: | Train Loss: 0.01008 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.165 | Test Acc: 0.750 | Test Loss: 0.362\n",
      "Epoch 731: | Train Loss: 0.01006 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.120 | Test Acc: 0.749 | Test Loss: 0.362\n",
      "Epoch 732: | Train Loss: 0.01007 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.162 | Test Acc: 0.750 | Test Loss: 0.361\n",
      "Epoch 733: | Train Loss: 0.01004 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.123 | Test Acc: 0.747 | Test Loss: 0.362\n",
      "Epoch 734: | Train Loss: 0.01004 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.155 | Test Acc: 0.751 | Test Loss: 0.360\n",
      "Epoch 735: | Train Loss: 0.01003 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.131 | Test Acc: 0.743 | Test Loss: 0.363\n",
      "Epoch 736: | Train Loss: 0.01004 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.149 | Test Acc: 0.751 | Test Loss: 0.360\n",
      "Epoch 737: | Train Loss: 0.01004 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.736 | Test Loss: 0.365\n",
      "Epoch 738: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.750 | Test Loss: 0.361\n",
      "Epoch 739: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.729 | Test Loss: 0.367\n",
      "Epoch 740: | Train Loss: 0.01014 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.748 | Test Loss: 0.361\n",
      "Epoch 741: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 742: | Train Loss: 0.01013 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.747 | Test Loss: 0.360\n",
      "Epoch 743: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.732 | Test Loss: 0.365\n",
      "Epoch 744: | Train Loss: 0.01007 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.748 | Test Loss: 0.360\n",
      "Epoch 745: | Train Loss: 0.01006 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.165 | Test Acc: 0.737 | Test Loss: 0.363\n",
      "Epoch 746: | Train Loss: 0.01004 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.096 | Test Acc: 0.749 | Test Loss: 0.361\n",
      "Epoch 747: | Train Loss: 0.01008 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.743 | Test Loss: 0.363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 748: | Train Loss: 0.01006 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.104 | Test Acc: 0.751 | Test Loss: 0.362\n",
      "Epoch 749: | Train Loss: 0.01008 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.165 | Test Acc: 0.751 | Test Loss: 0.362\n",
      "Epoch 750: | Train Loss: 0.01006 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.124 | Test Acc: 0.745 | Test Loss: 0.364\n",
      "Epoch 751: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.156 | Test Acc: 0.752 | Test Loss: 0.362\n",
      "Epoch 752: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.727 | Test Loss: 0.369\n",
      "Epoch 753: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.748 | Test Loss: 0.362\n",
      "Epoch 754: | Train Loss: 0.01011 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 755: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.749 | Test Loss: 0.360\n",
      "Epoch 756: | Train Loss: 0.01004 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.741 | Test Loss: 0.362\n",
      "Epoch 757: | Train Loss: 0.01000 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.753 | Test Loss: 0.358\n",
      "Epoch 758: | Train Loss: 0.00997 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.155 | Test Acc: 0.750 | Test Loss: 0.359\n",
      "Epoch 759: | Train Loss: 0.00996 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.131 | Test Acc: 0.749 | Test Loss: 0.360\n",
      "Epoch 760: | Train Loss: 0.00997 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.149 | Test Acc: 0.753 | Test Loss: 0.359\n",
      "Epoch 761: | Train Loss: 0.01000 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.150 | Test Acc: 0.739 | Test Loss: 0.363\n",
      "Epoch 762: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.752 | Test Loss: 0.360\n",
      "Epoch 763: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.733 | Test Loss: 0.366\n",
      "Epoch 764: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.751 | Test Loss: 0.360\n",
      "Epoch 765: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.737 | Test Loss: 0.364\n",
      "Epoch 766: | Train Loss: 0.01004 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.751 | Test Loss: 0.358\n",
      "Epoch 767: | Train Loss: 0.01001 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.165 | Test Acc: 0.742 | Test Loss: 0.361\n",
      "Epoch 768: | Train Loss: 0.00999 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.108 | Test Acc: 0.752 | Test Loss: 0.359\n",
      "Epoch 769: | Train Loss: 0.01001 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 770: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.084 | Test Acc: 0.750 | Test Loss: 0.363\n",
      "Epoch 771: | Train Loss: 0.01014 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.192 | Test Acc: 0.745 | Test Loss: 0.366\n",
      "Epoch 772: | Train Loss: 0.01013 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.079 | Test Acc: 0.752 | Test Loss: 0.367\n",
      "Epoch 773: | Train Loss: 0.01023 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.172 | Test Acc: 0.753 | Test Loss: 0.365\n",
      "Epoch 774: | Train Loss: 0.01013 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.124 | Test Acc: 0.741 | Test Loss: 0.368\n",
      "Epoch 775: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.150 | Test Acc: 0.752 | Test Loss: 0.362\n",
      "Epoch 776: | Train Loss: 0.01011 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.720 | Test Loss: 0.371\n",
      "Epoch 777: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.746 | Test Loss: 0.361\n",
      "Epoch 778: | Train Loss: 0.01010 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.735 | Test Loss: 0.363\n",
      "Epoch 779: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.753 | Test Loss: 0.358\n",
      "Epoch 780: | Train Loss: 0.00998 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.152 | Test Acc: 0.752 | Test Loss: 0.358\n",
      "Epoch 781: | Train Loss: 0.00995 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.146 | Test Acc: 0.746 | Test Loss: 0.360\n",
      "Epoch 782: | Train Loss: 0.00997 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.123 | Test Acc: 0.752 | Test Loss: 0.359\n",
      "Epoch 783: | Train Loss: 0.01002 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.730 | Test Loss: 0.366\n",
      "Epoch 784: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.750 | Test Loss: 0.359\n",
      "Epoch 785: | Train Loss: 0.01005 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.738 | Test Loss: 0.363\n",
      "Epoch 786: | Train Loss: 0.01002 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.754 | Test Loss: 0.358\n",
      "Epoch 787: | Train Loss: 0.00997 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.134 | Test Acc: 0.751 | Test Loss: 0.358\n",
      "Epoch 788: | Train Loss: 0.00994 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.155 | Test Acc: 0.751 | Test Loss: 0.358\n",
      "Epoch 789: | Train Loss: 0.00994 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.123 | Test Acc: 0.754 | Test Loss: 0.357\n",
      "Epoch 790: | Train Loss: 0.00996 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.165 | Test Acc: 0.742 | Test Loss: 0.361\n",
      "Epoch 791: | Train Loss: 0.00998 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.123 | Test Acc: 0.752 | Test Loss: 0.358\n",
      "Epoch 792: | Train Loss: 0.01000 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.165 | Test Acc: 0.736 | Test Loss: 0.363\n",
      "Epoch 793: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.751 | Test Loss: 0.359\n",
      "Epoch 794: | Train Loss: 0.01003 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.732 | Test Loss: 0.365\n",
      "Epoch 795: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.139 | Test Acc: 0.752 | Test Loss: 0.360\n",
      "Epoch 796: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.735 | Test Loss: 0.366\n",
      "Epoch 797: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.146 | Test Acc: 0.754 | Test Loss: 0.360\n",
      "Epoch 798: | Train Loss: 0.01004 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.134 | Test Acc: 0.743 | Test Loss: 0.364\n",
      "Epoch 799: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.162 | Test Acc: 0.756 | Test Loss: 0.359\n",
      "Epoch 800: | Train Loss: 0.01001 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.124 | Test Acc: 0.752 | Test Loss: 0.361\n",
      "Epoch 801: | Train Loss: 0.01002 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.169 | Test Acc: 0.753 | Test Loss: 0.361\n",
      "Epoch 802: | Train Loss: 0.01002 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.096 | Test Acc: 0.754 | Test Loss: 0.362\n",
      "Epoch 803: | Train Loss: 0.01007 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.193 | Test Acc: 0.747 | Test Loss: 0.363\n",
      "Epoch 804: | Train Loss: 0.01006 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.079 | Test Acc: 0.750 | Test Loss: 0.363\n",
      "Epoch 805: | Train Loss: 0.01015 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.179 | Test Acc: 0.745 | Test Loss: 0.362\n",
      "Epoch 806: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.108 | Test Acc: 0.755 | Test Loss: 0.358\n",
      "Epoch 807: | Train Loss: 0.00998 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.162 | Test Acc: 0.752 | Test Loss: 0.358\n",
      "Epoch 808: | Train Loss: 0.00992 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.153 | Test Acc: 0.753 | Test Loss: 0.358\n",
      "Epoch 809: | Train Loss: 0.00991 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.123 | Test Acc: 0.755 | Test Loss: 0.357\n",
      "Epoch 810: | Train Loss: 0.00994 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.169 | Test Acc: 0.746 | Test Loss: 0.360\n",
      "Epoch 811: | Train Loss: 0.00997 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.100 | Test Acc: 0.753 | Test Loss: 0.359\n",
      "Epoch 812: | Train Loss: 0.01000 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.166 | Test Acc: 0.748 | Test Loss: 0.360\n",
      "Epoch 813: | Train Loss: 0.00995 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.127 | Test Acc: 0.755 | Test Loss: 0.357\n",
      "Epoch 814: | Train Loss: 0.00992 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.150 | Test Acc: 0.755 | Test Loss: 0.357\n",
      "Epoch 815: | Train Loss: 0.00991 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.159 | Test Acc: 0.750 | Test Loss: 0.358\n",
      "Epoch 816: | Train Loss: 0.00992 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.128 | Test Acc: 0.755 | Test Loss: 0.357\n",
      "Epoch 817: | Train Loss: 0.00995 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.166 | Test Acc: 0.737 | Test Loss: 0.362\n",
      "Epoch 818: | Train Loss: 0.01000 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.748 | Test Loss: 0.360\n",
      "Epoch 819: | Train Loss: 0.01008 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.707 | Test Loss: 0.377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 820: | Train Loss: 0.01036 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.741 | Test Loss: 0.370\n",
      "Epoch 821: | Train Loss: 0.01038 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.150 | Test Acc: 0.680 | Test Loss: 0.404\n",
      "Epoch 822: | Train Loss: 0.01107 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.159 | Test Acc: 0.757 | Test Loss: 0.358\n",
      "Epoch 823: | Train Loss: 0.00997 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.132 | Test Acc: 0.738 | Test Loss: 0.367\n",
      "Epoch 824: | Train Loss: 0.01031 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.649 | Test Loss: 0.432\n",
      "Epoch 825: | Train Loss: 0.01177 | Train F1: 0.005 | Train Acc: 0.018| Test F1: 0.116 | Test Acc: 0.756 | Test Loss: 0.360\n",
      "Epoch 826: | Train Loss: 0.01000 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.334 | Test Acc: 0.684 | Test Loss: 0.474\n",
      "Epoch 827: | Train Loss: 0.01339 | Train F1: 0.008 | Train Acc: 0.018| Test F1: 0.128 | Test Acc: 0.633 | Test Loss: 0.514\n",
      "Epoch 828: | Train Loss: 0.01394 | Train F1: 0.003 | Train Acc: 0.018| Test F1: 0.156 | Test Acc: 0.628 | Test Loss: 0.567\n",
      "Epoch 829: | Train Loss: 0.01530 | Train F1: 0.004 | Train Acc: 0.018| Test F1: 0.127 | Test Acc: 0.667 | Test Loss: 0.406\n",
      "Epoch 830: | Train Loss: 0.01112 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.131 | Test Acc: 0.659 | Test Loss: 0.386\n",
      "Epoch 831: | Train Loss: 0.01071 | Train F1: 0.004 | Train Acc: 0.018| Test F1: 0.127 | Test Acc: 0.635 | Test Loss: 0.420\n",
      "Epoch 832: | Train Loss: 0.01175 | Train F1: 0.004 | Train Acc: 0.017| Test F1: 0.104 | Test Acc: 0.658 | Test Loss: 0.385\n",
      "Epoch 833: | Train Loss: 0.01065 | Train F1: 0.003 | Train Acc: 0.018| Test F1: 0.124 | Test Acc: 0.624 | Test Loss: 0.390\n",
      "Epoch 834: | Train Loss: 0.01067 | Train F1: 0.003 | Train Acc: 0.018| Test F1: 0.127 | Test Acc: 0.598 | Test Loss: 0.409\n",
      "Epoch 835: | Train Loss: 0.01111 | Train F1: 0.004 | Train Acc: 0.017| Test F1: 0.096 | Test Acc: 0.616 | Test Loss: 0.404\n",
      "Epoch 836: | Train Loss: 0.01101 | Train F1: 0.003 | Train Acc: 0.018| Test F1: 0.108 | Test Acc: 0.640 | Test Loss: 0.388\n",
      "Epoch 837: | Train Loss: 0.01060 | Train F1: 0.003 | Train Acc: 0.018| Test F1: 0.107 | Test Acc: 0.673 | Test Loss: 0.383\n",
      "Epoch 838: | Train Loss: 0.01054 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.091 | Test Acc: 0.660 | Test Loss: 0.391\n",
      "Epoch 839: | Train Loss: 0.01083 | Train F1: 0.003 | Train Acc: 0.018| Test F1: 0.083 | Test Acc: 0.660 | Test Loss: 0.391\n",
      "Epoch 840: | Train Loss: 0.01084 | Train F1: 0.003 | Train Acc: 0.018| Test F1: 0.095 | Test Acc: 0.709 | Test Loss: 0.383\n",
      "Epoch 841: | Train Loss: 0.01057 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.107 | Test Acc: 0.704 | Test Loss: 0.382\n",
      "Epoch 842: | Train Loss: 0.01049 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.088 | Test Acc: 0.686 | Test Loss: 0.386\n",
      "Epoch 843: | Train Loss: 0.01058 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.096 | Test Acc: 0.671 | Test Loss: 0.390\n",
      "Epoch 844: | Train Loss: 0.01066 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.108 | Test Acc: 0.668 | Test Loss: 0.389\n",
      "Epoch 845: | Train Loss: 0.01063 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.100 | Test Acc: 0.695 | Test Loss: 0.384\n",
      "Epoch 846: | Train Loss: 0.01053 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.087 | Test Acc: 0.712 | Test Loss: 0.381\n",
      "Epoch 847: | Train Loss: 0.01049 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.095 | Test Acc: 0.717 | Test Loss: 0.382\n",
      "Epoch 848: | Train Loss: 0.01054 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.111 | Test Acc: 0.719 | Test Loss: 0.384\n",
      "Epoch 849: | Train Loss: 0.01059 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.719 | Test Loss: 0.383\n",
      "Epoch 850: | Train Loss: 0.01055 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.095 | Test Acc: 0.714 | Test Loss: 0.381\n",
      "Epoch 851: | Train Loss: 0.01049 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.706 | Test Loss: 0.382\n",
      "Epoch 852: | Train Loss: 0.01049 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.697 | Test Loss: 0.384\n",
      "Epoch 853: | Train Loss: 0.01052 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.107 | Test Acc: 0.699 | Test Loss: 0.384\n",
      "Epoch 854: | Train Loss: 0.01054 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.702 | Test Loss: 0.383\n",
      "Epoch 855: | Train Loss: 0.01051 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.706 | Test Loss: 0.382\n",
      "Epoch 856: | Train Loss: 0.01048 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.714 | Test Loss: 0.381\n",
      "Epoch 857: | Train Loss: 0.01047 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.717 | Test Loss: 0.381\n",
      "Epoch 858: | Train Loss: 0.01050 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.716 | Test Loss: 0.381\n",
      "Epoch 859: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.715 | Test Loss: 0.381\n",
      "Epoch 860: | Train Loss: 0.01048 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 861: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.708 | Test Loss: 0.381\n",
      "Epoch 862: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.706 | Test Loss: 0.382\n",
      "Epoch 863: | Train Loss: 0.01048 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.707 | Test Loss: 0.382\n",
      "Epoch 864: | Train Loss: 0.01048 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.708 | Test Loss: 0.381\n",
      "Epoch 865: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.709 | Test Loss: 0.381\n",
      "Epoch 866: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 867: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.716 | Test Loss: 0.380\n",
      "Epoch 868: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.137 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 869: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 870: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 871: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.708 | Test Loss: 0.381\n",
      "Epoch 872: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.708 | Test Loss: 0.381\n",
      "Epoch 873: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.145 | Test Acc: 0.709 | Test Loss: 0.381\n",
      "Epoch 874: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.710 | Test Loss: 0.381\n",
      "Epoch 875: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 876: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.716 | Test Loss: 0.380\n",
      "Epoch 877: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.716 | Test Loss: 0.380\n",
      "Epoch 878: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 879: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 880: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.711 | Test Loss: 0.380\n",
      "Epoch 881: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.710 | Test Loss: 0.380\n",
      "Epoch 882: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.711 | Test Loss: 0.380\n",
      "Epoch 883: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.711 | Test Loss: 0.380\n",
      "Epoch 884: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 885: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 886: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 887: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 888: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 889: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 890: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 891: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 892: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 893: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 894: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 895: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 896: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 897: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 898: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 899: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 900: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 901: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 902: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 903: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 904: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 905: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 906: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 907: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 908: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 909: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 910: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 911: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 912: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 913: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 914: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 915: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 916: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 917: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.713 | Test Loss: 0.379\n",
      "Epoch 918: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 919: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.713 | Test Loss: 0.379\n",
      "Epoch 920: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 921: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 922: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 923: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.713 | Test Loss: 0.379\n",
      "Epoch 924: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 925: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 926: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 927: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.713 | Test Loss: 0.379\n",
      "Epoch 928: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.146 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 929: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 930: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 931: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 932: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.717 | Test Loss: 0.379\n",
      "Epoch 933: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.169 | Test Acc: 0.711 | Test Loss: 0.380\n",
      "Epoch 934: | Train Loss: 0.01043 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.721 | Test Loss: 0.379\n",
      "Epoch 935: | Train Loss: 0.01045 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.183 | Test Acc: 0.706 | Test Loss: 0.383\n",
      "Epoch 936: | Train Loss: 0.01047 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.095 | Test Acc: 0.718 | Test Loss: 0.382\n",
      "Epoch 937: | Train Loss: 0.01056 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.205 | Test Acc: 0.690 | Test Loss: 0.387\n",
      "Epoch 938: | Train Loss: 0.01058 | Train F1: 0.006 | Train Acc: 0.019| Test F1: 0.075 | Test Acc: 0.700 | Test Loss: 0.388\n",
      "Epoch 939: | Train Loss: 0.01076 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.205 | Test Acc: 0.690 | Test Loss: 0.387\n",
      "Epoch 940: | Train Loss: 0.01058 | Train F1: 0.006 | Train Acc: 0.019| Test F1: 0.095 | Test Acc: 0.719 | Test Loss: 0.381\n",
      "Epoch 941: | Train Loss: 0.01053 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.165 | Test Acc: 0.711 | Test Loss: 0.380\n",
      "Epoch 942: | Train Loss: 0.01043 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 943: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.115 | Test Acc: 0.721 | Test Loss: 0.380\n",
      "Epoch 944: | Train Loss: 0.01046 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.180 | Test Acc: 0.698 | Test Loss: 0.384\n",
      "Epoch 945: | Train Loss: 0.01050 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.087 | Test Acc: 0.718 | Test Loss: 0.382\n",
      "Epoch 946: | Train Loss: 0.01055 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.173 | Test Acc: 0.707 | Test Loss: 0.382\n",
      "Epoch 947: | Train Loss: 0.01046 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 948: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 949: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.165 | Test Acc: 0.710 | Test Loss: 0.381\n",
      "Epoch 950: | Train Loss: 0.01044 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.720 | Test Loss: 0.380\n",
      "Epoch 951: | Train Loss: 0.01048 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.173 | Test Acc: 0.707 | Test Loss: 0.382\n",
      "Epoch 952: | Train Loss: 0.01045 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.719 | Test Loss: 0.379\n",
      "Epoch 953: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 954: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 955: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.721 | Test Loss: 0.379\n",
      "Epoch 956: | Train Loss: 0.01043 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.169 | Test Acc: 0.710 | Test Loss: 0.381\n",
      "Epoch 957: | Train Loss: 0.01044 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.721 | Test Loss: 0.379\n",
      "Epoch 958: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 959: | Train Loss: 0.01041 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 960: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 961: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 962: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.719 | Test Loss: 0.379\n",
      "Epoch 963: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.169 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 964: | Train Loss: 0.01042 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.721 | Test Loss: 0.379\n",
      "Epoch 965: | Train Loss: 0.01043 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.169 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 966: | Train Loss: 0.01042 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.719 | Test Loss: 0.378\n",
      "Epoch 967: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.713 | Test Loss: 0.379\n",
      "Epoch 968: | Train Loss: 0.01040 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 969: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.716 | Test Loss: 0.379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 970: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 971: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.145 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 972: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 973: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 974: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 975: | Train Loss: 0.01039 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.718 | Test Loss: 0.378\n",
      "Epoch 976: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.173 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 977: | Train Loss: 0.01041 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.722 | Test Loss: 0.379\n",
      "Epoch 978: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.190 | Test Acc: 0.703 | Test Loss: 0.383\n",
      "Epoch 979: | Train Loss: 0.01047 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.709 | Test Loss: 0.384\n",
      "Epoch 980: | Train Loss: 0.01062 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.226 | Test Acc: 0.682 | Test Loss: 0.389\n",
      "Epoch 981: | Train Loss: 0.01063 | Train F1: 0.006 | Train Acc: 0.019| Test F1: 0.076 | Test Acc: 0.693 | Test Loss: 0.393\n",
      "Epoch 982: | Train Loss: 0.01092 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.205 | Test Acc: 0.695 | Test Loss: 0.385\n",
      "Epoch 983: | Train Loss: 0.01054 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.721 | Test Loss: 0.378\n",
      "Epoch 984: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.717 | Test Loss: 0.378\n",
      "Epoch 985: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.180 | Test Acc: 0.702 | Test Loss: 0.383\n",
      "Epoch 986: | Train Loss: 0.01046 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.079 | Test Acc: 0.710 | Test Loss: 0.384\n",
      "Epoch 987: | Train Loss: 0.01062 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.193 | Test Acc: 0.696 | Test Loss: 0.384\n",
      "Epoch 988: | Train Loss: 0.01051 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.721 | Test Loss: 0.379\n",
      "Epoch 989: | Train Loss: 0.01045 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 990: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 991: | Train Loss: 0.01040 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.720 | Test Loss: 0.380\n",
      "Epoch 992: | Train Loss: 0.01047 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.180 | Test Acc: 0.703 | Test Loss: 0.382\n",
      "Epoch 993: | Train Loss: 0.01046 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.722 | Test Loss: 0.379\n",
      "Epoch 994: | Train Loss: 0.01043 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 995: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 996: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.723 | Test Loss: 0.378\n",
      "Epoch 997: | Train Loss: 0.01042 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.176 | Test Acc: 0.710 | Test Loss: 0.381\n",
      "Epoch 998: | Train Loss: 0.01043 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.723 | Test Loss: 0.378\n",
      "Epoch 999: | Train Loss: 0.01042 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 1000: | Train Loss: 0.01039 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.717 | Test Loss: 0.378\n",
      "Epoch 1001: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.719 | Test Loss: 0.378\n",
      "Epoch 1002: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.169 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 1003: | Train Loss: 0.01040 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.723 | Test Loss: 0.378\n",
      "Epoch 1004: | Train Loss: 0.01041 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 1005: | Train Loss: 0.01039 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.718 | Test Loss: 0.378\n",
      "Epoch 1006: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.145 | Test Acc: 0.718 | Test Loss: 0.378\n",
      "Epoch 1007: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 1008: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.720 | Test Loss: 0.378\n",
      "Epoch 1009: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.173 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 1010: | Train Loss: 0.01039 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.722 | Test Loss: 0.378\n",
      "Epoch 1011: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.169 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 1012: | Train Loss: 0.01039 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.720 | Test Loss: 0.377\n",
      "Epoch 1013: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 1014: | Train Loss: 0.01037 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.718 | Test Loss: 0.377\n",
      "Epoch 1015: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.718 | Test Loss: 0.378\n",
      "Epoch 1016: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.718 | Test Loss: 0.377\n",
      "Epoch 1017: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.719 | Test Loss: 0.377\n",
      "Epoch 1018: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.718 | Test Loss: 0.378\n",
      "Epoch 1019: | Train Loss: 0.01036 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.719 | Test Loss: 0.377\n",
      "Epoch 1020: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.169 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 1021: | Train Loss: 0.01037 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.723 | Test Loss: 0.377\n",
      "Epoch 1022: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.183 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 1023: | Train Loss: 0.01041 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.720 | Test Loss: 0.380\n",
      "Epoch 1024: | Train Loss: 0.01049 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.217 | Test Acc: 0.691 | Test Loss: 0.387\n",
      "Epoch 1025: | Train Loss: 0.01056 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.076 | Test Acc: 0.694 | Test Loss: 0.392\n",
      "Epoch 1026: | Train Loss: 0.01090 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.228 | Test Acc: 0.685 | Test Loss: 0.389\n",
      "Epoch 1027: | Train Loss: 0.01061 | Train F1: 0.006 | Train Acc: 0.019| Test F1: 0.084 | Test Acc: 0.709 | Test Loss: 0.383\n",
      "Epoch 1028: | Train Loss: 0.01061 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.184 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 1029: | Train Loss: 0.01041 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.719 | Test Loss: 0.377\n",
      "Epoch 1030: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.136 | Test Acc: 0.722 | Test Loss: 0.377\n",
      "Epoch 1031: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.704 | Test Loss: 0.382\n",
      "Epoch 1032: | Train Loss: 0.01044 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.713 | Test Loss: 0.382\n",
      "Epoch 1033: | Train Loss: 0.01057 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.192 | Test Acc: 0.701 | Test Loss: 0.383\n",
      "Epoch 1034: | Train Loss: 0.01047 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.722 | Test Loss: 0.378\n",
      "Epoch 1035: | Train Loss: 0.01042 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.717 | Test Loss: 0.378\n",
      "Epoch 1036: | Train Loss: 0.01036 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.717 | Test Loss: 0.378\n",
      "Epoch 1037: | Train Loss: 0.01035 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.724 | Test Loss: 0.378\n",
      "Epoch 1038: | Train Loss: 0.01040 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.184 | Test Acc: 0.707 | Test Loss: 0.381\n",
      "Epoch 1039: | Train Loss: 0.01042 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.722 | Test Loss: 0.379\n",
      "Epoch 1040: | Train Loss: 0.01045 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.173 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 1041: | Train Loss: 0.01038 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.720 | Test Loss: 0.377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1042: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.720 | Test Loss: 0.377\n",
      "Epoch 1043: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.170 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 1044: | Train Loss: 0.01037 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.724 | Test Loss: 0.378\n",
      "Epoch 1045: | Train Loss: 0.01040 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.176 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 1046: | Train Loss: 0.01039 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.725 | Test Loss: 0.377\n",
      "Epoch 1047: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.717 | Test Loss: 0.378\n",
      "Epoch 1048: | Train Loss: 0.01035 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.720 | Test Loss: 0.377\n",
      "Epoch 1049: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1050: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.170 | Test Acc: 0.717 | Test Loss: 0.378\n",
      "Epoch 1051: | Train Loss: 0.01036 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.725 | Test Loss: 0.377\n",
      "Epoch 1052: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 1053: | Train Loss: 0.01038 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.725 | Test Loss: 0.377\n",
      "Epoch 1054: | Train Loss: 0.01039 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 1055: | Train Loss: 0.01037 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.136 | Test Acc: 0.725 | Test Loss: 0.377\n",
      "Epoch 1056: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.170 | Test Acc: 0.718 | Test Loss: 0.378\n",
      "Epoch 1057: | Train Loss: 0.01035 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.722 | Test Loss: 0.376\n",
      "Epoch 1058: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.719 | Test Loss: 0.377\n",
      "Epoch 1059: | Train Loss: 0.01034 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.146 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1060: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.720 | Test Loss: 0.376\n",
      "Epoch 1061: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1062: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1063: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.720 | Test Loss: 0.376\n",
      "Epoch 1064: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1065: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.720 | Test Loss: 0.376\n",
      "Epoch 1066: | Train Loss: 0.01032 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1067: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.167 | Test Acc: 0.719 | Test Loss: 0.377\n",
      "Epoch 1068: | Train Loss: 0.01032 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.724 | Test Loss: 0.376\n",
      "Epoch 1069: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.188 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 1070: | Train Loss: 0.01036 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.721 | Test Loss: 0.379\n",
      "Epoch 1071: | Train Loss: 0.01048 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.244 | Test Acc: 0.685 | Test Loss: 0.389\n",
      "Epoch 1072: | Train Loss: 0.01062 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.037 | Test Acc: 0.680 | Test Loss: 0.407\n",
      "Epoch 1073: | Train Loss: 0.01137 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.221 | Test Acc: 0.693 | Test Loss: 0.386\n",
      "Epoch 1074: | Train Loss: 0.01054 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.725 | Test Loss: 0.376\n",
      "Epoch 1075: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.724 | Test Loss: 0.376\n",
      "Epoch 1076: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.201 | Test Acc: 0.698 | Test Loss: 0.383\n",
      "Epoch 1077: | Train Loss: 0.01047 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.071 | Test Acc: 0.697 | Test Loss: 0.391\n",
      "Epoch 1078: | Train Loss: 0.01087 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.215 | Test Acc: 0.693 | Test Loss: 0.386\n",
      "Epoch 1079: | Train Loss: 0.01053 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.723 | Test Loss: 0.378\n",
      "Epoch 1080: | Train Loss: 0.01042 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.720 | Test Loss: 0.376\n",
      "Epoch 1081: | Train Loss: 0.01032 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 1082: | Train Loss: 0.01035 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.088 | Test Acc: 0.721 | Test Loss: 0.380\n",
      "Epoch 1083: | Train Loss: 0.01048 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.196 | Test Acc: 0.701 | Test Loss: 0.382\n",
      "Epoch 1084: | Train Loss: 0.01045 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.722 | Test Loss: 0.378\n",
      "Epoch 1085: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.167 | Test Acc: 0.720 | Test Loss: 0.377\n",
      "Epoch 1086: | Train Loss: 0.01033 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.720 | Test Loss: 0.376\n",
      "Epoch 1087: | Train Loss: 0.01032 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.726 | Test Loss: 0.377\n",
      "Epoch 1088: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.191 | Test Acc: 0.708 | Test Loss: 0.380\n",
      "Epoch 1089: | Train Loss: 0.01040 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.725 | Test Loss: 0.377\n",
      "Epoch 1090: | Train Loss: 0.01040 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.167 | Test Acc: 0.720 | Test Loss: 0.377\n",
      "Epoch 1091: | Train Loss: 0.01033 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.722 | Test Loss: 0.376\n",
      "Epoch 1092: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.727 | Test Loss: 0.376\n",
      "Epoch 1093: | Train Loss: 0.01034 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.181 | Test Acc: 0.713 | Test Loss: 0.379\n",
      "Epoch 1094: | Train Loss: 0.01036 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.726 | Test Loss: 0.377\n",
      "Epoch 1095: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.719 | Test Loss: 0.377\n",
      "Epoch 1096: | Train Loss: 0.01034 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.724 | Test Loss: 0.375\n",
      "Epoch 1097: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.724 | Test Loss: 0.375\n",
      "Epoch 1098: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.167 | Test Acc: 0.719 | Test Loss: 0.377\n",
      "Epoch 1099: | Train Loss: 0.01033 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.728 | Test Loss: 0.376\n",
      "Epoch 1100: | Train Loss: 0.01035 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.718 | Test Loss: 0.378\n",
      "Epoch 1101: | Train Loss: 0.01034 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.132 | Test Acc: 0.728 | Test Loss: 0.375\n",
      "Epoch 1102: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.168 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1103: | Train Loss: 0.01031 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.724 | Test Loss: 0.375\n",
      "Epoch 1104: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.724 | Test Loss: 0.375\n",
      "Epoch 1105: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.168 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1106: | Train Loss: 0.01031 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.136 | Test Acc: 0.728 | Test Loss: 0.375\n",
      "Epoch 1107: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.720 | Test Loss: 0.377\n",
      "Epoch 1108: | Train Loss: 0.01033 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.728 | Test Loss: 0.376\n",
      "Epoch 1109: | Train Loss: 0.01035 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.191 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 1110: | Train Loss: 0.01035 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.726 | Test Loss: 0.376\n",
      "Epoch 1111: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.713 | Test Loss: 0.379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1112: | Train Loss: 0.01037 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.724 | Test Loss: 0.377\n",
      "Epoch 1113: | Train Loss: 0.01042 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.193 | Test Acc: 0.708 | Test Loss: 0.380\n",
      "Epoch 1114: | Train Loss: 0.01039 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.723 | Test Loss: 0.378\n",
      "Epoch 1115: | Train Loss: 0.01045 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.199 | Test Acc: 0.707 | Test Loss: 0.381\n",
      "Epoch 1116: | Train Loss: 0.01041 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.722 | Test Loss: 0.378\n",
      "Epoch 1117: | Train Loss: 0.01045 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.193 | Test Acc: 0.709 | Test Loss: 0.380\n",
      "Epoch 1118: | Train Loss: 0.01039 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.725 | Test Loss: 0.377\n",
      "Epoch 1119: | Train Loss: 0.01040 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.190 | Test Acc: 0.714 | Test Loss: 0.378\n",
      "Epoch 1120: | Train Loss: 0.01035 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.728 | Test Loss: 0.375\n",
      "Epoch 1121: | Train Loss: 0.01034 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.720 | Test Loss: 0.377\n",
      "Epoch 1122: | Train Loss: 0.01031 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.136 | Test Acc: 0.729 | Test Loss: 0.374\n",
      "Epoch 1123: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.178 | Test Acc: 0.722 | Test Loss: 0.376\n",
      "Epoch 1124: | Train Loss: 0.01030 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.728 | Test Loss: 0.374\n",
      "Epoch 1125: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.723 | Test Loss: 0.375\n",
      "Epoch 1126: | Train Loss: 0.01029 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.139 | Test Acc: 0.729 | Test Loss: 0.374\n",
      "Epoch 1127: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.722 | Test Loss: 0.376\n",
      "Epoch 1128: | Train Loss: 0.01030 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.132 | Test Acc: 0.730 | Test Loss: 0.374\n",
      "Epoch 1129: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.191 | Test Acc: 0.718 | Test Loss: 0.377\n",
      "Epoch 1130: | Train Loss: 0.01033 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.726 | Test Loss: 0.377\n",
      "Epoch 1131: | Train Loss: 0.01040 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.205 | Test Acc: 0.704 | Test Loss: 0.382\n",
      "Epoch 1132: | Train Loss: 0.01043 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.075 | Test Acc: 0.709 | Test Loss: 0.384\n",
      "Epoch 1133: | Train Loss: 0.01064 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.230 | Test Acc: 0.694 | Test Loss: 0.385\n",
      "Epoch 1134: | Train Loss: 0.01052 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.075 | Test Acc: 0.709 | Test Loss: 0.384\n",
      "Epoch 1135: | Train Loss: 0.01064 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.199 | Test Acc: 0.707 | Test Loss: 0.381\n",
      "Epoch 1136: | Train Loss: 0.01040 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.729 | Test Loss: 0.375\n",
      "Epoch 1137: | Train Loss: 0.01033 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.724 | Test Loss: 0.375\n",
      "Epoch 1138: | Train Loss: 0.01027 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.168 | Test Acc: 0.724 | Test Loss: 0.374\n",
      "Epoch 1139: | Train Loss: 0.01027 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.132 | Test Acc: 0.730 | Test Loss: 0.374\n",
      "Epoch 1140: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.714 | Test Loss: 0.378\n",
      "Epoch 1141: | Train Loss: 0.01034 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.724 | Test Loss: 0.378\n",
      "Epoch 1142: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.199 | Test Acc: 0.705 | Test Loss: 0.381\n",
      "Epoch 1143: | Train Loss: 0.01041 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.723 | Test Loss: 0.378\n",
      "Epoch 1144: | Train Loss: 0.01045 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.714 | Test Loss: 0.378\n",
      "Epoch 1145: | Train Loss: 0.01035 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.128 | Test Acc: 0.731 | Test Loss: 0.374\n",
      "Epoch 1146: | Train Loss: 0.01031 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.725 | Test Loss: 0.374\n",
      "Epoch 1147: | Train Loss: 0.01027 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.164 | Test Acc: 0.726 | Test Loss: 0.374\n",
      "Epoch 1148: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.139 | Test Acc: 0.730 | Test Loss: 0.373\n",
      "Epoch 1149: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.722 | Test Loss: 0.376\n",
      "Epoch 1150: | Train Loss: 0.01029 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.729 | Test Loss: 0.375\n",
      "Epoch 1151: | Train Loss: 0.01033 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.715 | Test Loss: 0.378\n",
      "Epoch 1152: | Train Loss: 0.01034 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.727 | Test Loss: 0.376\n",
      "Epoch 1153: | Train Loss: 0.01039 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.712 | Test Loss: 0.379\n",
      "Epoch 1154: | Train Loss: 0.01035 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.727 | Test Loss: 0.376\n",
      "Epoch 1155: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.715 | Test Loss: 0.378\n",
      "Epoch 1156: | Train Loss: 0.01033 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.729 | Test Loss: 0.375\n",
      "Epoch 1157: | Train Loss: 0.01033 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.184 | Test Acc: 0.720 | Test Loss: 0.376\n",
      "Epoch 1158: | Train Loss: 0.01030 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.128 | Test Acc: 0.731 | Test Loss: 0.374\n",
      "Epoch 1159: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.178 | Test Acc: 0.724 | Test Loss: 0.375\n",
      "Epoch 1160: | Train Loss: 0.01027 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.731 | Test Loss: 0.373\n",
      "Epoch 1161: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.725 | Test Loss: 0.374\n",
      "Epoch 1162: | Train Loss: 0.01026 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.143 | Test Acc: 0.731 | Test Loss: 0.373\n",
      "Epoch 1163: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.726 | Test Loss: 0.374\n",
      "Epoch 1164: | Train Loss: 0.01026 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.143 | Test Acc: 0.731 | Test Loss: 0.373\n",
      "Epoch 1165: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.178 | Test Acc: 0.725 | Test Loss: 0.374\n",
      "Epoch 1166: | Train Loss: 0.01026 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.132 | Test Acc: 0.732 | Test Loss: 0.373\n",
      "Epoch 1167: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.192 | Test Acc: 0.720 | Test Loss: 0.376\n",
      "Epoch 1168: | Train Loss: 0.01029 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.727 | Test Loss: 0.376\n",
      "Epoch 1169: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.224 | Test Acc: 0.701 | Test Loss: 0.382\n",
      "Epoch 1170: | Train Loss: 0.01044 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.067 | Test Acc: 0.703 | Test Loss: 0.389\n",
      "Epoch 1171: | Train Loss: 0.01081 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.238 | Test Acc: 0.692 | Test Loss: 0.386\n",
      "Epoch 1172: | Train Loss: 0.01054 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.071 | Test Acc: 0.711 | Test Loss: 0.383\n",
      "Epoch 1173: | Train Loss: 0.01063 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.200 | Test Acc: 0.711 | Test Loss: 0.379\n",
      "Epoch 1174: | Train Loss: 0.01036 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.132 | Test Acc: 0.732 | Test Loss: 0.373\n",
      "Epoch 1175: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.728 | Test Loss: 0.373\n",
      "Epoch 1176: | Train Loss: 0.01024 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.727 | Test Loss: 0.373\n",
      "Epoch 1177: | Train Loss: 0.01024 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.128 | Test Acc: 0.732 | Test Loss: 0.373\n",
      "Epoch 1178: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.713 | Test Loss: 0.378\n",
      "Epoch 1179: | Train Loss: 0.01034 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.722 | Test Loss: 0.378\n",
      "Epoch 1180: | Train Loss: 0.01048 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.212 | Test Acc: 0.704 | Test Loss: 0.381\n",
      "Epoch 1181: | Train Loss: 0.01041 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.722 | Test Loss: 0.378\n",
      "Epoch 1182: | Train Loss: 0.01048 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.714 | Test Loss: 0.378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1183: | Train Loss: 0.01033 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.128 | Test Acc: 0.733 | Test Loss: 0.373\n",
      "Epoch 1184: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.728 | Test Loss: 0.373\n",
      "Epoch 1185: | Train Loss: 0.01023 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.729 | Test Loss: 0.373\n",
      "Epoch 1186: | Train Loss: 0.01023 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.733 | Test Loss: 0.372\n",
      "Epoch 1187: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.184 | Test Acc: 0.722 | Test Loss: 0.375\n",
      "Epoch 1188: | Train Loss: 0.01027 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.731 | Test Loss: 0.374\n",
      "Epoch 1189: | Train Loss: 0.01032 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.714 | Test Loss: 0.378\n",
      "Epoch 1190: | Train Loss: 0.01032 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.728 | Test Loss: 0.376\n",
      "Epoch 1191: | Train Loss: 0.01039 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.713 | Test Loss: 0.378\n",
      "Epoch 1192: | Train Loss: 0.01033 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.729 | Test Loss: 0.375\n",
      "Epoch 1193: | Train Loss: 0.01035 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.195 | Test Acc: 0.719 | Test Loss: 0.376\n",
      "Epoch 1194: | Train Loss: 0.01029 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.733 | Test Loss: 0.373\n",
      "Epoch 1195: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.725 | Test Loss: 0.374\n",
      "Epoch 1196: | Train Loss: 0.01025 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.734 | Test Loss: 0.372\n",
      "Epoch 1197: | Train Loss: 0.01024 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.729 | Test Loss: 0.373\n",
      "Epoch 1198: | Train Loss: 0.01022 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.733 | Test Loss: 0.372\n",
      "Epoch 1199: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.730 | Test Loss: 0.372\n",
      "Epoch 1200: | Train Loss: 0.01022 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.732 | Test Loss: 0.372\n",
      "Epoch 1201: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.730 | Test Loss: 0.372\n",
      "Epoch 1202: | Train Loss: 0.01021 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.732 | Test Loss: 0.372\n",
      "Epoch 1203: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.730 | Test Loss: 0.372\n",
      "Epoch 1204: | Train Loss: 0.01021 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.733 | Test Loss: 0.371\n",
      "Epoch 1205: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.175 | Test Acc: 0.730 | Test Loss: 0.372\n",
      "Epoch 1206: | Train Loss: 0.01021 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.735 | Test Loss: 0.371\n",
      "Epoch 1207: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.191 | Test Acc: 0.723 | Test Loss: 0.375\n",
      "Epoch 1208: | Train Loss: 0.01026 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.728 | Test Loss: 0.375\n",
      "Epoch 1209: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.241 | Test Acc: 0.696 | Test Loss: 0.385\n",
      "Epoch 1210: | Train Loss: 0.01051 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.045 | Test Acc: 0.687 | Test Loss: 0.402\n",
      "Epoch 1211: | Train Loss: 0.01123 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.231 | Test Acc: 0.700 | Test Loss: 0.383\n",
      "Epoch 1212: | Train Loss: 0.01045 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.734 | Test Loss: 0.372\n",
      "Epoch 1213: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.731 | Test Loss: 0.372\n",
      "Epoch 1214: | Train Loss: 0.01020 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.174 | Test Acc: 0.729 | Test Loss: 0.372\n",
      "Epoch 1215: | Train Loss: 0.01021 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.733 | Test Loss: 0.373\n",
      "Epoch 1216: | Train Loss: 0.01030 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.221 | Test Acc: 0.706 | Test Loss: 0.381\n",
      "Epoch 1217: | Train Loss: 0.01041 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.063 | Test Acc: 0.703 | Test Loss: 0.389\n",
      "Epoch 1218: | Train Loss: 0.01083 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.231 | Test Acc: 0.698 | Test Loss: 0.383\n",
      "Epoch 1219: | Train Loss: 0.01045 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.727 | Test Loss: 0.376\n",
      "Epoch 1220: | Train Loss: 0.01041 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.188 | Test Acc: 0.725 | Test Loss: 0.374\n",
      "Epoch 1221: | Train Loss: 0.01025 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.735 | Test Loss: 0.371\n",
      "Epoch 1222: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.735 | Test Loss: 0.371\n",
      "Epoch 1223: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.184 | Test Acc: 0.725 | Test Loss: 0.374\n",
      "Epoch 1224: | Train Loss: 0.01024 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.731 | Test Loss: 0.374\n",
      "Epoch 1225: | Train Loss: 0.01033 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.202 | Test Acc: 0.711 | Test Loss: 0.378\n",
      "Epoch 1226: | Train Loss: 0.01034 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.725 | Test Loss: 0.377\n",
      "Epoch 1227: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.196 | Test Acc: 0.715 | Test Loss: 0.377\n",
      "Epoch 1228: | Train Loss: 0.01031 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.735 | Test Loss: 0.372\n",
      "Epoch 1229: | Train Loss: 0.01027 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.174 | Test Acc: 0.729 | Test Loss: 0.372\n",
      "Epoch 1230: | Train Loss: 0.01021 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.734 | Test Loss: 0.371\n",
      "Epoch 1231: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.158 | Test Acc: 0.736 | Test Loss: 0.370\n",
      "Epoch 1232: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.728 | Test Loss: 0.373\n",
      "Epoch 1233: | Train Loss: 0.01021 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.736 | Test Loss: 0.372\n",
      "Epoch 1234: | Train Loss: 0.01025 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.191 | Test Acc: 0.721 | Test Loss: 0.375\n",
      "Epoch 1235: | Train Loss: 0.01026 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.732 | Test Loss: 0.374\n",
      "Epoch 1236: | Train Loss: 0.01033 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.715 | Test Loss: 0.377\n",
      "Epoch 1237: | Train Loss: 0.01030 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.731 | Test Loss: 0.374\n",
      "Epoch 1238: | Train Loss: 0.01034 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.719 | Test Loss: 0.376\n",
      "Epoch 1239: | Train Loss: 0.01028 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.734 | Test Loss: 0.372\n",
      "Epoch 1240: | Train Loss: 0.01029 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.191 | Test Acc: 0.723 | Test Loss: 0.374\n",
      "Epoch 1241: | Train Loss: 0.01024 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.737 | Test Loss: 0.371\n",
      "Epoch 1242: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.185 | Test Acc: 0.728 | Test Loss: 0.373\n",
      "Epoch 1243: | Train Loss: 0.01021 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.738 | Test Loss: 0.370\n",
      "Epoch 1244: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.181 | Test Acc: 0.729 | Test Loss: 0.372\n",
      "Epoch 1245: | Train Loss: 0.01020 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.738 | Test Loss: 0.370\n",
      "Epoch 1246: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.188 | Test Acc: 0.727 | Test Loss: 0.373\n",
      "Epoch 1247: | Train Loss: 0.01021 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.737 | Test Loss: 0.371\n",
      "Epoch 1248: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.721 | Test Loss: 0.375\n",
      "Epoch 1249: | Train Loss: 0.01026 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.096 | Test Acc: 0.729 | Test Loss: 0.375\n",
      "Epoch 1250: | Train Loss: 0.01037 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.224 | Test Acc: 0.707 | Test Loss: 0.380\n",
      "Epoch 1251: | Train Loss: 0.01038 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.071 | Test Acc: 0.712 | Test Loss: 0.382\n",
      "Epoch 1252: | Train Loss: 0.01064 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.231 | Test Acc: 0.705 | Test Loss: 0.381\n",
      "Epoch 1253: | Train Loss: 0.01040 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.725 | Test Loss: 0.376\n",
      "Epoch 1254: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.200 | Test Acc: 0.718 | Test Loss: 0.376\n",
      "Epoch 1255: | Train Loss: 0.01027 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.737 | Test Loss: 0.371\n",
      "Epoch 1256: | Train Loss: 0.01023 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.181 | Test Acc: 0.731 | Test Loss: 0.371\n",
      "Epoch 1257: | Train Loss: 0.01018 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.154 | Test Acc: 0.739 | Test Loss: 0.369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1258: | Train Loss: 0.01017 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.735 | Test Loss: 0.370\n",
      "Epoch 1259: | Train Loss: 0.01016 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.737 | Test Loss: 0.369\n",
      "Epoch 1260: | Train Loss: 0.01016 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.168 | Test Acc: 0.738 | Test Loss: 0.369\n",
      "Epoch 1261: | Train Loss: 0.01016 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.735 | Test Loss: 0.370\n",
      "Epoch 1262: | Train Loss: 0.01016 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.154 | Test Acc: 0.739 | Test Loss: 0.369\n",
      "Epoch 1263: | Train Loss: 0.01016 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.181 | Test Acc: 0.732 | Test Loss: 0.371\n",
      "Epoch 1264: | Train Loss: 0.01018 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.738 | Test Loss: 0.370\n",
      "Epoch 1265: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.202 | Test Acc: 0.719 | Test Loss: 0.375\n",
      "Epoch 1266: | Train Loss: 0.01027 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.075 | Test Acc: 0.722 | Test Loss: 0.377\n",
      "Epoch 1267: | Train Loss: 0.01047 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.244 | Test Acc: 0.699 | Test Loss: 0.384\n",
      "Epoch 1268: | Train Loss: 0.01047 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.054 | Test Acc: 0.701 | Test Loss: 0.391\n",
      "Epoch 1269: | Train Loss: 0.01091 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.221 | Test Acc: 0.708 | Test Loss: 0.379\n",
      "Epoch 1270: | Train Loss: 0.01035 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.739 | Test Loss: 0.370\n",
      "Epoch 1271: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.737 | Test Loss: 0.369\n",
      "Epoch 1272: | Train Loss: 0.01014 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.178 | Test Acc: 0.732 | Test Loss: 0.371\n",
      "Epoch 1273: | Train Loss: 0.01016 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.736 | Test Loss: 0.371\n",
      "Epoch 1274: | Train Loss: 0.01026 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.221 | Test Acc: 0.712 | Test Loss: 0.378\n",
      "Epoch 1275: | Train Loss: 0.01034 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.067 | Test Acc: 0.712 | Test Loss: 0.383\n",
      "Epoch 1276: | Train Loss: 0.01067 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.231 | Test Acc: 0.706 | Test Loss: 0.380\n",
      "Epoch 1277: | Train Loss: 0.01037 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.731 | Test Loss: 0.373\n",
      "Epoch 1278: | Train Loss: 0.01033 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.188 | Test Acc: 0.727 | Test Loss: 0.372\n",
      "Epoch 1279: | Train Loss: 0.01019 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.158 | Test Acc: 0.740 | Test Loss: 0.369\n",
      "Epoch 1280: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.168 | Test Acc: 0.740 | Test Loss: 0.369\n",
      "Epoch 1281: | Train Loss: 0.01014 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.178 | Test Acc: 0.734 | Test Loss: 0.370\n",
      "Epoch 1282: | Train Loss: 0.01016 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.740 | Test Loss: 0.370\n",
      "Epoch 1283: | Train Loss: 0.01021 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.200 | Test Acc: 0.723 | Test Loss: 0.374\n",
      "Epoch 1284: | Train Loss: 0.01024 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.731 | Test Loss: 0.374\n",
      "Epoch 1285: | Train Loss: 0.01035 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.219 | Test Acc: 0.712 | Test Loss: 0.377\n",
      "Epoch 1286: | Train Loss: 0.01030 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.728 | Test Loss: 0.375\n",
      "Epoch 1287: | Train Loss: 0.01039 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.206 | Test Acc: 0.718 | Test Loss: 0.375\n",
      "Epoch 1288: | Train Loss: 0.01026 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.738 | Test Loss: 0.371\n",
      "Epoch 1289: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.185 | Test Acc: 0.731 | Test Loss: 0.371\n",
      "Epoch 1290: | Train Loss: 0.01017 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.154 | Test Acc: 0.741 | Test Loss: 0.368\n",
      "Epoch 1291: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.738 | Test Loss: 0.369\n",
      "Epoch 1292: | Train Loss: 0.01013 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.171 | Test Acc: 0.739 | Test Loss: 0.368\n",
      "Epoch 1293: | Train Loss: 0.01013 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.161 | Test Acc: 0.741 | Test Loss: 0.368\n",
      "Epoch 1294: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.178 | Test Acc: 0.734 | Test Loss: 0.370\n",
      "Epoch 1295: | Train Loss: 0.01015 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.741 | Test Loss: 0.369\n",
      "Epoch 1296: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.724 | Test Loss: 0.373\n",
      "Epoch 1297: | Train Loss: 0.01021 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.096 | Test Acc: 0.732 | Test Loss: 0.373\n",
      "Epoch 1298: | Train Loss: 0.01034 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.231 | Test Acc: 0.710 | Test Loss: 0.379\n",
      "Epoch 1299: | Train Loss: 0.01034 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.067 | Test Acc: 0.713 | Test Loss: 0.381\n",
      "Epoch 1300: | Train Loss: 0.01062 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.231 | Test Acc: 0.706 | Test Loss: 0.379\n",
      "Epoch 1301: | Train Loss: 0.01035 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.729 | Test Loss: 0.374\n",
      "Epoch 1302: | Train Loss: 0.01036 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.196 | Test Acc: 0.723 | Test Loss: 0.373\n",
      "Epoch 1303: | Train Loss: 0.01022 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.741 | Test Loss: 0.369\n",
      "Epoch 1304: | Train Loss: 0.01019 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.185 | Test Acc: 0.735 | Test Loss: 0.370\n",
      "Epoch 1305: | Train Loss: 0.01014 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.154 | Test Acc: 0.743 | Test Loss: 0.368\n",
      "Epoch 1306: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.174 | Test Acc: 0.737 | Test Loss: 0.369\n",
      "Epoch 1307: | Train Loss: 0.01012 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.160 | Test Acc: 0.743 | Test Loss: 0.367\n",
      "Epoch 1308: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.739 | Test Loss: 0.368\n",
      "Epoch 1309: | Train Loss: 0.01011 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.160 | Test Acc: 0.743 | Test Loss: 0.367\n",
      "Epoch 1310: | Train Loss: 0.01011 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.739 | Test Loss: 0.368\n",
      "Epoch 1311: | Train Loss: 0.01011 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.154 | Test Acc: 0.743 | Test Loss: 0.367\n",
      "Epoch 1312: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.185 | Test Acc: 0.735 | Test Loss: 0.370\n",
      "Epoch 1313: | Train Loss: 0.01013 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.741 | Test Loss: 0.369\n",
      "Epoch 1314: | Train Loss: 0.01018 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.219 | Test Acc: 0.718 | Test Loss: 0.375\n",
      "Epoch 1315: | Train Loss: 0.01025 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.076 | Test Acc: 0.717 | Test Loss: 0.379\n",
      "Epoch 1316: | Train Loss: 0.01055 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.254 | Test Acc: 0.694 | Test Loss: 0.386\n",
      "Epoch 1317: | Train Loss: 0.01051 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.050 | Test Acc: 0.695 | Test Loss: 0.394\n",
      "Epoch 1318: | Train Loss: 0.01102 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.221 | Test Acc: 0.710 | Test Loss: 0.377\n",
      "Epoch 1319: | Train Loss: 0.01029 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.164 | Test Acc: 0.744 | Test Loss: 0.367\n",
      "Epoch 1320: | Train Loss: 0.01011 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.139 | Test Acc: 0.743 | Test Loss: 0.367\n",
      "Epoch 1321: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.231 | Test Acc: 0.710 | Test Loss: 0.377\n",
      "Epoch 1322: | Train Loss: 0.01030 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.059 | Test Acc: 0.703 | Test Loss: 0.387\n",
      "Epoch 1323: | Train Loss: 0.01081 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.236 | Test Acc: 0.699 | Test Loss: 0.382\n",
      "Epoch 1324: | Train Loss: 0.01040 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.734 | Test Loss: 0.371\n",
      "Epoch 1325: | Train Loss: 0.01028 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.188 | Test Acc: 0.737 | Test Loss: 0.369\n",
      "Epoch 1326: | Train Loss: 0.01013 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.171 | Test Acc: 0.740 | Test Loss: 0.368\n",
      "Epoch 1327: | Train Loss: 0.01010 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.139 | Test Acc: 0.744 | Test Loss: 0.367\n",
      "Epoch 1328: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.196 | Test Acc: 0.726 | Test Loss: 0.372\n",
      "Epoch 1329: | Train Loss: 0.01019 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.733 | Test Loss: 0.374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1330: | Train Loss: 0.01036 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.224 | Test Acc: 0.720 | Test Loss: 0.376\n",
      "Epoch 1331: | Train Loss: 0.01028 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.733 | Test Loss: 0.373\n",
      "Epoch 1332: | Train Loss: 0.01034 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.198 | Test Acc: 0.724 | Test Loss: 0.372\n",
      "Epoch 1333: | Train Loss: 0.01018 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.744 | Test Loss: 0.367\n",
      "Epoch 1334: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.174 | Test Acc: 0.740 | Test Loss: 0.368\n",
      "Epoch 1335: | Train Loss: 0.01009 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.742 | Test Loss: 0.367\n",
      "Epoch 1336: | Train Loss: 0.01009 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.147 | Test Acc: 0.745 | Test Loss: 0.367\n",
      "Epoch 1337: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.192 | Test Acc: 0.729 | Test Loss: 0.371\n",
      "Epoch 1338: | Train Loss: 0.01014 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.741 | Test Loss: 0.369\n",
      "Epoch 1339: | Train Loss: 0.01020 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.199 | Test Acc: 0.725 | Test Loss: 0.373\n",
      "Epoch 1340: | Train Loss: 0.01020 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.738 | Test Loss: 0.371\n",
      "Epoch 1341: | Train Loss: 0.01028 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.209 | Test Acc: 0.725 | Test Loss: 0.373\n",
      "Epoch 1342: | Train Loss: 0.01021 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.739 | Test Loss: 0.370\n",
      "Epoch 1343: | Train Loss: 0.01023 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.198 | Test Acc: 0.726 | Test Loss: 0.371\n",
      "Epoch 1344: | Train Loss: 0.01016 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.743 | Test Loss: 0.367\n",
      "Epoch 1345: | Train Loss: 0.01016 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.195 | Test Acc: 0.731 | Test Loss: 0.370\n",
      "Epoch 1346: | Train Loss: 0.01013 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.745 | Test Loss: 0.367\n",
      "Epoch 1347: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.188 | Test Acc: 0.736 | Test Loss: 0.369\n",
      "Epoch 1348: | Train Loss: 0.01011 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.127 | Test Acc: 0.746 | Test Loss: 0.367\n",
      "Epoch 1349: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.736 | Test Loss: 0.369\n",
      "Epoch 1350: | Train Loss: 0.01012 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.119 | Test Acc: 0.744 | Test Loss: 0.368\n",
      "Epoch 1351: | Train Loss: 0.01016 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.199 | Test Acc: 0.726 | Test Loss: 0.372\n",
      "Epoch 1352: | Train Loss: 0.01018 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.096 | Test Acc: 0.732 | Test Loss: 0.372\n",
      "Epoch 1353: | Train Loss: 0.01032 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.233 | Test Acc: 0.708 | Test Loss: 0.378\n",
      "Epoch 1354: | Train Loss: 0.01032 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.067 | Test Acc: 0.713 | Test Loss: 0.381\n",
      "Epoch 1355: | Train Loss: 0.01062 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.236 | Test Acc: 0.704 | Test Loss: 0.379\n",
      "Epoch 1356: | Train Loss: 0.01034 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.729 | Test Loss: 0.372\n",
      "Epoch 1357: | Train Loss: 0.01035 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.202 | Test Acc: 0.723 | Test Loss: 0.372\n",
      "Epoch 1358: | Train Loss: 0.01018 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.744 | Test Loss: 0.367\n",
      "Epoch 1359: | Train Loss: 0.01015 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.195 | Test Acc: 0.738 | Test Loss: 0.368\n",
      "Epoch 1360: | Train Loss: 0.01010 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.135 | Test Acc: 0.747 | Test Loss: 0.366\n",
      "Epoch 1361: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.188 | Test Acc: 0.742 | Test Loss: 0.367\n",
      "Epoch 1362: | Train Loss: 0.01009 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.151 | Test Acc: 0.747 | Test Loss: 0.366\n",
      "Epoch 1363: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.188 | Test Acc: 0.739 | Test Loss: 0.368\n",
      "Epoch 1364: | Train Loss: 0.01008 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.139 | Test Acc: 0.746 | Test Loss: 0.366\n",
      "Epoch 1365: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.195 | Test Acc: 0.731 | Test Loss: 0.369\n",
      "Epoch 1366: | Train Loss: 0.01011 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.740 | Test Loss: 0.368\n",
      "Epoch 1367: | Train Loss: 0.01018 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.225 | Test Acc: 0.716 | Test Loss: 0.375\n",
      "Epoch 1368: | Train Loss: 0.01023 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.076 | Test Acc: 0.719 | Test Loss: 0.377\n",
      "Epoch 1369: | Train Loss: 0.01051 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.247 | Test Acc: 0.704 | Test Loss: 0.381\n",
      "Epoch 1370: | Train Loss: 0.01038 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.067 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 1371: | Train Loss: 0.01061 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.225 | Test Acc: 0.713 | Test Loss: 0.375\n",
      "Epoch 1372: | Train Loss: 0.01025 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.115 | Test Acc: 0.742 | Test Loss: 0.367\n",
      "Epoch 1373: | Train Loss: 0.01016 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.188 | Test Acc: 0.736 | Test Loss: 0.368\n",
      "Epoch 1374: | Train Loss: 0.01008 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.165 | Test Acc: 0.748 | Test Loss: 0.365\n",
      "Epoch 1375: | Train Loss: 0.01005 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.178 | Test Acc: 0.747 | Test Loss: 0.365\n",
      "Epoch 1376: | Train Loss: 0.01004 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.178 | Test Acc: 0.743 | Test Loss: 0.366\n",
      "Epoch 1377: | Train Loss: 0.01005 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.161 | Test Acc: 0.748 | Test Loss: 0.365\n",
      "Epoch 1378: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.188 | Test Acc: 0.734 | Test Loss: 0.368\n",
      "Epoch 1379: | Train Loss: 0.01009 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.112 | Test Acc: 0.743 | Test Loss: 0.367\n",
      "Epoch 1380: | Train Loss: 0.01015 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.225 | Test Acc: 0.720 | Test Loss: 0.374\n",
      "Epoch 1381: | Train Loss: 0.01021 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.076 | Test Acc: 0.723 | Test Loss: 0.377\n",
      "Epoch 1382: | Train Loss: 0.01048 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.247 | Test Acc: 0.709 | Test Loss: 0.379\n",
      "Epoch 1383: | Train Loss: 0.01035 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.067 | Test Acc: 0.719 | Test Loss: 0.378\n",
      "Epoch 1384: | Train Loss: 0.01055 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.225 | Test Acc: 0.717 | Test Loss: 0.374\n",
      "Epoch 1385: | Train Loss: 0.01022 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.742 | Test Loss: 0.367\n",
      "Epoch 1386: | Train Loss: 0.01015 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.188 | Test Acc: 0.734 | Test Loss: 0.368\n",
      "Epoch 1387: | Train Loss: 0.01008 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.169 | Test Acc: 0.749 | Test Loss: 0.364\n",
      "Epoch 1388: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.178 | Test Acc: 0.745 | Test Loss: 0.365\n",
      "Epoch 1389: | Train Loss: 0.01004 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.178 | Test Acc: 0.748 | Test Loss: 0.364\n",
      "Epoch 1390: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.171 | Test Acc: 0.749 | Test Loss: 0.364\n",
      "Epoch 1391: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.178 | Test Acc: 0.745 | Test Loss: 0.365\n",
      "Epoch 1392: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.169 | Test Acc: 0.749 | Test Loss: 0.364\n",
      "Epoch 1393: | Train Loss: 0.01004 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.188 | Test Acc: 0.737 | Test Loss: 0.367\n",
      "Epoch 1394: | Train Loss: 0.01006 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.127 | Test Acc: 0.745 | Test Loss: 0.365\n",
      "Epoch 1395: | Train Loss: 0.01011 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.218 | Test Acc: 0.720 | Test Loss: 0.373\n",
      "Epoch 1396: | Train Loss: 0.01018 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.075 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1397: | Train Loss: 0.01046 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.253 | Test Acc: 0.697 | Test Loss: 0.383\n",
      "Epoch 1398: | Train Loss: 0.01045 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.054 | Test Acc: 0.701 | Test Loss: 0.388\n",
      "Epoch 1399: | Train Loss: 0.01088 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.231 | Test Acc: 0.708 | Test Loss: 0.377\n",
      "Epoch 1400: | Train Loss: 0.01028 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.747 | Test Loss: 0.365\n",
      "Epoch 1401: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.164 | Test Acc: 0.746 | Test Loss: 0.366\n",
      "Epoch 1402: | Train Loss: 0.01006 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.200 | Test Acc: 0.742 | Test Loss: 0.369\n",
      "Epoch 1403: | Train Loss: 0.01011 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.084 | Test Acc: 0.734 | Test Loss: 0.373\n",
      "Epoch 1404: | Train Loss: 0.01037 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.251 | Test Acc: 0.711 | Test Loss: 0.379\n",
      "Epoch 1405: | Train Loss: 0.01036 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.059 | Test Acc: 0.712 | Test Loss: 0.383\n",
      "Epoch 1406: | Train Loss: 0.01069 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.215 | Test Acc: 0.719 | Test Loss: 0.373\n",
      "Epoch 1407: | Train Loss: 0.01018 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.750 | Test Loss: 0.364\n",
      "Epoch 1408: | Train Loss: 0.01005 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.139 | Test Acc: 0.750 | Test Loss: 0.366\n",
      "Epoch 1409: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.225 | Test Acc: 0.735 | Test Loss: 0.372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1410: | Train Loss: 0.01019 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.067 | Test Acc: 0.726 | Test Loss: 0.378\n",
      "Epoch 1411: | Train Loss: 0.01053 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.233 | Test Acc: 0.721 | Test Loss: 0.374\n",
      "Epoch 1412: | Train Loss: 0.01023 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.742 | Test Loss: 0.367\n",
      "Epoch 1413: | Train Loss: 0.01016 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.181 | Test Acc: 0.736 | Test Loss: 0.367\n",
      "Epoch 1414: | Train Loss: 0.01006 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.178 | Test Acc: 0.751 | Test Loss: 0.364\n",
      "Epoch 1415: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.139 | Test Acc: 0.751 | Test Loss: 0.365\n",
      "Epoch 1416: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.200 | Test Acc: 0.739 | Test Loss: 0.369\n",
      "Epoch 1417: | Train Loss: 0.01011 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.092 | Test Acc: 0.741 | Test Loss: 0.369\n",
      "Epoch 1418: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.222 | Test Acc: 0.724 | Test Loss: 0.372\n",
      "Epoch 1419: | Train Loss: 0.01018 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.738 | Test Loss: 0.368\n",
      "Epoch 1420: | Train Loss: 0.01023 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.726 | Test Loss: 0.370\n",
      "Epoch 1421: | Train Loss: 0.01012 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.143 | Test Acc: 0.749 | Test Loss: 0.364\n",
      "Epoch 1422: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.181 | Test Acc: 0.742 | Test Loss: 0.366\n",
      "Epoch 1423: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.751 | Test Loss: 0.363\n",
      "Epoch 1424: | Train Loss: 0.01001 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.165 | Test Acc: 0.751 | Test Loss: 0.363\n",
      "Epoch 1425: | Train Loss: 0.01001 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.184 | Test Acc: 0.743 | Test Loss: 0.366\n",
      "Epoch 1426: | Train Loss: 0.01003 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.123 | Test Acc: 0.749 | Test Loss: 0.364\n",
      "Epoch 1427: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.200 | Test Acc: 0.728 | Test Loss: 0.370\n",
      "Epoch 1428: | Train Loss: 0.01011 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.738 | Test Loss: 0.368\n",
      "Epoch 1429: | Train Loss: 0.01022 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.225 | Test Acc: 0.717 | Test Loss: 0.373\n",
      "Epoch 1430: | Train Loss: 0.01020 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.731 | Test Loss: 0.371\n",
      "Epoch 1431: | Train Loss: 0.01032 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.225 | Test Acc: 0.718 | Test Loss: 0.373\n",
      "Epoch 1432: | Train Loss: 0.01019 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.738 | Test Loss: 0.368\n",
      "Epoch 1433: | Train Loss: 0.01022 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.203 | Test Acc: 0.728 | Test Loss: 0.370\n",
      "Epoch 1434: | Train Loss: 0.01011 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.747 | Test Loss: 0.365\n",
      "Epoch 1435: | Train Loss: 0.01010 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.737 | Test Loss: 0.367\n",
      "Epoch 1436: | Train Loss: 0.01005 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.123 | Test Acc: 0.750 | Test Loss: 0.364\n",
      "Epoch 1437: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.197 | Test Acc: 0.741 | Test Loss: 0.366\n",
      "Epoch 1438: | Train Loss: 0.01004 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.115 | Test Acc: 0.751 | Test Loss: 0.365\n",
      "Epoch 1439: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.206 | Test Acc: 0.741 | Test Loss: 0.368\n",
      "Epoch 1440: | Train Loss: 0.01009 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.100 | Test Acc: 0.748 | Test Loss: 0.369\n",
      "Epoch 1441: | Train Loss: 0.01020 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.246 | Test Acc: 0.738 | Test Loss: 0.372\n",
      "Epoch 1442: | Train Loss: 0.01019 | Train F1: 0.007 | Train Acc: 0.021| Test F1: 0.080 | Test Acc: 0.743 | Test Loss: 0.375\n",
      "Epoch 1443: | Train Loss: 0.01040 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.249 | Test Acc: 0.740 | Test Loss: 0.373\n",
      "Epoch 1444: | Train Loss: 0.01024 | Train F1: 0.007 | Train Acc: 0.021| Test F1: 0.092 | Test Acc: 0.749 | Test Loss: 0.374\n",
      "Epoch 1445: | Train Loss: 0.01034 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.225 | Test Acc: 0.747 | Test Loss: 0.370\n",
      "Epoch 1446: | Train Loss: 0.01016 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.112 | Test Acc: 0.753 | Test Loss: 0.368\n",
      "Epoch 1447: | Train Loss: 0.01015 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.202 | Test Acc: 0.750 | Test Loss: 0.366\n",
      "Epoch 1448: | Train Loss: 0.01006 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.154 | Test Acc: 0.753 | Test Loss: 0.364\n",
      "Epoch 1449: | Train Loss: 0.01003 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.181 | Test Acc: 0.750 | Test Loss: 0.364\n",
      "Epoch 1450: | Train Loss: 0.01000 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.169 | Test Acc: 0.753 | Test Loss: 0.362\n",
      "Epoch 1451: | Train Loss: 0.00999 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.181 | Test Acc: 0.745 | Test Loss: 0.364\n",
      "Epoch 1452: | Train Loss: 0.00999 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.166 | Test Acc: 0.752 | Test Loss: 0.363\n",
      "Epoch 1453: | Train Loss: 0.01002 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.198 | Test Acc: 0.728 | Test Loss: 0.369\n",
      "Epoch 1454: | Train Loss: 0.01009 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.737 | Test Loss: 0.367\n",
      "Epoch 1455: | Train Loss: 0.01021 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.240 | Test Acc: 0.693 | Test Loss: 0.384\n",
      "Epoch 1456: | Train Loss: 0.01044 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.063 | Test Acc: 0.705 | Test Loss: 0.384\n",
      "Epoch 1457: | Train Loss: 0.01075 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.249 | Test Acc: 0.690 | Test Loss: 0.385\n",
      "Epoch 1458: | Train Loss: 0.01048 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.725 | Test Loss: 0.373\n",
      "Epoch 1459: | Train Loss: 0.01039 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.215 | Test Acc: 0.735 | Test Loss: 0.368\n",
      "Epoch 1460: | Train Loss: 0.01008 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.143 | Test Acc: 0.754 | Test Loss: 0.364\n",
      "Epoch 1461: | Train Loss: 0.01003 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.185 | Test Acc: 0.754 | Test Loss: 0.364\n",
      "Epoch 1462: | Train Loss: 0.01003 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.738 | Test Loss: 0.368\n",
      "Epoch 1463: | Train Loss: 0.01008 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.173 | Test Acc: 0.752 | Test Loss: 0.363\n",
      "Epoch 1464: | Train Loss: 0.01003 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.198 | Test Acc: 0.737 | Test Loss: 0.366\n",
      "Epoch 1465: | Train Loss: 0.01003 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.112 | Test Acc: 0.749 | Test Loss: 0.365\n",
      "Epoch 1466: | Train Loss: 0.01011 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.235 | Test Acc: 0.740 | Test Loss: 0.370\n",
      "Epoch 1467: | Train Loss: 0.01015 | Train F1: 0.007 | Train Acc: 0.021| Test F1: 0.088 | Test Acc: 0.746 | Test Loss: 0.374\n",
      "Epoch 1468: | Train Loss: 0.01035 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.241 | Test Acc: 0.742 | Test Loss: 0.371\n",
      "Epoch 1469: | Train Loss: 0.01017 | Train F1: 0.007 | Train Acc: 0.021| Test F1: 0.112 | Test Acc: 0.751 | Test Loss: 0.367\n",
      "Epoch 1470: | Train Loss: 0.01015 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.201 | Test Acc: 0.743 | Test Loss: 0.366\n",
      "Epoch 1471: | Train Loss: 0.01002 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.162 | Test Acc: 0.753 | Test Loss: 0.362\n",
      "Epoch 1472: | Train Loss: 0.01000 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.181 | Test Acc: 0.742 | Test Loss: 0.365\n",
      "Epoch 1473: | Train Loss: 0.01001 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.175 | Test Acc: 0.754 | Test Loss: 0.362\n",
      "Epoch 1474: | Train Loss: 0.01000 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.747 | Test Loss: 0.364\n",
      "Epoch 1475: | Train Loss: 0.00999 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.755 | Test Loss: 0.362\n",
      "Epoch 1476: | Train Loss: 0.00996 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.754 | Test Loss: 0.362\n",
      "Epoch 1477: | Train Loss: 0.00995 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.178 | Test Acc: 0.752 | Test Loss: 0.362\n",
      "Epoch 1478: | Train Loss: 0.00995 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.755 | Test Loss: 0.361\n",
      "Epoch 1479: | Train Loss: 0.00997 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.181 | Test Acc: 0.745 | Test Loss: 0.364\n",
      "Epoch 1480: | Train Loss: 0.00999 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.177 | Test Acc: 0.753 | Test Loss: 0.362\n",
      "Epoch 1481: | Train Loss: 0.00999 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.198 | Test Acc: 0.735 | Test Loss: 0.366\n",
      "Epoch 1482: | Train Loss: 0.01003 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.112 | Test Acc: 0.744 | Test Loss: 0.365\n",
      "Epoch 1483: | Train Loss: 0.01012 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.244 | Test Acc: 0.717 | Test Loss: 0.375\n",
      "Epoch 1484: | Train Loss: 0.01023 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.054 | Test Acc: 0.709 | Test Loss: 0.385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1485: | Train Loss: 0.01077 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.253 | Test Acc: 0.711 | Test Loss: 0.377\n",
      "Epoch 1486: | Train Loss: 0.01029 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.732 | Test Loss: 0.369\n",
      "Epoch 1487: | Train Loss: 0.01028 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.218 | Test Acc: 0.723 | Test Loss: 0.371\n",
      "Epoch 1488: | Train Loss: 0.01012 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.744 | Test Loss: 0.364\n",
      "Epoch 1489: | Train Loss: 0.01011 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.209 | Test Acc: 0.728 | Test Loss: 0.369\n",
      "Epoch 1490: | Train Loss: 0.01008 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.743 | Test Loss: 0.365\n",
      "Epoch 1491: | Train Loss: 0.01014 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.243 | Test Acc: 0.726 | Test Loss: 0.371\n",
      "Epoch 1492: | Train Loss: 0.01014 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.080 | Test Acc: 0.731 | Test Loss: 0.372\n",
      "Epoch 1493: | Train Loss: 0.01037 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.250 | Test Acc: 0.726 | Test Loss: 0.373\n",
      "Epoch 1494: | Train Loss: 0.01020 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.080 | Test Acc: 0.736 | Test Loss: 0.372\n",
      "Epoch 1495: | Train Loss: 0.01034 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.237 | Test Acc: 0.737 | Test Loss: 0.369\n",
      "Epoch 1496: | Train Loss: 0.01010 | Train F1: 0.007 | Train Acc: 0.021| Test F1: 0.112 | Test Acc: 0.752 | Test Loss: 0.364\n",
      "Epoch 1497: | Train Loss: 0.01007 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.201 | Test Acc: 0.746 | Test Loss: 0.364\n",
      "Epoch 1498: | Train Loss: 0.00999 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.159 | Test Acc: 0.756 | Test Loss: 0.361\n",
      "Epoch 1499: | Train Loss: 0.00997 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.191 | Test Acc: 0.749 | Test Loss: 0.363\n"
     ]
    }
   ],
   "source": [
    "# Training separate models\n",
    "args.lr = 0.05\n",
    "classifiers = [NNClassifier(args) for _ in range(1)]\n",
    "\n",
    "# optimizers = [torch.optim.Adam(classifiers[i].classifier.parameters(), lr=args.lr) for i in range(args.n_clusters)]\n",
    "EPOCHS = 1500\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "\n",
    "latents_X = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id_train = model.clustering.update_assign(latents_X.detach().numpy())\n",
    "X_latents_data_loader = list(zip(latents_X, cluster_id_train, y_train))\n",
    "train_loader_latents = torch.utils.data.DataLoader(X_latents_data_loader,\n",
    "    batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "latents_test = model.autoencoder(torch.FloatTensor(np.array(X_test)).to(args.device), latent=True)\n",
    "\n",
    "# plot(latents_X, y_train, latents_test, y_test)\n",
    "\n",
    "for e in range(1, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    alpha = (1-e/EPOCHS)\n",
    "    acc = 0\n",
    "\n",
    "    new_y = []\n",
    "    y_pred = []\n",
    "    y_pred_idx, loss = classifiers[0].fit(latents_X, torch.tensor(y_train).to(device))\n",
    "    y_pred.append(y_pred_idx)\n",
    "    new_y.append(y_train)\n",
    "    epoch_loss += loss\n",
    "    y_pred = np.vstack(y_pred)\n",
    "    new_y = np.hstack(new_y)\n",
    "\n",
    "    f1 = f1_score(np.argmax(y_pred, axis=1), new_y)\n",
    "    acc = roc_auc_score(new_y, y_pred[:,1])\n",
    "    epoch_acc += acc.item()\n",
    "    epoch_f1 += f1.item()\n",
    "\n",
    "    test_preds = []\n",
    "    test_loss = 0.0\n",
    "    new_y_test = []\n",
    "    classifiers[0].classifier.eval()\n",
    "    latents_idx = latents_test\n",
    "    y_pred_idx = classifiers[0](latents_idx)\n",
    "    test_loss += nn.CrossEntropyLoss(reduction='mean')(y_pred_idx, torch.tensor(y_test).to(device))\n",
    "    test_preds.append(y_pred_idx.detach().numpy())\n",
    "    new_y_test.append(y_test)\n",
    "\n",
    "    test_preds = np.vstack(test_preds)\n",
    "    new_y_test = np.hstack(new_y_test).reshape(-1)\n",
    "    test_f1 = f1_score(np.argmax(test_preds, axis=1), new_y_test)\n",
    "    test_acc = roc_auc_score(new_y_test, test_preds[:,1])\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {epoch_loss/len(train_loader):.5f} | Train F1: {epoch_f1/len(train_loader):.3f} | Train Acc: {epoch_acc/len(train_loader):.3f}| Test F1: {test_f1:.3f} | Test Acc: {test_acc:.3f} | Test Loss: {test_loss:.3f}')\n",
    "\n",
    "out = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id = model.clustering.update_assign(out.cpu().detach().numpy())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Approch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNClassifier(\n",
      "  (criterion): CrossEntropyLoss()\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 001: | Train Loss: 0.66446 | Train F1: 0.434 | Train Acc: 0.707| Test F1: 0.848 | Test Acc: 0.761 | Test Loss: 0.559\n",
      "Epoch 002: | Train Loss: 0.46852 | Train F1: 0.857 | Train Acc: 0.840| Test F1: 0.866 | Test Acc: 0.884 | Test Loss: 0.404\n",
      "Epoch 003: | Train Loss: 0.40044 | Train F1: 0.870 | Train Acc: 0.880| Test F1: 0.877 | Test Acc: 0.897 | Test Loss: 0.380\n",
      "Epoch 004: | Train Loss: 0.38454 | Train F1: 0.877 | Train Acc: 0.890| Test F1: 0.885 | Test Acc: 0.901 | Test Loss: 0.366\n",
      "Epoch 005: | Train Loss: 0.37209 | Train F1: 0.884 | Train Acc: 0.896| Test F1: 0.890 | Test Acc: 0.905 | Test Loss: 0.355\n",
      "Epoch 006: | Train Loss: 0.36119 | Train F1: 0.889 | Train Acc: 0.901| Test F1: 0.891 | Test Acc: 0.909 | Test Loss: 0.346\n",
      "Epoch 007: | Train Loss: 0.35325 | Train F1: 0.894 | Train Acc: 0.904| Test F1: 0.894 | Test Acc: 0.908 | Test Loss: 0.342\n",
      "Epoch 008: | Train Loss: 0.34803 | Train F1: 0.897 | Train Acc: 0.906| Test F1: 0.898 | Test Acc: 0.915 | Test Loss: 0.334\n",
      "Epoch 009: | Train Loss: 0.34385 | Train F1: 0.896 | Train Acc: 0.909| Test F1: 0.899 | Test Acc: 0.916 | Test Loss: 0.332\n",
      "Epoch 010: | Train Loss: 0.34037 | Train F1: 0.899 | Train Acc: 0.910| Test F1: 0.901 | Test Acc: 0.917 | Test Loss: 0.329\n",
      "Epoch 011: | Train Loss: 0.33756 | Train F1: 0.899 | Train Acc: 0.912| Test F1: 0.901 | Test Acc: 0.920 | Test Loss: 0.326\n",
      "Epoch 012: | Train Loss: 0.33515 | Train F1: 0.900 | Train Acc: 0.914| Test F1: 0.902 | Test Acc: 0.921 | Test Loss: 0.324\n",
      "Epoch 013: | Train Loss: 0.33193 | Train F1: 0.900 | Train Acc: 0.915| Test F1: 0.903 | Test Acc: 0.921 | Test Loss: 0.323\n",
      "Epoch 014: | Train Loss: 0.33016 | Train F1: 0.901 | Train Acc: 0.916| Test F1: 0.904 | Test Acc: 0.922 | Test Loss: 0.322\n",
      "Epoch 015: | Train Loss: 0.32906 | Train F1: 0.903 | Train Acc: 0.915| Test F1: 0.905 | Test Acc: 0.924 | Test Loss: 0.321\n",
      "Epoch 016: | Train Loss: 0.32819 | Train F1: 0.902 | Train Acc: 0.917| Test F1: 0.904 | Test Acc: 0.924 | Test Loss: 0.320\n",
      "Epoch 017: | Train Loss: 0.32581 | Train F1: 0.903 | Train Acc: 0.918| Test F1: 0.902 | Test Acc: 0.925 | Test Loss: 0.317\n",
      "Epoch 018: | Train Loss: 0.32433 | Train F1: 0.902 | Train Acc: 0.919| Test F1: 0.905 | Test Acc: 0.924 | Test Loss: 0.317\n",
      "Epoch 019: | Train Loss: 0.32381 | Train F1: 0.901 | Train Acc: 0.920| Test F1: 0.904 | Test Acc: 0.925 | Test Loss: 0.316\n",
      "Epoch 020: | Train Loss: 0.32172 | Train F1: 0.903 | Train Acc: 0.919| Test F1: 0.905 | Test Acc: 0.926 | Test Loss: 0.315\n",
      "Epoch 021: | Train Loss: 0.32128 | Train F1: 0.903 | Train Acc: 0.920| Test F1: 0.905 | Test Acc: 0.923 | Test Loss: 0.318\n",
      "Epoch 022: | Train Loss: 0.32044 | Train F1: 0.903 | Train Acc: 0.921| Test F1: 0.906 | Test Acc: 0.926 | Test Loss: 0.314\n",
      "Epoch 023: | Train Loss: 0.31915 | Train F1: 0.903 | Train Acc: 0.921| Test F1: 0.905 | Test Acc: 0.927 | Test Loss: 0.313\n",
      "Epoch 024: | Train Loss: 0.31892 | Train F1: 0.903 | Train Acc: 0.921| Test F1: 0.906 | Test Acc: 0.927 | Test Loss: 0.313\n",
      "Epoch 025: | Train Loss: 0.31741 | Train F1: 0.904 | Train Acc: 0.922| Test F1: 0.905 | Test Acc: 0.927 | Test Loss: 0.312\n",
      "Epoch 026: | Train Loss: 0.31723 | Train F1: 0.904 | Train Acc: 0.922| Test F1: 0.907 | Test Acc: 0.927 | Test Loss: 0.311\n",
      "Epoch 027: | Train Loss: 0.31598 | Train F1: 0.905 | Train Acc: 0.922| Test F1: 0.906 | Test Acc: 0.929 | Test Loss: 0.310\n",
      "Epoch 028: | Train Loss: 0.31550 | Train F1: 0.904 | Train Acc: 0.923| Test F1: 0.905 | Test Acc: 0.929 | Test Loss: 0.309\n",
      "Epoch 029: | Train Loss: 0.31480 | Train F1: 0.904 | Train Acc: 0.923| Test F1: 0.907 | Test Acc: 0.928 | Test Loss: 0.309\n",
      "Epoch 030: | Train Loss: 0.31448 | Train F1: 0.906 | Train Acc: 0.923| Test F1: 0.907 | Test Acc: 0.929 | Test Loss: 0.308\n",
      "Epoch 031: | Train Loss: 0.31338 | Train F1: 0.905 | Train Acc: 0.923| Test F1: 0.908 | Test Acc: 0.929 | Test Loss: 0.307\n",
      "Epoch 032: | Train Loss: 0.31233 | Train F1: 0.905 | Train Acc: 0.924| Test F1: 0.904 | Test Acc: 0.929 | Test Loss: 0.311\n",
      "Epoch 033: | Train Loss: 0.31287 | Train F1: 0.904 | Train Acc: 0.925| Test F1: 0.909 | Test Acc: 0.929 | Test Loss: 0.306\n",
      "Epoch 034: | Train Loss: 0.31178 | Train F1: 0.905 | Train Acc: 0.925| Test F1: 0.909 | Test Acc: 0.929 | Test Loss: 0.306\n",
      "Epoch 035: | Train Loss: 0.31194 | Train F1: 0.906 | Train Acc: 0.925| Test F1: 0.908 | Test Acc: 0.929 | Test Loss: 0.308\n",
      "Epoch 036: | Train Loss: 0.31110 | Train F1: 0.904 | Train Acc: 0.925| Test F1: 0.910 | Test Acc: 0.930 | Test Loss: 0.305\n",
      "Epoch 037: | Train Loss: 0.31145 | Train F1: 0.905 | Train Acc: 0.924| Test F1: 0.907 | Test Acc: 0.931 | Test Loss: 0.304\n",
      "Epoch 038: | Train Loss: 0.31104 | Train F1: 0.905 | Train Acc: 0.925| Test F1: 0.908 | Test Acc: 0.931 | Test Loss: 0.304\n",
      "Epoch 039: | Train Loss: 0.31005 | Train F1: 0.905 | Train Acc: 0.925| Test F1: 0.909 | Test Acc: 0.931 | Test Loss: 0.304\n",
      "Epoch 040: | Train Loss: 0.30879 | Train F1: 0.905 | Train Acc: 0.927| Test F1: 0.906 | Test Acc: 0.931 | Test Loss: 0.305\n",
      "Epoch 041: | Train Loss: 0.30870 | Train F1: 0.905 | Train Acc: 0.927| Test F1: 0.909 | Test Acc: 0.931 | Test Loss: 0.303\n",
      "Epoch 042: | Train Loss: 0.30829 | Train F1: 0.906 | Train Acc: 0.927| Test F1: 0.909 | Test Acc: 0.932 | Test Loss: 0.302\n",
      "Epoch 043: | Train Loss: 0.30677 | Train F1: 0.906 | Train Acc: 0.927| Test F1: 0.909 | Test Acc: 0.932 | Test Loss: 0.301\n",
      "Epoch 044: | Train Loss: 0.30800 | Train F1: 0.907 | Train Acc: 0.927| Test F1: 0.909 | Test Acc: 0.932 | Test Loss: 0.302\n",
      "Epoch 045: | Train Loss: 0.30594 | Train F1: 0.907 | Train Acc: 0.928| Test F1: 0.906 | Test Acc: 0.932 | Test Loss: 0.302\n",
      "Epoch 046: | Train Loss: 0.30689 | Train F1: 0.906 | Train Acc: 0.928| Test F1: 0.909 | Test Acc: 0.931 | Test Loss: 0.304\n",
      "Epoch 047: | Train Loss: 0.30671 | Train F1: 0.906 | Train Acc: 0.927| Test F1: 0.908 | Test Acc: 0.933 | Test Loss: 0.301\n",
      "Epoch 048: | Train Loss: 0.30598 | Train F1: 0.907 | Train Acc: 0.928| Test F1: 0.907 | Test Acc: 0.933 | Test Loss: 0.302\n",
      "Epoch 049: | Train Loss: 0.30505 | Train F1: 0.907 | Train Acc: 0.928| Test F1: 0.909 | Test Acc: 0.933 | Test Loss: 0.300\n",
      "Epoch 050: | Train Loss: 0.30489 | Train F1: 0.907 | Train Acc: 0.928| Test F1: 0.907 | Test Acc: 0.933 | Test Loss: 0.302\n",
      "Epoch 051: | Train Loss: 0.30459 | Train F1: 0.907 | Train Acc: 0.929| Test F1: 0.908 | Test Acc: 0.933 | Test Loss: 0.300\n",
      "Epoch 052: | Train Loss: 0.30444 | Train F1: 0.907 | Train Acc: 0.929| Test F1: 0.909 | Test Acc: 0.933 | Test Loss: 0.300\n",
      "Epoch 053: | Train Loss: 0.30468 | Train F1: 0.907 | Train Acc: 0.928| Test F1: 0.908 | Test Acc: 0.933 | Test Loss: 0.300\n",
      "Epoch 054: | Train Loss: 0.30350 | Train F1: 0.907 | Train Acc: 0.929| Test F1: 0.908 | Test Acc: 0.934 | Test Loss: 0.299\n",
      "Epoch 055: | Train Loss: 0.30461 | Train F1: 0.908 | Train Acc: 0.929| Test F1: 0.908 | Test Acc: 0.933 | Test Loss: 0.301\n",
      "Epoch 056: | Train Loss: 0.30283 | Train F1: 0.907 | Train Acc: 0.929| Test F1: 0.908 | Test Acc: 0.934 | Test Loss: 0.299\n",
      "Epoch 057: | Train Loss: 0.30353 | Train F1: 0.907 | Train Acc: 0.930| Test F1: 0.906 | Test Acc: 0.930 | Test Loss: 0.304\n",
      "Epoch 058: | Train Loss: 0.30417 | Train F1: 0.907 | Train Acc: 0.929| Test F1: 0.908 | Test Acc: 0.933 | Test Loss: 0.299\n",
      "Epoch 059: | Train Loss: 0.30237 | Train F1: 0.907 | Train Acc: 0.930| Test F1: 0.909 | Test Acc: 0.934 | Test Loss: 0.298\n",
      "Epoch 060: | Train Loss: 0.30252 | Train F1: 0.907 | Train Acc: 0.929| Test F1: 0.907 | Test Acc: 0.934 | Test Loss: 0.299\n",
      "Epoch 061: | Train Loss: 0.30233 | Train F1: 0.907 | Train Acc: 0.930| Test F1: 0.908 | Test Acc: 0.934 | Test Loss: 0.298\n",
      "Epoch 062: | Train Loss: 0.30229 | Train F1: 0.908 | Train Acc: 0.930| Test F1: 0.910 | Test Acc: 0.935 | Test Loss: 0.298\n",
      "Epoch 063: | Train Loss: 0.30136 | Train F1: 0.907 | Train Acc: 0.930| Test F1: 0.909 | Test Acc: 0.935 | Test Loss: 0.299\n",
      "Epoch 064: | Train Loss: 0.30226 | Train F1: 0.908 | Train Acc: 0.930| Test F1: 0.908 | Test Acc: 0.935 | Test Loss: 0.299\n",
      "Epoch 065: | Train Loss: 0.30134 | Train F1: 0.908 | Train Acc: 0.930| Test F1: 0.909 | Test Acc: 0.934 | Test Loss: 0.299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 066: | Train Loss: 0.30134 | Train F1: 0.908 | Train Acc: 0.930| Test F1: 0.909 | Test Acc: 0.935 | Test Loss: 0.298\n",
      "Epoch 067: | Train Loss: 0.30077 | Train F1: 0.908 | Train Acc: 0.931| Test F1: 0.908 | Test Acc: 0.934 | Test Loss: 0.298\n",
      "Epoch 068: | Train Loss: 0.30007 | Train F1: 0.907 | Train Acc: 0.931| Test F1: 0.909 | Test Acc: 0.934 | Test Loss: 0.300\n",
      "Epoch 069: | Train Loss: 0.30084 | Train F1: 0.909 | Train Acc: 0.931| Test F1: 0.909 | Test Acc: 0.934 | Test Loss: 0.299\n",
      "Epoch 070: | Train Loss: 0.30021 | Train F1: 0.908 | Train Acc: 0.930| Test F1: 0.910 | Test Acc: 0.935 | Test Loss: 0.295\n",
      "Epoch 071: | Train Loss: 0.30044 | Train F1: 0.908 | Train Acc: 0.931| Test F1: 0.910 | Test Acc: 0.935 | Test Loss: 0.297\n",
      "Epoch 072: | Train Loss: 0.30124 | Train F1: 0.908 | Train Acc: 0.931| Test F1: 0.911 | Test Acc: 0.935 | Test Loss: 0.298\n",
      "Epoch 073: | Train Loss: 0.30042 | Train F1: 0.909 | Train Acc: 0.931| Test F1: 0.909 | Test Acc: 0.934 | Test Loss: 0.297\n",
      "Epoch 074: | Train Loss: 0.30028 | Train F1: 0.908 | Train Acc: 0.931| Test F1: 0.907 | Test Acc: 0.935 | Test Loss: 0.297\n",
      "Epoch 075: | Train Loss: 0.30057 | Train F1: 0.907 | Train Acc: 0.931| Test F1: 0.911 | Test Acc: 0.935 | Test Loss: 0.296\n",
      "Epoch 076: | Train Loss: 0.30071 | Train F1: 0.907 | Train Acc: 0.931| Test F1: 0.908 | Test Acc: 0.934 | Test Loss: 0.299\n",
      "Epoch 077: | Train Loss: 0.29909 | Train F1: 0.908 | Train Acc: 0.931| Test F1: 0.908 | Test Acc: 0.935 | Test Loss: 0.298\n",
      "Epoch 078: | Train Loss: 0.29925 | Train F1: 0.908 | Train Acc: 0.931| Test F1: 0.907 | Test Acc: 0.935 | Test Loss: 0.299\n",
      "Epoch 079: | Train Loss: 0.29943 | Train F1: 0.909 | Train Acc: 0.931| Test F1: 0.908 | Test Acc: 0.934 | Test Loss: 0.298\n",
      "Epoch 080: | Train Loss: 0.29935 | Train F1: 0.908 | Train Acc: 0.931| Test F1: 0.909 | Test Acc: 0.935 | Test Loss: 0.298\n",
      "Epoch 081: | Train Loss: 0.29936 | Train F1: 0.908 | Train Acc: 0.931| Test F1: 0.908 | Test Acc: 0.935 | Test Loss: 0.297\n",
      "Epoch 082: | Train Loss: 0.29932 | Train F1: 0.907 | Train Acc: 0.931| Test F1: 0.911 | Test Acc: 0.935 | Test Loss: 0.297\n",
      "Epoch 083: | Train Loss: 0.29948 | Train F1: 0.908 | Train Acc: 0.931| Test F1: 0.911 | Test Acc: 0.934 | Test Loss: 0.298\n",
      "Epoch 084: | Train Loss: 0.29892 | Train F1: 0.908 | Train Acc: 0.931| Test F1: 0.910 | Test Acc: 0.936 | Test Loss: 0.296\n",
      "Epoch 085: | Train Loss: 0.29911 | Train F1: 0.909 | Train Acc: 0.932| Test F1: 0.908 | Test Acc: 0.935 | Test Loss: 0.296\n",
      "Epoch 086: | Train Loss: 0.29929 | Train F1: 0.908 | Train Acc: 0.930| Test F1: 0.908 | Test Acc: 0.934 | Test Loss: 0.298\n",
      "Epoch 087: | Train Loss: 0.29979 | Train F1: 0.907 | Train Acc: 0.932| Test F1: 0.910 | Test Acc: 0.934 | Test Loss: 0.298\n",
      "Epoch 088: | Train Loss: 0.29841 | Train F1: 0.910 | Train Acc: 0.931| Test F1: 0.910 | Test Acc: 0.935 | Test Loss: 0.295\n",
      "Epoch 089: | Train Loss: 0.29814 | Train F1: 0.909 | Train Acc: 0.931| Test F1: 0.910 | Test Acc: 0.936 | Test Loss: 0.295\n",
      "Epoch 090: | Train Loss: 0.29827 | Train F1: 0.908 | Train Acc: 0.931| Test F1: 0.911 | Test Acc: 0.936 | Test Loss: 0.294\n",
      "Epoch 091: | Train Loss: 0.29759 | Train F1: 0.909 | Train Acc: 0.932| Test F1: 0.909 | Test Acc: 0.936 | Test Loss: 0.295\n",
      "Epoch 092: | Train Loss: 0.29831 | Train F1: 0.909 | Train Acc: 0.932| Test F1: 0.909 | Test Acc: 0.935 | Test Loss: 0.295\n",
      "Epoch 093: | Train Loss: 0.29869 | Train F1: 0.908 | Train Acc: 0.932| Test F1: 0.911 | Test Acc: 0.934 | Test Loss: 0.298\n",
      "Epoch 094: | Train Loss: 0.29850 | Train F1: 0.909 | Train Acc: 0.932| Test F1: 0.910 | Test Acc: 0.935 | Test Loss: 0.295\n",
      "Epoch 095: | Train Loss: 0.29773 | Train F1: 0.909 | Train Acc: 0.931| Test F1: 0.910 | Test Acc: 0.934 | Test Loss: 0.297\n",
      "Epoch 096: | Train Loss: 0.29852 | Train F1: 0.908 | Train Acc: 0.931| Test F1: 0.911 | Test Acc: 0.935 | Test Loss: 0.296\n",
      "Epoch 097: | Train Loss: 0.29736 | Train F1: 0.909 | Train Acc: 0.932| Test F1: 0.909 | Test Acc: 0.935 | Test Loss: 0.295\n",
      "Epoch 098: | Train Loss: 0.29789 | Train F1: 0.909 | Train Acc: 0.931| Test F1: 0.908 | Test Acc: 0.934 | Test Loss: 0.298\n",
      "Epoch 099: | Train Loss: 0.29808 | Train F1: 0.908 | Train Acc: 0.931| Test F1: 0.910 | Test Acc: 0.934 | Test Loss: 0.297\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (14265x10 and 117x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-7714abb4e718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {e+0:03}: | Train Loss: {epoch_loss/len(train_loader):.5f} | Train F1: {epoch_f1/len(train_loader):.3f} | Train Acc: {epoch_acc/len(train_loader):.3f}| Test F1: {test_f1:.3f} | Test Acc: {test_acc:.3f} | Test Loss: {test_loss:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mcluster_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_assign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-3a32c594e1b3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, latent, classifier_idx)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (14265x10 and 117x128)"
     ]
    }
   ],
   "source": [
    "args.latent_dim = args.input_dim\n",
    "# print(X_train.shape)\n",
    "m = NNClassifier(args)\n",
    "args.latent_dim = 20\n",
    "print(m)\n",
    "EPOCHS = 100\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "\n",
    "for e in range(1, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    alpha = (1-e/EPOCHS)\n",
    "    acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#         latents = model.autoencoder(torch.FloatTensor(np.array(X_batch)).to(args.device), latent=True)\n",
    "#         cluster_id = model.clustering.update_assign(latents.cpu().detach().numpy())\n",
    "        y_pred, train_loss = m.fit(X_batch, y_batch)\n",
    "        epoch_loss += train_loss\n",
    "\n",
    "        f1 = f1_score(np.argmax(y_pred, axis=1), y_batch.detach().numpy())\n",
    "        acc = roc_auc_score(y_batch.detach().numpy(), y_pred[:,1])\n",
    "        epoch_f1 += f1.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    test_loss = 0.0\n",
    "\n",
    "    m.classifier.eval()\n",
    "    test_pred = m(torch.FloatTensor(np.array(X_test)).to(args.device))\n",
    "    test_loss += nn.CrossEntropyLoss(reduction='mean')(test_pred, torch.tensor(y_test).to(device))\n",
    "\n",
    "    test_f1 = f1_score(np.argmax(test_pred.detach().numpy(), axis=1), y_test)\n",
    "    test_acc = roc_auc_score(y_test, test_pred[:,1].detach().numpy())\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {epoch_loss/len(train_loader):.5f} | Train F1: {epoch_f1/len(train_loader):.3f} | Train Acc: {epoch_acc/len(train_loader):.3f}| Test F1: {test_f1:.3f} | Test Acc: {test_acc:.3f} | Test Loss: {test_loss:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No embedding Cluster-then-predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 1.33171 | Train F1: 0.539 | Train Acc: 0.489| Test F1: 0.712 | Test Acc: 0.660 | Test Loss: 1.308\n",
      "Epoch 002: | Train Loss: 1.30581 | Train F1: 0.625 | Train Acc: 0.492| Test F1: 0.641 | Test Acc: 0.672 | Test Loss: 1.296\n",
      "Epoch 003: | Train Loss: 1.29019 | Train F1: 0.631 | Train Acc: 0.493| Test F1: 0.666 | Test Acc: 0.675 | Test Loss: 1.296\n",
      "Epoch 004: | Train Loss: 1.28375 | Train F1: 0.639 | Train Acc: 0.492| Test F1: 0.657 | Test Acc: 0.678 | Test Loss: 1.290\n",
      "Epoch 005: | Train Loss: 1.27533 | Train F1: 0.645 | Train Acc: 0.492| Test F1: 0.662 | Test Acc: 0.677 | Test Loss: 1.320\n",
      "Epoch 006: | Train Loss: 1.27607 | Train F1: 0.645 | Train Acc: 0.492| Test F1: 0.663 | Test Acc: 0.681 | Test Loss: 1.301\n",
      "Epoch 007: | Train Loss: 1.27040 | Train F1: 0.648 | Train Acc: 0.493| Test F1: 0.667 | Test Acc: 0.680 | Test Loss: 1.307\n",
      "Epoch 008: | Train Loss: 1.26900 | Train F1: 0.649 | Train Acc: 0.493| Test F1: 0.668 | Test Acc: 0.683 | Test Loss: 1.298\n",
      "Epoch 009: | Train Loss: 1.26793 | Train F1: 0.649 | Train Acc: 0.493| Test F1: 0.668 | Test Acc: 0.682 | Test Loss: 1.301\n",
      "Epoch 010: | Train Loss: 1.26802 | Train F1: 0.648 | Train Acc: 0.493| Test F1: 0.669 | Test Acc: 0.684 | Test Loss: 1.297\n",
      "Epoch 011: | Train Loss: 1.26558 | Train F1: 0.649 | Train Acc: 0.493| Test F1: 0.670 | Test Acc: 0.687 | Test Loss: 1.297\n",
      "Epoch 012: | Train Loss: 1.26218 | Train F1: 0.649 | Train Acc: 0.493| Test F1: 0.679 | Test Acc: 0.693 | Test Loss: 1.301\n",
      "Epoch 013: | Train Loss: 1.26021 | Train F1: 0.651 | Train Acc: 0.494| Test F1: 0.670 | Test Acc: 0.683 | Test Loss: 1.309\n",
      "Epoch 014: | Train Loss: 1.26138 | Train F1: 0.654 | Train Acc: 0.492| Test F1: 0.672 | Test Acc: 0.686 | Test Loss: 1.317\n",
      "Epoch 015: | Train Loss: 1.25832 | Train F1: 0.653 | Train Acc: 0.494| Test F1: 0.670 | Test Acc: 0.684 | Test Loss: 1.314\n",
      "Epoch 016: | Train Loss: 1.25870 | Train F1: 0.650 | Train Acc: 0.494| Test F1: 0.668 | Test Acc: 0.683 | Test Loss: 1.313\n",
      "Epoch 017: | Train Loss: 1.25911 | Train F1: 0.646 | Train Acc: 0.493| Test F1: 0.660 | Test Acc: 0.682 | Test Loss: 1.333\n",
      "Epoch 018: | Train Loss: 1.25776 | Train F1: 0.650 | Train Acc: 0.493| Test F1: 0.670 | Test Acc: 0.684 | Test Loss: 1.324\n",
      "Epoch 019: | Train Loss: 1.25795 | Train F1: 0.651 | Train Acc: 0.494| Test F1: 0.668 | Test Acc: 0.685 | Test Loss: 1.320\n",
      "Epoch 020: | Train Loss: 1.25426 | Train F1: 0.647 | Train Acc: 0.494| Test F1: 0.670 | Test Acc: 0.684 | Test Loss: 1.326\n",
      "Epoch 021: | Train Loss: 1.25360 | Train F1: 0.649 | Train Acc: 0.494| Test F1: 0.666 | Test Acc: 0.682 | Test Loss: 1.340\n",
      "Epoch 022: | Train Loss: 1.25405 | Train F1: 0.649 | Train Acc: 0.494| Test F1: 0.666 | Test Acc: 0.680 | Test Loss: 1.339\n",
      "Epoch 023: | Train Loss: 1.25636 | Train F1: 0.647 | Train Acc: 0.494| Test F1: 0.664 | Test Acc: 0.683 | Test Loss: 1.323\n",
      "Epoch 024: | Train Loss: 1.25289 | Train F1: 0.650 | Train Acc: 0.494| Test F1: 0.665 | Test Acc: 0.683 | Test Loss: 1.343\n",
      "Epoch 025: | Train Loss: 1.25174 | Train F1: 0.651 | Train Acc: 0.495| Test F1: 0.667 | Test Acc: 0.683 | Test Loss: 1.347\n",
      "Epoch 026: | Train Loss: 1.25261 | Train F1: 0.651 | Train Acc: 0.494| Test F1: 0.666 | Test Acc: 0.685 | Test Loss: 1.326\n",
      "Epoch 027: | Train Loss: 1.25232 | Train F1: 0.649 | Train Acc: 0.494| Test F1: 0.650 | Test Acc: 0.683 | Test Loss: 1.331\n",
      "Epoch 028: | Train Loss: 1.25313 | Train F1: 0.651 | Train Acc: 0.493| Test F1: 0.666 | Test Acc: 0.683 | Test Loss: 1.347\n",
      "Epoch 029: | Train Loss: 1.25217 | Train F1: 0.649 | Train Acc: 0.494| Test F1: 0.656 | Test Acc: 0.681 | Test Loss: 1.354\n",
      "Epoch 030: | Train Loss: 1.25331 | Train F1: 0.650 | Train Acc: 0.494| Test F1: 0.662 | Test Acc: 0.682 | Test Loss: 1.366\n",
      "Epoch 031: | Train Loss: 1.24972 | Train F1: 0.652 | Train Acc: 0.494| Test F1: 0.673 | Test Acc: 0.683 | Test Loss: 1.366\n",
      "Epoch 032: | Train Loss: 1.25344 | Train F1: 0.648 | Train Acc: 0.495| Test F1: 0.673 | Test Acc: 0.682 | Test Loss: 1.359\n",
      "Epoch 033: | Train Loss: 1.25237 | Train F1: 0.651 | Train Acc: 0.495| Test F1: 0.665 | Test Acc: 0.682 | Test Loss: 1.352\n",
      "Epoch 034: | Train Loss: 1.24933 | Train F1: 0.652 | Train Acc: 0.494| Test F1: 0.668 | Test Acc: 0.683 | Test Loss: 1.362\n",
      "Epoch 035: | Train Loss: 1.24992 | Train F1: 0.651 | Train Acc: 0.495| Test F1: 0.672 | Test Acc: 0.683 | Test Loss: 1.350\n",
      "Epoch 036: | Train Loss: 1.24858 | Train F1: 0.655 | Train Acc: 0.494| Test F1: 0.666 | Test Acc: 0.683 | Test Loss: 1.368\n",
      "Epoch 037: | Train Loss: 1.24867 | Train F1: 0.653 | Train Acc: 0.494| Test F1: 0.666 | Test Acc: 0.683 | Test Loss: 1.364\n",
      "Epoch 038: | Train Loss: 1.24854 | Train F1: 0.653 | Train Acc: 0.495| Test F1: 0.662 | Test Acc: 0.682 | Test Loss: 1.361\n",
      "Epoch 039: | Train Loss: 1.24689 | Train F1: 0.655 | Train Acc: 0.495| Test F1: 0.669 | Test Acc: 0.683 | Test Loss: 1.369\n",
      "Epoch 040: | Train Loss: 1.24686 | Train F1: 0.653 | Train Acc: 0.495| Test F1: 0.663 | Test Acc: 0.681 | Test Loss: 1.379\n",
      "Epoch 041: | Train Loss: 1.24753 | Train F1: 0.652 | Train Acc: 0.495| Test F1: 0.656 | Test Acc: 0.681 | Test Loss: 1.384\n",
      "Epoch 042: | Train Loss: 1.24688 | Train F1: 0.651 | Train Acc: 0.495| Test F1: 0.661 | Test Acc: 0.683 | Test Loss: 1.367\n",
      "Epoch 043: | Train Loss: 1.24489 | Train F1: 0.653 | Train Acc: 0.495| Test F1: 0.656 | Test Acc: 0.680 | Test Loss: 1.402\n",
      "Epoch 044: | Train Loss: 1.24761 | Train F1: 0.650 | Train Acc: 0.495| Test F1: 0.658 | Test Acc: 0.681 | Test Loss: 1.382\n",
      "Epoch 045: | Train Loss: 1.24576 | Train F1: 0.649 | Train Acc: 0.495| Test F1: 0.656 | Test Acc: 0.681 | Test Loss: 1.391\n",
      "Epoch 046: | Train Loss: 1.24555 | Train F1: 0.651 | Train Acc: 0.495| Test F1: 0.664 | Test Acc: 0.682 | Test Loss: 1.383\n",
      "Epoch 047: | Train Loss: 1.24507 | Train F1: 0.650 | Train Acc: 0.494| Test F1: 0.665 | Test Acc: 0.684 | Test Loss: 1.361\n",
      "Epoch 048: | Train Loss: 1.24379 | Train F1: 0.651 | Train Acc: 0.495| Test F1: 0.664 | Test Acc: 0.683 | Test Loss: 1.363\n",
      "Epoch 049: | Train Loss: 1.24415 | Train F1: 0.651 | Train Acc: 0.495| Test F1: 0.658 | Test Acc: 0.682 | Test Loss: 1.375\n",
      "Epoch 050: | Train Loss: 1.24356 | Train F1: 0.650 | Train Acc: 0.495| Test F1: 0.661 | Test Acc: 0.683 | Test Loss: 1.376\n",
      "Epoch 051: | Train Loss: 1.24205 | Train F1: 0.653 | Train Acc: 0.495| Test F1: 0.660 | Test Acc: 0.682 | Test Loss: 1.375\n",
      "Epoch 052: | Train Loss: 1.24215 | Train F1: 0.649 | Train Acc: 0.496| Test F1: 0.658 | Test Acc: 0.682 | Test Loss: 1.384\n",
      "Epoch 053: | Train Loss: 1.24196 | Train F1: 0.652 | Train Acc: 0.495| Test F1: 0.655 | Test Acc: 0.681 | Test Loss: 1.393\n",
      "Epoch 054: | Train Loss: 1.24297 | Train F1: 0.651 | Train Acc: 0.495| Test F1: 0.656 | Test Acc: 0.681 | Test Loss: 1.368\n",
      "Epoch 055: | Train Loss: 1.24272 | Train F1: 0.648 | Train Acc: 0.495| Test F1: 0.665 | Test Acc: 0.683 | Test Loss: 1.362\n",
      "Epoch 056: | Train Loss: 1.24186 | Train F1: 0.648 | Train Acc: 0.495| Test F1: 0.666 | Test Acc: 0.684 | Test Loss: 1.348\n",
      "Epoch 057: | Train Loss: 1.24153 | Train F1: 0.650 | Train Acc: 0.495| Test F1: 0.664 | Test Acc: 0.685 | Test Loss: 1.360\n",
      "Epoch 058: | Train Loss: 1.24038 | Train F1: 0.649 | Train Acc: 0.495| Test F1: 0.653 | Test Acc: 0.681 | Test Loss: 1.392\n",
      "Epoch 059: | Train Loss: 1.24113 | Train F1: 0.650 | Train Acc: 0.495| Test F1: 0.650 | Test Acc: 0.681 | Test Loss: 1.395\n",
      "Epoch 060: | Train Loss: 1.24090 | Train F1: 0.651 | Train Acc: 0.495| Test F1: 0.661 | Test Acc: 0.682 | Test Loss: 1.379\n",
      "Epoch 061: | Train Loss: 1.24054 | Train F1: 0.649 | Train Acc: 0.494| Test F1: 0.668 | Test Acc: 0.683 | Test Loss: 1.368\n",
      "Epoch 062: | Train Loss: 1.23849 | Train F1: 0.652 | Train Acc: 0.494| Test F1: 0.658 | Test Acc: 0.681 | Test Loss: 1.400\n",
      "Epoch 063: | Train Loss: 1.23875 | Train F1: 0.650 | Train Acc: 0.495| Test F1: 0.654 | Test Acc: 0.680 | Test Loss: 1.407\n",
      "Epoch 064: | Train Loss: 1.24047 | Train F1: 0.647 | Train Acc: 0.495| Test F1: 0.653 | Test Acc: 0.681 | Test Loss: 1.408\n",
      "Epoch 065: | Train Loss: 1.23882 | Train F1: 0.653 | Train Acc: 0.495| Test F1: 0.634 | Test Acc: 0.680 | Test Loss: 1.423\n",
      "Epoch 066: | Train Loss: 1.23856 | Train F1: 0.651 | Train Acc: 0.495| Test F1: 0.634 | Test Acc: 0.679 | Test Loss: 1.423\n",
      "Epoch 067: | Train Loss: 1.23834 | Train F1: 0.650 | Train Acc: 0.495| Test F1: 0.631 | Test Acc: 0.679 | Test Loss: 1.431\n",
      "Epoch 068: | Train Loss: 1.24131 | Train F1: 0.648 | Train Acc: 0.496| Test F1: 0.624 | Test Acc: 0.676 | Test Loss: 1.439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 069: | Train Loss: 1.23887 | Train F1: 0.654 | Train Acc: 0.494| Test F1: 0.624 | Test Acc: 0.677 | Test Loss: 1.445\n",
      "Epoch 070: | Train Loss: 1.23760 | Train F1: 0.653 | Train Acc: 0.495| Test F1: 0.632 | Test Acc: 0.678 | Test Loss: 1.411\n",
      "Epoch 071: | Train Loss: 1.23805 | Train F1: 0.651 | Train Acc: 0.495| Test F1: 0.653 | Test Acc: 0.680 | Test Loss: 1.381\n",
      "Epoch 072: | Train Loss: 1.24288 | Train F1: 0.654 | Train Acc: 0.496| Test F1: 0.638 | Test Acc: 0.678 | Test Loss: 1.501\n",
      "Epoch 073: | Train Loss: 1.24840 | Train F1: 0.655 | Train Acc: 0.495| Test F1: 0.614 | Test Acc: 0.676 | Test Loss: 1.439\n",
      "Epoch 074: | Train Loss: 1.24254 | Train F1: 0.651 | Train Acc: 0.495| Test F1: 0.643 | Test Acc: 0.677 | Test Loss: 1.404\n",
      "Epoch 075: | Train Loss: 1.23991 | Train F1: 0.652 | Train Acc: 0.496| Test F1: 0.637 | Test Acc: 0.676 | Test Loss: 1.432\n",
      "Epoch 076: | Train Loss: 1.23929 | Train F1: 0.655 | Train Acc: 0.496| Test F1: 0.623 | Test Acc: 0.674 | Test Loss: 1.480\n",
      "Epoch 077: | Train Loss: 1.23846 | Train F1: 0.657 | Train Acc: 0.495| Test F1: 0.625 | Test Acc: 0.674 | Test Loss: 1.463\n",
      "Epoch 078: | Train Loss: 1.24105 | Train F1: 0.649 | Train Acc: 0.495| Test F1: 0.632 | Test Acc: 0.675 | Test Loss: 1.474\n",
      "Epoch 079: | Train Loss: 1.23960 | Train F1: 0.653 | Train Acc: 0.495| Test F1: 0.630 | Test Acc: 0.675 | Test Loss: 1.487\n",
      "Epoch 080: | Train Loss: 1.23756 | Train F1: 0.650 | Train Acc: 0.495| Test F1: 0.627 | Test Acc: 0.674 | Test Loss: 1.478\n",
      "Epoch 081: | Train Loss: 1.23574 | Train F1: 0.655 | Train Acc: 0.494| Test F1: 0.628 | Test Acc: 0.674 | Test Loss: 1.478\n",
      "Epoch 082: | Train Loss: 1.23781 | Train F1: 0.650 | Train Acc: 0.496| Test F1: 0.631 | Test Acc: 0.675 | Test Loss: 1.445\n",
      "Epoch 083: | Train Loss: 1.23702 | Train F1: 0.652 | Train Acc: 0.495| Test F1: 0.634 | Test Acc: 0.677 | Test Loss: 1.437\n",
      "Epoch 084: | Train Loss: 1.23677 | Train F1: 0.645 | Train Acc: 0.495| Test F1: 0.631 | Test Acc: 0.675 | Test Loss: 1.434\n",
      "Epoch 085: | Train Loss: 1.23409 | Train F1: 0.651 | Train Acc: 0.495| Test F1: 0.632 | Test Acc: 0.675 | Test Loss: 1.437\n",
      "Epoch 086: | Train Loss: 1.23677 | Train F1: 0.652 | Train Acc: 0.495| Test F1: 0.635 | Test Acc: 0.675 | Test Loss: 1.415\n",
      "Epoch 087: | Train Loss: 1.23666 | Train F1: 0.655 | Train Acc: 0.495| Test F1: 0.636 | Test Acc: 0.673 | Test Loss: 1.456\n",
      "Epoch 088: | Train Loss: 1.23583 | Train F1: 0.652 | Train Acc: 0.495| Test F1: 0.649 | Test Acc: 0.675 | Test Loss: 1.410\n",
      "Epoch 089: | Train Loss: 1.23649 | Train F1: 0.655 | Train Acc: 0.495| Test F1: 0.638 | Test Acc: 0.675 | Test Loss: 1.407\n",
      "Epoch 090: | Train Loss: 1.23501 | Train F1: 0.651 | Train Acc: 0.495| Test F1: 0.659 | Test Acc: 0.678 | Test Loss: 1.401\n",
      "Epoch 091: | Train Loss: 1.23542 | Train F1: 0.654 | Train Acc: 0.496| Test F1: 0.642 | Test Acc: 0.674 | Test Loss: 1.429\n",
      "Epoch 092: | Train Loss: 1.23928 | Train F1: 0.653 | Train Acc: 0.495| Test F1: 0.679 | Test Acc: 0.683 | Test Loss: 1.369\n",
      "Epoch 093: | Train Loss: 1.23990 | Train F1: 0.648 | Train Acc: 0.497| Test F1: 0.683 | Test Acc: 0.683 | Test Loss: 1.380\n",
      "Epoch 094: | Train Loss: 1.23802 | Train F1: 0.651 | Train Acc: 0.496| Test F1: 0.679 | Test Acc: 0.683 | Test Loss: 1.375\n",
      "Epoch 095: | Train Loss: 1.23791 | Train F1: 0.652 | Train Acc: 0.496| Test F1: 0.679 | Test Acc: 0.683 | Test Loss: 1.374\n",
      "Epoch 096: | Train Loss: 1.23785 | Train F1: 0.651 | Train Acc: 0.496| Test F1: 0.680 | Test Acc: 0.683 | Test Loss: 1.368\n",
      "Epoch 097: | Train Loss: 1.23978 | Train F1: 0.648 | Train Acc: 0.496| Test F1: 0.676 | Test Acc: 0.681 | Test Loss: 1.371\n",
      "Epoch 098: | Train Loss: 1.24024 | Train F1: 0.648 | Train Acc: 0.496| Test F1: 0.677 | Test Acc: 0.681 | Test Loss: 1.373\n",
      "Epoch 099: | Train Loss: 1.23841 | Train F1: 0.647 | Train Acc: 0.495| Test F1: 0.676 | Test Acc: 0.682 | Test Loss: 1.375\n"
     ]
    }
   ],
   "source": [
    "# Training separate models\n",
    "args.lr = 0.02\n",
    "args.latent_dim = args.input_dim\n",
    "classifiers = [NNClassifier(args) for _ in range(args.n_clusters)]\n",
    "args.latent_dim = 20\n",
    "# optimizers = [torch.optim.Adam(classifiers[i].classifier.parameters(), lr=args.lr) for i in range(args.n_clusters)]\n",
    "EPOCHS = 100\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "\n",
    "\n",
    "latents_X = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id_train = model.clustering.update_assign(latents_X.cpu().detach().numpy())\n",
    "\n",
    "X_latents_data_loader = list(zip(torch.FloatTensor(np.array(X_train)).to(args.device), cluster_id_train, y_train))\n",
    "train_loader_latents = torch.utils.data.DataLoader(X_latents_data_loader,\n",
    "    batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "latents_test = model.autoencoder(torch.FloatTensor(np.array(X_test)).to(args.device), latent=True)\n",
    "cluster_id_test = model.clustering.update_assign(latents_test.cpu().detach().numpy())\n",
    "# plot(latents_X, y_train, latents_test, y_test)\n",
    "\n",
    "for e in range(1, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    alpha = (1-e/EPOCHS)\n",
    "    acc = 0\n",
    "#     plot(latents_X, y_train, latents_test, y_test)\n",
    "\n",
    "    for X_batch, cluster_batch, y_batch in train_loader_latents:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        new_y_batch = []\n",
    "        y_pred = []\n",
    "        for k in range(args.n_clusters):\n",
    "            idx = np.where(cluster_batch == k)[0]\n",
    "            y_pred_idx, loss = classifiers[k].fit(X_batch[idx], y_batch[idx])\n",
    "#             print(\"F1 Cluster: \", k, f1_score(np.argmax(y_pred_idx, axis=1), y_batch[idx]))\n",
    "#             print(\"Cluster: \", k, len(idx))\n",
    "            new_y_batch.append(y_batch[idx])\n",
    "            y_pred.append(y_pred_idx)\n",
    "            epoch_loss += loss\n",
    "\n",
    "        y_pred = np.vstack(y_pred)\n",
    "        new_y_batch = np.hstack(new_y_batch)\n",
    "#         print(y_pred.shape, new_y_batch, len(X_batch))\n",
    "        f1 = f1_score(np.argmax(y_pred, axis=1), new_y_batch)\n",
    "        acc = roc_auc_score(y_batch, y_pred[:,1])\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_f1 += f1.item()\n",
    "\n",
    "    test_preds = []\n",
    "    test_loss = 0.0\n",
    "    new_y_test = []\n",
    "    \n",
    "    # Epoch Testing\n",
    "    for k in range(args.n_clusters):\n",
    "        classifiers[k].classifier.eval()\n",
    "        idx = np.where(cluster_id_test == k)[0]\n",
    "        latents_idx = latents_test[idx]\n",
    "        y_pred_idx = classifiers[k](torch.FloatTensor(np.array(X_test)).to(args.device)[idx])\n",
    "        test_loss += nn.CrossEntropyLoss(reduction='mean')(y_pred_idx, torch.tensor(y_test[idx]).to(device))\n",
    "        test_preds.append(y_pred_idx.detach().numpy())\n",
    "        new_y_test.append(y_test[idx])\n",
    "\n",
    "    test_preds = np.vstack(test_preds)\n",
    "    new_y_test = np.hstack(new_y_test).reshape(-1)\n",
    "    test_f1 = f1_score(np.argmax(test_preds, axis=1), new_y_test)\n",
    "    test_acc = roc_auc_score(new_y_test, test_preds[:,1])\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {epoch_loss/len(train_loader):.5f} | Train F1: {epoch_f1/len(train_loader):.3f} | Train Acc: {epoch_acc/len(train_loader):.3f}| Test F1: {test_f1:.3f} | Test Acc: {test_acc:.3f} | Test Loss: {test_loss:.3f}')\n",
    "\n",
    "out = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id = model.clustering.update_assign(out.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMNN over CAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMNN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(DMNN, self).__init__()\n",
    "        self.args = args\n",
    "        self.input_dim = args.input_dim\n",
    "        self.output_dim = self.input_dim\n",
    "        self.n_classes = args.n_classes\n",
    "        self.n_clusters = args.n_clusters\n",
    "        self.latent_dim = args.latent_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(args.input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, self.latent_dim)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, args.input_dim)\n",
    "        )\n",
    "        self.classifiers = []\n",
    "        for _ in range(self.n_clusters):\n",
    "            self.classifiers.append(nn.Sequential(\n",
    "                nn.Linear(self.latent_dim, 8),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(8, 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4, args.n_classes)\n",
    "            ))\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, self.n_clusters),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, output=\"latent\"):\n",
    "        x = self.encoder(inputs)\n",
    "        if output == \"latent\":\n",
    "            return x\n",
    "        else:\n",
    "            outs = []\n",
    "            for i in range(self.n_clusters):\n",
    "                outs.append(self.classifiers[i](x))\n",
    "            return self.gate(x), self.decoder(x), outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "criterion_rec = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "dmnn = DMNN(args)\n",
    "dmnn.encoder = model.autoencoder.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 1.01595 | Acc: 0.143\n",
      "Epoch 002: | Loss: 1.00623 | Acc: 0.142\n",
      "Epoch 003: | Loss: 1.01526 | Acc: 0.142\n",
      "Epoch 004: | Loss: 1.01585 | Acc: 0.141\n",
      "Epoch 005: | Loss: 1.00990 | Acc: 0.143\n",
      "Epoch 006: | Loss: 1.00686 | Acc: 0.145\n",
      "Epoch 007: | Loss: 1.00647 | Acc: 0.141\n",
      "Epoch 008: | Loss: 1.01017 | Acc: 0.142\n",
      "Epoch 009: | Loss: 1.00969 | Acc: 0.143\n"
     ]
    }
   ],
   "source": [
    "dmnn.train()\n",
    "device = \"cpu\"\n",
    "EPOCHS = 10\n",
    "for e in range(1, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        X_inp = (X_batch + np.random.normal(0, 0.01, size=X_batch.shape)).clone().detach().float()\n",
    "        gate, X_rec, y_pred_local = dmnn(X_inp, output=\"decoded\")\n",
    "#         print(y_pred)\n",
    "#         print(gate)\n",
    "        y_pred = torch.zeros((len(X_inp), args.n_classes))\n",
    "        for j in range(args.n_clusters):\n",
    "            y_pred += gate[:,j].reshape(-1,1)*y_pred_local[j]\n",
    "        \n",
    "        acc = f1_score(np.argmax(y_pred.detach().numpy(), axis=1), y_batch.unsqueeze(1))\n",
    "\n",
    "        loss_rec = criterion_rec(X_batch, X_rec)\n",
    "        loss_rec.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss_rec.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 1.70800 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 002: | Loss: 1.70806 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 003: | Loss: 1.70827 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 004: | Loss: 1.70864 | Acc: 0.495 | Test Acc: 0.500\n",
      "Epoch 005: | Loss: 1.70823 | Acc: 0.495 | Test Acc: 0.500\n",
      "Epoch 006: | Loss: 1.70844 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 007: | Loss: 1.70799 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 008: | Loss: 1.70808 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 009: | Loss: 1.70822 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 010: | Loss: 1.70845 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 011: | Loss: 1.70791 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 012: | Loss: 1.70862 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 013: | Loss: 1.70786 | Acc: 0.492 | Test Acc: 0.500\n",
      "Epoch 014: | Loss: 1.70811 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 015: | Loss: 1.70786 | Acc: 0.495 | Test Acc: 0.500\n",
      "Epoch 016: | Loss: 1.70861 | Acc: 0.492 | Test Acc: 0.500\n",
      "Epoch 017: | Loss: 1.70820 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 018: | Loss: 1.70843 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 019: | Loss: 1.70846 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 020: | Loss: 1.70835 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 021: | Loss: 1.70829 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 022: | Loss: 1.70829 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 023: | Loss: 1.70802 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 024: | Loss: 1.70858 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 025: | Loss: 1.70795 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 026: | Loss: 1.70842 | Acc: 0.492 | Test Acc: 0.500\n",
      "Epoch 027: | Loss: 1.70863 | Acc: 0.492 | Test Acc: 0.500\n",
      "Epoch 028: | Loss: 1.70776 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 029: | Loss: 1.70817 | Acc: 0.492 | Test Acc: 0.500\n",
      "Epoch 030: | Loss: 1.70836 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 031: | Loss: 1.70758 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 032: | Loss: 1.70775 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 033: | Loss: 1.70823 | Acc: 0.496 | Test Acc: 0.500\n",
      "Epoch 034: | Loss: 1.70816 | Acc: 0.492 | Test Acc: 0.500\n",
      "Epoch 035: | Loss: 1.70830 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 036: | Loss: 1.70795 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 037: | Loss: 1.70813 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 038: | Loss: 1.70782 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 039: | Loss: 1.70839 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 040: | Loss: 1.70794 | Acc: 0.495 | Test Acc: 0.500\n",
      "Epoch 041: | Loss: 1.70854 | Acc: 0.492 | Test Acc: 0.500\n",
      "Epoch 042: | Loss: 1.70825 | Acc: 0.492 | Test Acc: 0.500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-6884c5ddcd07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#         acc = f1_score(np.argmax(y_pred.detach().numpy(), axis=1), y_batch.unsqueeze(1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss_rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training separate models\n",
    "args.lr = 0.002\n",
    "EPOCHS = 300\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "\n",
    "####\n",
    "model.train()\n",
    "device = \"cpu\"\n",
    "EPOCHS = 100\n",
    "\n",
    "latents_X = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "X_latents_data_loader = list(zip(latents_X, cluster_id_train, y_train))\n",
    "train_loader_latents = torch.utils.data.DataLoader(X_latents_data_loader,\n",
    "    batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "latents_test = model.autoencoder(torch.FloatTensor(np.array(X_test)).to(args.device), latent=True)\n",
    "\n",
    "for param in dmnn.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for e in range(1, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    alpha = (1-e/EPOCHS)\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        gate, X_tilde, y_pred_local = dmnn(X_batch, output=\"decoded\")\n",
    "        X_latent = dmnn(X_batch, output=\"latent\")\n",
    "        loss = torch.tensor(0.).to(device)\n",
    "        loss_rec = torch.tensor(0.).to(device)\n",
    "        y_pred = torch.zeros((len(X_batch), args.n_classes))\n",
    "\n",
    "        for j in range(args.n_clusters):\n",
    "            loss += torch.sum(gate[j].reshape(-1,1)*criterion(y_pred_local[j], y_batch))\n",
    "            y_pred += gate[:,j].reshape(-1,1)*y_pred_local[j]\n",
    "\n",
    "#         loss = criterion(y_pred, y_batch)\n",
    "        gate.detach()\n",
    "        loss_rec = 0.4*criterion_rec(X_batch, X_tilde)\n",
    "#         acc = f1_score(np.argmax(y_pred.detach().numpy(), axis=1), y_batch.unsqueeze(1))\n",
    "        acc = roc_auc_score(y_batch.unsqueeze(1), y_pred[:,1].detach().numpy())\n",
    "        loss_rec.backward(retain_graph=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += (loss+loss_rec).item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    gate, X1, y_pred_local = dmnn(torch.tensor(X_test).to(device).float(), output=\"decoded\")\n",
    "    y_pred = torch.zeros((len(X1), args.n_classes))\n",
    "    for j in range(args.n_clusters):\n",
    "        y_pred += gate[:,j].reshape(-1,1)*y_pred_local[j]\n",
    "\n",
    "#     test_f1 = f1_score(np.argmax(y_pred.detach().numpy(), axis=1), y_test)\n",
    "    test_f1 = roc_auc_score(y_test, y_pred[:,1].detach().numpy())\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f} | Test Acc: {test_f1:.3f}')\n",
    "#### Synthetic Max AUC: 0.833: alpha=2.5, 0.848: alpha=0\n",
    "#### wid-- CAC_DMNN: F1=~0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "X_train = X_train.astype(float)\n",
    "gate, X1, preds = dmnn(torch.tensor(X_train).float().to(device), output=\"decoded\")\n",
    "x_emb_train = reducer.fit_transform(X1.detach().numpy())\n",
    "x_emb_test = reducer.transform(dmnn(torch.tensor(X_test).float().to(device)).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 1,  ..., 1, 0, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fcfc4a1b190>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAxOUlEQVR4nO3deZRU1b3o8e+uoScamu4GmmZuxmYSkBYEZVBA0eesxCkRE28wyyHJy2Bys7Luu/fdu3L1xmhiYuIjxqhRwSRExAEREEQJg4BMDTQ080wzNPRE17TfH9WNTY9V1efUOXXq91mLBV11zj6/orp/vWuf395baa0RQghhXy6rAxBCCNE6SdRCCGFzkqiFEMLmJFELIYTNSaIWQgib85jRaJcuXXS/fv3MaFoIIRxp48aNp7XWXZt7zpRE3a9fPzZs2GBG00II4UhKqYMtPSdDH0IIYXOSqIUQwuYkUQshhM1JohZCCJuTRN0Svx8++ww2bgRZD0UIYSFJ1A1t3AhTpoBSkJICkydDURGkpcG6dVZHJ4RIUqaU5yWUEyfghz+Et95q+RifD66+GtauhfHj4xebEKJV5eXlfPHFF5w8eZKTJ0/i9/spKCjg7rvvxuNxTnpTZixzWlRUpG1fR11TAz//OTz3XHTnVVRAZqY5MQkhIuLz+Xj11Vc5fvx4i8fMnDmT8QnUsVJKbdRaFzX3XHIOffzud5CREX2SBvje94yPRwgRlbfffrvVJA3w0UcfsXLlyvgEZLLkS9S/+hU8+WTs57/yinGxCCGiVlVVxf79+yM69tNPP+XNN98kEAiYHJW5kmvo47334Lbb2t/O8uVw/fXtb0cIERF/eTkrf/ELtgABt5va1NTwTf8oeL1evvGNb9C7d29zgmwnGfoA+M//NCZJ17clhIgLrTV/+Y//YF1KClUdOlCblhZTO36fj1deeYU9e/YYHKH5kiNR794N//ZvxrV38qRxbQkhWnVk6VJOZGQQ9Hq/elCp6Oc31J0z7/XXjQ0wDpIjUd9+u7HtdehgbHtCiBad2L2bgKuZVBXl0Ee9QXv2cPHixXZGFV/OKTRsyZ49sGuXsW3G+NFLiITl81Hz+9+zZu9eSnNzUT16cNPNN9OrVy/TL13VuTP69OmmT2gdU7LeP2AAGzZs4NprrzUguvhwdqKurAzPLDTa2bPGtymEDR08eJB1zz5LsKyMPYMGQXY2GuDYMf708ssol4vvf//7dOrUqc22gsEgSilczfWOW7HzwoXmE3IsPWql8Hu9rFu3ThK1bfz1r+EJKkaTMWrhcGfPnuWll16Cykp0VhaB3NxmE6PWml//+tf87Gc/a3EmYHFxMR988AE1NTUAFBQUcNddd5HZxsSxQCDAu+++y6mysthfSHO9bqWorKykrKyMrl2b3VDFdpxdnjd6NGzZYny7ffrAwRY3YxAioS1btozVq1fjqa0l6PWiI+gB33rrrRQWFlJVVUV2djaBQIAzZ85wdP9+lnz8MSGXK5wwQyFQirT0dH784x/jcrk4d+4cy5cvZ//+/aSlpZGfn8/hw4e5cOFCuPEYhzjqr9XSuW63m8cee4ycnJzo2zZBa+V5zu1R/+lP5iRpgM6dzWlXCIvt3buX1atX07G8nIqsLFyhEJF05T755BM+/PBD3G43gUCAUCgUfkJrcLu/OtDlAq25WFPD3LlzKSgoYO3atZeerq6u5mzjocUYbxrWX6slwWCQVatWcccdd8TWfhw5M1EfPgz/8i/mtb97t3ltC2ERrTVLliwBoCIrC5QK94QjUFVVBYST32VaGVuuX0jJVG0k+ZKSEnOvbxBnJup77jG3/QQr7RGiNVprVq9ezeeff07aiROQlRXujULsvdlww00fa097kV4zimskSpmeMxO12ePjffua274QcbRw4UK2bt1KRkUFFZmZXyXp9jI7KUPSbOrhzERt9ps3ZYq57QsRJ++//z7Vf/87D65dS0ZNDTuGDmX1pEnxSbJGake8lZWVbVagWM2ZidrtBjNXy3r33fAdZaN6HkJYoLy8nPTnn+eGzz4jxe8HoGtZGfsGDOB4jx6xJ79YqzQsus6mTZuYPHmyAQGZx3mZprzc3CQNcP48vPGGudcQwkSBQIC5zzxDv/37WT9+PJtHjcKXkoI3EOCWRYtwN74p6GCnm5v1aDMR9aiVUv8b+BdAA9uAb2qt7TkK//TT8bnOyy/DQw/F51pCGGzNmjUQDPL2/fcTcLvxBAIsmTmTO955h70DBpBz5gxleXmxNR5JL9eIT6QG9KY9Hk9cpsG3V5uJWinVE/guMExrXaOU+itwH/CqybHFprW9D41UN8tKiES0fv16atLTLyVLv9uNX2vm339/+ICGq9MZPYwRr6GRCKSmpjJq1Cirw2hTpL/SPEC6UsoDZADHzAupnepnMwkhWlRTXd2kR6vqE2h9EjUjQTe+hsVuvfVWUlNTrQ6jTW0maq31UeBZ4BBwHDivtf7Y7MBiVl0dn+ts2WL+WLgQJigrK2syMUW19L0cy7rPjWlNWnU1Xp/PNgm6XkFBgdUhRKTNRK2UygZuBwqAHkAHpdTXmzlujlJqg1JqQ1l7FlFpr7q713G5jvTeRQJ66623miTMwt27cdVP+27E3Y4OiQqF6HLqFIN27cKfkhJzO0ZTdb+oTJ8ZaZBIhj6mA/u11mVaaz/wD2Bi44O01nO11kVa6yLLVqTy+eJ7vUmTkqbgXjhDbW0t5eXluBt1aPJOnGB4cXGTxz0+Hz2PHIn5+1y7XJzOy2PbmDExx2w4rRlWXEzOmTO88sorvPnmm5ixOJ2RIknUh4CrlVIZSikFTAN2mhtWjJYti29t844d8MgjkqxFQrnms88YvXnzZd+3Z3Jzmf7xx/Q5fBiP309qTQ0ev58hJSWk19S0f8jCRuPSKMW+gQMp3BlOY6Wlpdhitc9WtFn1obVep5T6O7AJCABfAnPNDiwmZ8+Gy37i6c9/hu7d4Re/iO91hYiBWrWKaz7/nKU33HDZ4zuGDWPG0qU8+Je/UJ6Tw7nsbHLKylBK8bsnnmi90eaqOLQm++xZQm4352242qQvJeWymJcvX85VV11lYUStc8561JWV8LWvweLF8b1uvbIy6NLFmmsLEaGy6dP5OC+PvQMHNllnulN5Obe89x4D9u1DK0XpwIG8f+utVHbsGFVJXa9Dh7h7wQIyqqtRWnM2J4e3772Xc7m5Zryk6GlN3wMHOJ+VRXmDtajvu+8+hgwZYllYra1H7YxE7fPB2LHhvRGtqsRIS5PaamF7WyZN4v3Jkwm0cmNP1X0qvSyRR5ioO1RW8uQLL5Da4H5RCKjq0IHnf/ADdMO1qa2gNSk+H4N27aK4mfrp2bNn069fv/jHReuJ2hlTyBcsgAMHrC2Xu3gRPvrIuusLEYEtQ4e2mXC1Uk13dYmwTG/U5s1NqkdcgNfvZ/CePdGGa6y6+H0eT7NJGuC1117D0qq1FjgjUa9cGR76sNq3v211BEK0yOfzcTA/v/UKh9aeiyBZZ50/j7eZDpMrFKKjGfuXRqP+hmYbvfr58+fHKaDIOSNR9+0bHnqw2pEjVkcgRIvcbjd4PAS93tgrldrojR/s14/a5oZVlOJwAqypAeGNfWtra60O4zLOSNQPP9zmb8m4+ec/rY5AiGa53W6uuOKK1kvl6jeEjdGuwkLOZWfjb7Ajuc/rZe+AAZzMz4+53XhbuXKl1SFcxhmJukePcLVHr15g9bz9P/7R2usL0Yrp06fj8bRQlas1nkAAolnitH79jjoht5tXvvUtPrv2Wk7n5nIiL4+l06fzt1mz2hl5fG3fvt3qEC7jnI0DJk2CQ4dgz55wsnz2WWviOHXKmusKEYEdO3agmql5RmtwucLVIFqHk3Ukn1Kb6X37U1P5bOpUPps69av2E4zX67U6hMs4o0d97BjMmQMDB8I3v9nuj2/tsn+/NdcVIgL79+/HXzdN3O3zkXnhQvhnpeFmti5X5DN8I0nCdpmRGIUJEyZYHcJlEr9HXVICo0d/tTP4vn3hcWK3O7qPcEY5fDj+1xQiQllZWZf+feOSJSyZObP5A+uTa1v105Em4fqKkQRI2h6PhzF2WpsEJ/SoH3jgqyTdkFVbCSXRFkYi8dRPk06tqWFISUn8E2cCDINorcM74NhIYifqQ4dg0yaro7hcMJgQ34wiOWVnZ6OUIr2mhrTaWnocO3Zpyc96noYr6BmdyJtrz2Y/L8FgkPXr11sdxmUSJ1EvWgQ33BAe5igshBtvhCuusDqqpnw++OtfrY5CiGadPn0al8vF+awsgm43o778MjwLsf6GotZ0qJ881lICNboGu4Wbmx6/P/6LrNWptMMEugbsP0ZdWQk9ezZdpL+kxJp4IvH738O991odhRDNUkqh3W5+98QT1GRkNEmUDVe763z2LNOWL6f/vn1cTEtj2fXXs3PECLMDJO/YMW5csoQPb76Z07FusttOW7duDded24D9E3Xfvom3k8qBA1ZHIESzsrOzL00hr87MbP6gusSdWVHBnLlzSa2txaU1GTU1X5XvmTm2rTWdLlyg19GjlGdnm3edNixcuNA2idreQx8//GF4jen2aqnA3ywWfVwToi3FxcWEIvz+TK+u5s8PP8w/7ryTY3WzCs/m5pq/OYdSVHbsyIK77vrqF4MF7LTri3171E89Bc891/52vN7w0MncueEx7njo2DE+1xEiSitXrow4AZXl5YFSlOXlUTxyJMO3b6fH0aOcyc6ObsmGaHvgWnMmJ4fjPXqEv06Akj6z2a9HrTU89hj88pfta6dXL8jPD0+E+eILmDED3nrLmBhb43KFSwaFsJGqqiqWL19OeXl55CfVJ8i6tUF2DR2Khuh71NHuZK4UvvR0WyToEpvcC7PPxgEVFbBmDdx6a/s2qe3UCb78Evr3b/75RYvg9ttjb78tbjeUl0NL439CxNnu3buZN2+eIW25AgFC8R5KtJDH4+Gpp56Ky5Ry+28c8OKL0K1buOSuPUk6IwPOn285SQPcdJO5QxPBIFRXm9e+EFE4c+aMYUkawosu2U6jhaGMFAgE+PLLL01pOxrWJ+rVq+FHP2p+dmG0Ipmf7/WGy+da2Yqo3Y4eNa9tISKktWbuXAP3obbrFHCTdzhfsWKF5TcWrU/Uv/2tMUk6IwP+538iO/brXw/vCtOzZ/uv25zHHzenXSGicPLkSXzt+YTamB2TdBz4/X6OHz9uaQzWJ2ojVpvLzYW1a+HKKyM/Z8IEOHgQXnop3Ms20po1sGOHsW0KESWj9/7rcfgwNy9aZLsp32YLhUIErNyPFTsk6quvbt/5eXnhFetGjoz+XLcbHn00XKtt9Gyrn/zE2PaEiNKZM2cMa2vC6tXcP28eg3fvZnBJSVIla601PepLBS1i/e3b//gPeOGF2M695RZYuLD923AtXGj8OtI7dxrbnhBRKi0tNaSd9MpKLnTqxK9/8AOU1qRevIjSGp0kQyF5eXkt74oTJ9Yn6s6dw9tnxbKZ5DXXGLNX4h//CFVV7W+noZwcY9sTIkpGlZS5QyF2FRYSrEtWAa83qWbf1o/1p5hZgNAG64c+AN58M/pzUlPDwx5GMGP8qcEC7UJYYezYsW0fFMEQRlVmZnjn8kYaL4/q5OGQ9957j6CFa83bI1HfeWd4oko0QiG45x5jrv/QQ5Cebkxb9Wy2Q4RIPsOHDzeknWaHOFwuOlZU4Pb7Ta1jtovt27fz3//938ybN4+SkpK4l+vZI1G7XOGSuWgoZdzsv299C9LSjGmr3ujRxrYnRJSUUk03sm16UCQNNXnIFQySVlMT7mnX1zE7fMw6GAyye/duFixYwKJFi+J6bXskaghvShsNn8+4/Qlra8NT2I00f76x7QkRgzYTdSy0JuRycba1+zAO7mH7/X6Ki4s5duxY3K4ZUaJWSnVWSv1dKbVLKbVTKWX8Fr29e0d/TmqqMdc2clJAPYNrWIWIRV6M93FSW/vZqus9J0vVR3MCgQBffPEFZ41YhjkCkfaofwN8pLUuBEYBxtee5eXB4MHRndOtmzHXzsmBIUOMaaveqFHGtidEDKZOnRrTeSkpKaS3cd+muRuMgH2nmhtIa8327dv5wx/+wF/+8hdjZ4A2o81ErZTqBEwG/lQXoE9rXW5KNNEOFxj5zfDaa+Fp6EYx6EaOEO0Ra4/v4sWLXIx1aQeHJ+l6gUCAQCDAgQMH+OCDD0y9ViQ96v5AGfBnpdSXSqmXlVIdGh+klJqjlNqglNoQ89TVMWMiv6nXpUts12jJ2LFg0AQBILrp7EKYpGOMK0X6/f62KxuSJCG3JRQKsX37dlOnmUeSqD3AlcAftNZjgCrgp40P0lrP1VoXaa2LunbtGntEkSbqxx6L/Rotyc+HyZONaWviRGPaEaIdhg4danUISSEUCvHcc8+xevXqiLc6i0YkifoIcERrva7u678TTtzmaLADcqv69jXn+q+/bswiTdLbEDbgcrnaHGsWxqipqWHZsmUsWLDA8LbbTNRa6xPAYaVU/d22aYB5S8Pddltkx111lTnX79sXPvigfVPTk2gHDGF/MY81i5js2LHD8GqQSKs+ngTeVEptBUYDvzA0iob+/d8jO272bKisNCeGGTNg5szYzzeqGkUIA7jjtCtLr4MH+frrr/O955/na/Pm0eXUqbhc147Wr19vaHsRJWqt9ea68ecrtNZ3aK3PGRpFQ9nZ4fWl2/Lll8ZNIW/Ob38b+7l+v3FxCNFOzd4zMnpCitYopehz6BCdz59nSEkJI7duTarFmxo6cOCAoe3ZZ2ZiQ//8Z2THffyxOZNVILbV/OrFqQheiEgMHDiw6YNG30NRiuP5+aysq9t2AbXp6dHvWO4QRleA2PN/cfDgyG7oaW3MNl7NGTQo9nOT9JtT2NO61avjMqU74PWyucFiZH3373f0VPLWZBm8eqZ9M0pNTdvHpKdHv+pepNxuePrp2M6VRC1sxFVVFbcqpJr0dIJ1Y+I947gWht10Mjgv2TejuN3hsd4OTebWfOWZZ8yN4Sc/gT//Ofpv8gEDzIlHiGidOIE3muHB9vaAtWZHXe22OxDAZeEazlaqrq42tD37JmoIl7lduADf//7lvdT09PCuLE8+aX4MDz8c3ljg1VcjPyfWrcWEMNrhwxRt3Rq3IQjtdrNl1Ch8Xi9rJkwgFKeKE7upNLgiTZmxAHZRUZHesGGDsY0GAnD+fHhCjFVv/t//DrNmtX1cko7LCRu6cAHdvTv/96mn4jcJS2vcfj9BC7eustrIkSO56667ojpHKbVRa13U3HP27lE35PGEy/as/A19zz2weXO4hLAlxcVxC0eIttSmpvLaj38c34sqldRJGiLcBi0KiZOo7WLUqHD53fz5UFAQ/sWRkREehqmuhmHDrI5QiEs++OADDns8sqRBnBk9oiBznWN1773hP0LYVDAYpLi42JRFgkTrdu/ebWh70qMWwqGCwaC9knQS3bsx+v9dErUQDpWSkmLOnokQ/c7jSZSkAQYYXKIrQx9CCPMl2Rj5tGnTDG1PetRCOFhmZmZM56VcvEjO6dMtL6pUt8GtaN7evXsNbU8StRAOlhHDPqDj1q7lR88+S8+jRxNjOYRoh2HiYOvWrYa2lwDvghAiFhUVFZw5cyaqc/ocOMC05cvxBgL0Ono0uunnVrFhz/6UwWtxS6IWwqEqKiqi3jRg/Lp1eOvWUx+1ZQtenw/VnvU6QqH49HiVslWvOhgMcuTIEcPak0QthEN16dKFYJRJtkNVFfX909TaWr79xz8yeM8e3IEAKpaSM5crfuPZNutZ79y507C2JFEL4VApKSn07t07qnN2FRbia7DnZ+fz57lv/nx+8vTTl3raIjIuA8f3JVEL4WDRjlFvHDuWC1lZl5J1CPB5vSydMQNfaqoJETpXTk6OYW1JHbUQDnXu3DkqKiqiOsefmsr/mzOHsRs3UlhSQlWHDqwfP55DffuaFKVz+Qy8ESuJWgiHKisrw+Px4I9yyCKQmsq6iRNZN3GiSZE5n9vtNrRHLUMfQjhU586do07SwhjBYNDQaeSSqIVwqG3btlkdQlK7cOGCYW1JohbCoTZt2mR1CEnNyH0TJVEL4VARDXvYcPr1ZewcWxuMHHaSRC2EQxUUFLR9UP0kEbsmRJtNYonGyZMnDWtLErUQDjVUqfD078ZJuPHXdk/WCSrFwH0jJVEL4UDFW7bw4fbtaLe7aa+0uV6q1gnde7UjmUIuhGiR1polH36I3+tt6YD4BpSkjFxBL+JErZRyK6W+VEq9b9jVhRCGq62tpaq1G1mNes4qGCS9uhpXe1bJE01kZ2cb1lY0PervAcb15YUQpkhJScHtiWDScV3FR+fycu596y1CMvRhqJEjRxrWVkSJWinVC/hfwMuGXVkIYQqXy8XYsWPbPrAuMZ/LzubVb387MXZzSSA9e/Y0rK1I35lfA08RXkyrWUqpOUqpDUqpDWVlZUbEJoSIUcS7jyv11ZrRwlBdu3Y1rK02E7VS6hbglNZ6Y2vHaa3naq2LtNZFRgYohIjehg0brA4h6UX8yzICkfSorwFuU0odAOYD1yul3jAsAiGEocrLy8Oz4qS6wzKdO3c2tL027zhorf8V+FcApdRU4Eda668bGoUQot1CoRA7duxg4cKF4QesHs5I4trsadOmGdqerEcthANorZk3bx6lpaVWh/KVlpK0UQncpr8Ixo8fz/Dhww1tM6pErbVeCaw0NAIhRLsVFxfbK0m3xqjkarMkPXr0aGbOnEmqCVuWST2OEA6wePFiq0NIai6Xi+7du5uSpEEStRAJr7a21tC1j00X601OG98cDYVCHDp0yLT2JVELkeAuXrxodQjRiXXIwmZDHY0ZuaNLY5KohbApn89HRUUFuo2eZKdOnXC73XGKSrTEyLrpxqTqQwib8fv9vP/++xQXF6OUIi0tjZtvvpmhQ4c2e7xSihkzZvDRRx/FOVLR0KBBg0xrW3rUQtjMP/7xD3bs2EEwGCQQCFBZWck777zDkSNHLh0TDAbZv38/paWl+P1+xo8fT2ZmpoVRJzelFEVFRaa1Lz1qIWykoqKCXbt2NXnc7/fz2Wefcf/993Po0CHefPNN/H4/WmtcLhc33HADY8aMYfXq1YRCLS7JI0xSUFBAenq6ae1Lj1oIm9Ba88YbLa/OcOrUKXzV1Sz4/e/xV1dfGrsOhUJ89NFHdO/enYyMDBmvtsC+ffvYs2ePae1LohbCJpYsWdLqriA15eXMf/xxKr1eFDTZQXzJkiV85zvfYfz48ZKsLfC3v/3N0J3HG5JELYQNnDlzhnXr1rV6TK3WHOjVi5DHQ8jjCZerNag0uHDhAh06dGD8+PFtVooI42mtOXDggCltyxi1EDYwf/78iI7Tbezccu7cOS5evCiJ2gJmludJj1oIi+3fv58zJ0+2fpDWEMFNwrVr19KlSxdJ1Bbp16+fKe1Kj1oIC4RCIUpKSti6dSt7d+5kwurVrL36akIpKc2foHVEW2UdPXoUr9dLv379TPsYnjS0xh0MEnS725wVqZRi1qxZeFva+b2dJFELEUc1NTWcOnWKxYsXc7KuF9374EHWTZgQHnduvHRn/dcR7mfYvXt3AGbNmsWLL75ITU3Npd612+0mKDuNt01rrlq/nimffkp6TQ1VHTqwfNo0towZ0+IpPXr0MHXCiyRqIQygtebo0aPs3r2blJQURowYcdkuH1prVqxYwZo1awgGg5cNTRzu27flHluU455TpkwBICMjgyeffJL169ezadMmqqurcbvdaK2lzroNV61fz/Rly0ipq+DoWFnJzR9+SMDjobiFncXPnj1rakySqEViq6mB1athyBDo3duSELTWLFq0iOLiYvx+Py6Xi08//ZTbbruNkSNHcvz4cZYtW8a+fftMjyUzM5Nz586RkpJCRkYGu3fvprKykmAwaFrpmKNozZRPP72UpOul+P1ct2JFi4m6pqaGhQsXcscdd5gSliRqkbj69IHDhy9/7LPP4Npr4xaC1pp//vOfbN68+dJjoVCIUCjEu+++y6ZNm9oeKzawWuCZZ565dP2cnBzKy8tluCMKrmCQjBaWjM06f77Vc7ds2cKQIUNaXJOlPSRRi8Q0ZUrTJA0waRJUVUFGhimX1Vqzbt06Plm8GD98lWSbSbbBYDDuN/Rqa2sv/busrCyu13aCkNtNRceOdKqoaPLc2ZycNs9fsWKFJGohLlm1quXnOnQwZZH5Cxcu8MILLxAMBJpMNhEOoRTLpk/nlvffv2z4w+fxsHTGjDZPr6ysNCUsSdTCmfr3BwPHhH0+Hy+++OJXSVo41rZRo/ClpDCkpIScM2dIra1l+fTplA4e3Oa5DW8gG0kStXCm/fth+nRYtsyQ5rZv347P53NMkk5NTb1smERcrmToUEpiGMIwawU9mZkoEtPtt7d9zPLl8JvfGHK5Y8eOGdJOrJRShi205HK5+MEPfmBa7y+ZmTWNXBK1SEwLF0Z23I9+ZMh4ddeuXdvdRrRmzZrFiBEjuOqqq5gzZw5DhgyJ6nylVJPE4fV6mTx5MikpKUydOtXAaAVAYWGhKe1KohaJK5IEHAjAyJHNV4hEobCwEJfLFbedsNPS0hg2bBh33303N998M927d+fqq6+O6Nz65JyamkpRUdGlNapTUlKYOHEikydPBmhztb6oyNoiAPTs2dOUdmWMWiS2QCBc5dHaeOuuXXDTTbBtW0xjzKFQiHnz5n31QMNp3lrT5+BB8k6e5GxuLnv79494undrunbtitaa8vJy/va3v3H8+PGIz62f9Xjx4kU2b97MjBkzGDFiBKmpqbhcLrTWLFiwIKo22+SQsfv2chnw3jdHErVIbG43VFdDSgq0NLEjGIQ9e+C55+DxxyEtLapLlJaWcu7cufDUa6XC7blceH0+Zr/2Gl1On8YVCoHW+L1e1l59NYf69uVwr16EYlyk59ixY6xZs4bly5e3a8q33+9n2bJljBkz5lIS2bZtGyUlJTG3KVrWrVs3U9qVoQ+R+FwuOH0aWtvc1eeDp56CvLxwzzoKx48fD1d81KtbTW3asmXknTxJqs+HNxDAl5bGy3PmsG7CBA716YO70Q4s0QgGgyxdutSQdTn8fj9Hjx699PWmTZtkOrkJUlNT5WaiEK3q3BkqKuDRR6GlxfVDIbhwASZPjiqBZmdn42mmzSu2bcPToBe/+KabOJ+VhS81Fe12409JscXYrdaat99+m4sXLwLIlHKTPPTQQ6a13WaiVkr1VkqtUErtVEoVK6W+Z1o0QrTXL38Jffu2Oryhy8vRzz4bUXN+v5+ysjICgUCT51wNersa2FVYSKhxCV2DMcvmkn281NTU8MwzzzBv3jwyMzMtjcWJevbsSY8ePUxrP5J3KwD8UGu9SSnVEdiolFqqtd5hWlRCxKpjR9i0Cf7wB/j5z8M3G+to4HDv3pzq1o2NpaVMXL+ekePGXX7+hQvwxhuUf/IJxRkZLB8wgJb6xCVDhjC8uBh3XcLWrXzs9Xq9/PSnP2XhwoVsi3LoxUi7d+/G4/EQCoXwer34/X48Hk+zv4hEZHr27Mm3vvUtU6/RZqLWWh8Hjtf9u0IptRPoCUiiFvbUqRP85Cfw6aeweDEANWlpvD57NmdzctBK4fd6eWfxYlKysi7VJ+9csYLdL7xACLhi61YqBgxA9+/fYkXDxzfcQN+DB0mrqSHV72dwSQklQ4agG/aq6ypEsrOzcblc3HnnnRw+fJjy8nKT/xNaFggEUErRpUsXCgoK6NChA0uXLrUsnkTWrVs3HnnkEVP3S4Qoqz6UUv2AMUCTAkyl1BxgDkCfPn2MiE2I9nn+eVi8GA18cMstnOraNbyLSh1NeFPZ+mGAgN8Po0YBUDx8eJtbMFV17MjvnniC4cXF9N+7l8krV3KsZ08upqXhS0lBaY2uG/oYVdeuUoqBAweyYcMGc15zhLTWnDhxgjlz5hAKhSRRx2j48OGmJ2mI4maiUioTWAB8X2t9ofHzWuu5WusirXWRFbO4hGhiyBD42c8437EjO4YNuyxJNxQIBMIf/etXxFOKoNcbUW1wICWFLWPG8M499zD3scdQwSApdTXdusH4dMMdXXpbtMFBY/W/oE6cOGFxJInJ4/FwzTXXxOdakRyklPISTtJvaq3/YW5IQhjov/6L0iVLWh0/blEM55xvYc3izz//nGXLluFyuWyxFZZSihEjRgDhSRpuQGpBIufxePjOd75j2PorbYmk6kMBfwJ2aq2fMz8kIYwT0poPbrnF6jAulcbZIUlDuIe/d+9eqqqqyMvLQ9skrkTg8XiYM2cOubm5cbtmJEMf1wDfAK5XSm2u+3OzyXEJYYiSkhJZ5L8FlZWVLF26lIqKCknUEerevTtz5syJ+yJdkVR9fA7Id7lISOfOnbM6BNsKhULs2rWL6dOnXzaeLprXu3dv08vwWiLvjnC0gQMHtn5AO6Z5O4HL5WL37t2GLCTldFZutCDvjnC0bt26hcunWkvGSZyou3fvzkcffWR1GAnByqn3Mo9UOFooFAqXxrU0Rp3k49fHjx83bWlOpzFrrelIyDskHO3MmTNWh2BrPp/PNpUodjd9+nTLri2JWjiaN8b1oJNFp06dSE1NtToMW1NK8cADD9CxY0fLYpBELRxNklDrRowYQUZGhtVh2NrYsWMZNGiQpTHIGLVwNLlR1jylFC6Xi88//9zqUGzP6iQN0qMWDqa1Zvv27VaHYTv9+vVDKSUbCEQgPT3dFolaetTC0eRGWVOHDh2S/5cIdOvWjfvvvz8uq+O1RRK1EElGknTrXC4X1157Ldddd53VoVwiQx/CsUKhkC16Q6ZJ4ok6ZtJa06FDB6vDuIwkauFYbreb7t27Wx2GseqnvNclaZdsoWU4rTVDhw61OozLSKIWjua4OuoGmxvU/8lux6Se7OxsMjMzDQww8fXo0cPSmunmSKIWjnXy5EkOHTpkdRimqt//MVY33ngjPp/PwIgSm8fj4dZbb7U6jCYkUQvHeuONN6wOwXRaKSo7dYrpXKUU3bt3Jzs72+CoElNubi4PPvigLYfLJFELx6qsrLQ6hJjl5+dHdmA7bpbm5eWRlZXF9ddff2n/xHqNv04GDz30EP369bM6jGZJohaOdODAAatDaJd4LCZVP3V88ODB3HHHHWRlZV16fNq0aXzta18zPQY7sfMvJ/tGJkQ7rF271uoQ2iUe48aBBhUjw4cPZ/jw4YRCocuWPb377rtZsGCB6bHYgc/ns+26J9KjFo5UVVXVrvPdbjfp6ekGRWM/Xq+XkSNHNnm88drUI0aMYMiQIfEKyzIdO3a89InCjiRRC0fq0aNHu87XWjNz5kxHTpjxeDzk5eUxevToiI6v30Hdye68805bv9eSqIUjjR07tt1tbNmyJbw7jMNMmjSJhx9+OOIx2b59+5ockbXy8/MpKCiwOoxWSaIWjtS5c+d2nR8Khdi3b58xwdiMz+fD7XZHfHxRUZGtb7S1V//+/a0OoU2SqIUjnThxwtYfZa10/vz5qI7v2LEj3/zmN+natatJEVlHKcW1115rdRhtkkQtHCk9PV02bW1BLAm3R48ePPbYY7ZbA6O9br/9dtLS0qwOo03ynSwcqWvXruTm5lodhu24XC5GjBgR8/nDhg1zzCcVpZRty/Eak0QtHOvOO++0OgTbueaaa8jJyYn6vOrqalasWMHatWsd80mlfgp9InDuHQKR9GJJSPGQnp5OTU2NJdeeMmVK1OdUVlby0ksvcfHiRVtv3+V2uwkGgyilIqrWGTNmjO1WyWuJJGrhWKdOnbI6hGZZlaQB1qxZE/XNs1WrVlFTU2PrnWHuueceTp8+zblz5+jRowdffPEFp0+fbvH46667jkmTJsUxwvaRRC0c680337Q6BNtZvnw5o0aNiqonWVpaauskPWPGDIYPH37ZY1dddRUHDhxg1apVl637kp+fzwMPPJBwa3BHlKiVUjOB3wBu4GWt9dOmRiVEO2mtk2JGXSzeeecdHnrooYiPT09P59y5cyZGFBm3203fvn3p168f1dXV5OXlMXz48GY3h1BKUVBQQEFBAVprqqurSUlJSdiNJNpM1EopN/AiMAM4AnyhlFqktd5hdnBCCONFu7LghAkTWLRoEX6/35yAIjR69GhmzpwZ9eQbpZTt9kCMViSveBxQqrXeB6CUmg/cDkiiFiIBRTstfvjw4Zw6dYrPP//ckin16enpfOMb34h8jW4HiiRR9wQON/j6CDC+8UFKqTnAHIA+ffoYEpwQsSouLrY6BNuKZvo4hHuk119/PX6/37TlYwsLC6msrKS2tpa0tDS6detGZmYm/fv3p3fv3o6p3Y5VJIm6uf+hJr9WtdZzgbkARUVFzlvJRiSUTz75xOoQbCvWCS8FBQWsW7cupl717Nmzee+99zh79myTNu+9915SU1NjiilZRJKojwC9G3zdCzhmTjhCGCORt+Eyk8vlYvr06TGdO3DgQDp37hzVjUWXy8XgwYPp168fTz75ZEzXFZHNTPwCGKSUKlBKpQD3AYvMDUuI9nHyov/tobWOuY7b5XLxyCOPMGzYsIjPyc/P57bbbovpeuIrbfaotdYBpdQTwBLC5XmvaK1lAFDY2tixY1mxYoXVYdiO1pp9+/bFvBJehw4dmDVrFlprzp8/z+uvv96kh52RkcHgwYMpKiqiZ8+eRoSd9CKqc9Fafwh8aHIsQhhm4sSJkqhbYMRqcUopOnfuzHe/+13gq0qSZL/pZxaZmSgcqba21uoQbMnj8VBYWGh4u5KgzeWMZbCEaMQpK7zFqrkSPJfLxYMPPigVFglIetTCkdLT00lNTU26nrVSinHjxjF16lR27NjBli1bCIVCjBw5krFjx0ZdQy3sQRK1cKxp06bx4YfOurVy0003UVZWxoYNGy57fMSIERQWFjJw4MBLPeYrr7ySK6+80oowhcEkUQvHGjFiRMIn6t69e5OVlUV6ejqTJk26tOrdhAkT2LVrF263m2HDhiXMusoiNpKohWOlp6dbukh/eyilyMnJYfbs2c0OV+Tk5DBx4kQLIhNWkEQtHC07OzuhEnVaWhqpqamMGDGCSZMmyZiyACRRC4cbPXo0x44lxooHHo+HRx55hC5dulgdirAZSdTC0exaptdwTWWXy4VSittvv12StGiWJGrhaEqpiDc7jad77rmHwYMHU1ZWhs/nIz8/X4Y5RIvs2d0QwiCDBg2yXZIGOH36NEopunXrRq9evSRJi1ZJohaOZtehDzv+8hD2Zc/vYiEMYtfp0iNHjrQ6BJFAJFELR/N4PGRlZVkdxmUGDBhgu5iEvUmiFo43evRoq0O4pFu3btx///1WhyESjCRq4XgHDx60OgQgXIFyxx13yI1DETVJ1MLxunXrZnUIAIwfP578/HyrwxAJSBK1cLxx48ZZHQLjxo3jxhtvtDoMkaBkwotwvNzcXKZOncrKlSvjel2Xy0Vubi633HILffr0ieu1hbNIohZJYcqUKWzYsIHKykrTrpGfn88NN9xAnz59bFu/LRKTJGqRNB599FF+9atfGdqmx+NhzJgx3HTTTbJvoDCNJGqRNDIzMxk5ciTbtm1rVztdu3bl0UcfleoNETeSqEVSueuuu9Bas3379qjPvf766xk3bpxtZzsK51JmrDlQVFSkG+/pJoSdBINB1q1bR2lpKefPn+fs2bNNjunatSuBQIDevXszefJkcnNzLYhUJAul1EatdVFzz0mPWiQlt9vNxIkTL21npbWmpKSEI0eO0LdvXwYOHChjzsI2JFELQXjWYGFhIYWFhVaHIkQTUkMkhBA2J4laCCFsThK1EELYnCRqIYSwOUnUQghhc6bUUSulKoASwxu2ry7AaauDiLNke83J9noh+V6z1a+3r9a6a3NPmFWeV9JS4bYTKaU2JNPrheR7zcn2eiH5XrOdX68MfQghhM1JohZCCJszK1HPNaldu0q21wvJ95qT7fVC8r1m275eU24mCiGEMI4MfQghhM1JohZCCJszLFErpWYppYqVUiGlVFGj5/5VKVWqlCpRSjlyK2al1L8rpY4qpTbX/bnZ6pjMoJSaWfc+liqlfmp1PPGglDqglNpW9746bqF1pdQrSqlTSqntDR7LUUotVUrtqfs728oYjdbCa7btz7CRPertwF3AqoYPKqWGAfcBw4GZwO+VUk7dw+h5rfXouj8fWh2M0eretxeBm4BhwP11728yuK7ufbVlnW07vUr4Z7OhnwLLtdaDgOV1XzvJqzR9zWDTn2HDErXWeqfWurnZiLcD87XWtVrr/UApMM6o64q4GgeUaq33aa19wHzC769IYFrrVUDjLW5uB16r+/drwB3xjMlsLbxm24rHGHVP4HCDr4/UPeZETyilttZ9rHLUR8U6yfReNqSBj5VSG5VSc6wOJk7ytNbHAer+7mZxPPFiy5/hqBK1UmqZUmp7M39a61U1t59RQtYEtvH6/wAMAEYDx4FfWRmrSRzzXkbpGq31lYSHfB5XSk22OiBhCtv+DEe11ofWenoM1zgC9G7wdS/gWAztWC7S16+U+iPwvsnhWMEx72U0tNbH6v4+pZR6h/AQ0KrWz0p4J5VS+Vrr40qpfOCU1QGZTWt9sv7fdvsZjsfQxyLgPqVUqlKqABgErI/DdeOq7pu53p2Eb646zRfAIKVUgVIqhfBN4kUWx2QqpVQHpVTH+n8DN+DM97axRcDsun/PBt61MJa4sPPPsGGr5yml7gR+C3QFPlBKbdZa36i1LlZK/RXYAQSAx7XWQaOuayP/o5QaTXgo4ADwqKXRmEBrHVBKPQEsAdzAK1rrYovDMlse8E7djuQe4C2t9UfWhmQspdQ8YCrQRSl1BPg/wNPAX5VSjwCHgFnWRWi8Fl7zVLv+DMsUciGEsDmZmSiEEDYniVoIIWxOErUQQticJGohhLA5SdRCCGFzkqiFEMLmJFELIYTN/X+nQjF+KxOQSAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(torch.argmax(gate, axis=1))\n",
    "c_clusters = [color[int(y_train[i])] for i in range(len(y_train))]\n",
    "plt.scatter(x_emb_train[:,0], x_emb_train[:,1], color=c_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017895515130531153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fcfb485c5b0>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "clusters = torch.argmax(gate, axis=1).numpy()\n",
    "print(nmi(clusters, y_train))\n",
    "c_clusters = [color[int(clusters[i])] for i in range(len(y_train))]\n",
    "plt.scatter(x_emb_train[:,0], x_emb_train[:,1], color=c_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
