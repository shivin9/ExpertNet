{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import umap\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score as nmi,\\\n",
    "f1_score, roc_auc_score, roc_curve, accuracy_score, matthews_corrcoef as mcc\n",
    "from torch.utils.data import Subset\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "import numbers\n",
    "import torch.nn as nn\n",
    "from read_patients import *\n",
    "# from cac import batch_cac\n",
    "# from kmeans import batch_KMeans\n",
    "# from meanshift import batch_MeanShift\n",
    "# from autoencoder import AutoEncoder\n",
    "from sklearn.metrics import davies_bouldin_score as dbs, adjusted_rand_score as ari\n",
    "from matplotlib import pyplot as plt\n",
    "color = ['grey', 'red', 'blue', 'pink', 'brown', 'black', 'magenta', 'purple', 'orange', 'cyan', 'olive']\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.args = args\n",
    "        self.input_dim = args.input_dim\n",
    "        self.output_dim = self.input_dim\n",
    "        self.hidden_dims = args.hidden_dims\n",
    "        self.hidden_dims.append(args.latent_dim)\n",
    "        self.dims_list = (args.hidden_dims + \n",
    "                          args.hidden_dims[:-1][::-1])  # mirrored structure\n",
    "        self.n_layers = len(self.dims_list)\n",
    "        self.n_clusters = args.n_clusters\n",
    "        self.n_classes = args.n_classes\n",
    "        \n",
    "        # Validation check\n",
    "        print(len(n_layers))\n",
    "        assert self.n_layers % 2 > 0\n",
    "        assert self.dims_list[self.n_layers // 2] == args.latent_dim\n",
    "        \n",
    "        # Encoder Network\n",
    "        layers = OrderedDict()\n",
    "        for idx, hidden_dim in enumerate(self.hidden_dims):\n",
    "            if idx == 0:\n",
    "                layers.update(\n",
    "                    {'linear0': nn.Linear(self.input_dim, hidden_dim),\n",
    "                     'activation0': nn.ReLU()\n",
    "                    })\n",
    "            else:\n",
    "                layers.update(\n",
    "                    {'linear{}'.format(idx): nn.Linear(\n",
    "                        self.hidden_dims[idx-1], hidden_dim),\n",
    "                     'activation{}'.format(idx): nn.ReLU(),\n",
    "                     'bn{}'.format(idx): nn.BatchNorm1d(self.hidden_dims[idx])\n",
    "                    })\n",
    "        self.encoder = nn.Sequential(layers)\n",
    "        \n",
    "        # Decoder Network\n",
    "        layers = OrderedDict()\n",
    "        tmp_hidden_dims = self.hidden_dims[::-1]\n",
    "        for idx, hidden_dim in enumerate(tmp_hidden_dims):\n",
    "            if idx == len(tmp_hidden_dims) - 1:\n",
    "                layers.update(\n",
    "                    {'linear{}'.format(idx): nn.Linear(\n",
    "                        hidden_dim, self.output_dim),\n",
    "                    })\n",
    "            else:\n",
    "                layers.update(\n",
    "                    {'linear{}'.format(idx): nn.Linear(\n",
    "                        hidden_dim, tmp_hidden_dims[idx+1]),\n",
    "                     'activation{}'.format(idx): nn.ReLU(),\n",
    "                     'bn{}'.format(idx): nn.BatchNorm1d(tmp_hidden_dims[idx+1])\n",
    "                    })\n",
    "        self.decoder = nn.Sequential(layers)\n",
    "\n",
    "        # Classification Head\n",
    "#         self.classifiers = []\n",
    "#         for _ in range(self.n_clusters):\n",
    "#             self.classifiers.append(\n",
    "#                 nn.Sequential(\n",
    "#                     nn.Linear(20, 16),\n",
    "#                     nn.ReLU(),\n",
    "#                     nn.Linear(16, 8),\n",
    "#                     nn.ReLU(),\n",
    "#                     nn.Linear(8, self.n_classes)))\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr_str = '[Structure]: {}-'.format(self.input_dim)\n",
    "        for idx, dim in enumerate(self.dims_list):\n",
    "                repr_str += '{}-'.format(dim)\n",
    "        repr_str += str(self.output_dim) + '\\n'\n",
    "        repr_str += '[n_layers]: {}'.format(self.n_layers) + '\\n'\n",
    "        repr_str += '[n_clusters]: {}'.format(self.n_clusters) + '\\n'\n",
    "        repr_str += '[n_classes]: {}'.format(self.n_classes) + '\\n'\n",
    "        repr_str += '[input_dims]: {}'.format(self.input_dim)\n",
    "        return repr_str\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def forward(self, X, latent=False, classifier_idx=0):\n",
    "        body_output = self.encoder(X)\n",
    "        if latent:\n",
    "            return body_output\n",
    "#         probs = self.classifiers[classifier_idx](body_output)\n",
    "        return self.decoder(body_output), _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "## Batch k-Means\n",
    "\n",
    "def _parallel_compute_distance(X, cluster):\n",
    "    n_samples = X.shape[0]\n",
    "    dis_mat = np.zeros((n_samples, 1))\n",
    "    for i in range(n_samples):\n",
    "        dis_mat[i] += np.sqrt(np.sum((X[i] - cluster) ** 2, axis=0))\n",
    "    return dis_mat\n",
    "\n",
    "class batch_KMeans(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.latent_dim = args.latent_dim\n",
    "        self.n_clusters = args.n_clusters\n",
    "        self.clusters = np.zeros((self.n_clusters, self.latent_dim))\n",
    "        self.count = 100 * np.ones((self.n_clusters))  # serve as learning rate\n",
    "        self.n_jobs = args.n_jobs\n",
    "        self.positive_centers = np.zeros((self.n_clusters, self.latent_dim))\n",
    "        self.negative_centers = np.zeros((self.n_clusters, self.latent_dim))\n",
    "        self.cluster_stats = np.zeros((self.n_clusters,2))\n",
    "\n",
    "    \n",
    "    def _compute_dist(self, X):\n",
    "        dis_mat = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_parallel_compute_distance)(X, self.clusters[i])\n",
    "            for i in range(self.n_clusters))\n",
    "        dis_mat = np.hstack(dis_mat)\n",
    "        \n",
    "        return dis_mat\n",
    "\n",
    "\n",
    "    def init_cluster(self, X, y=None, indices=None):\n",
    "        \"\"\" Generate initial clusters using sklearn.Kmeans \"\"\"\n",
    "        model = KMeans(n_clusters=self.n_clusters,\n",
    "                       n_init=20)\n",
    "        model.fit(X)\n",
    "        self.clusters = model.cluster_centers_  # copy clusters\n",
    "        labels = model.labels_\n",
    "\n",
    "        for j in range(self.n_clusters):\n",
    "            pts_index = np.where(labels == j)[0]\n",
    "            cluster_pts = X[pts_index]        \n",
    "            n_class_index = np.where(y[pts_index] == 0)[0]\n",
    "            p_class_index = np.where(y[pts_index] == 1)[0]\n",
    "\n",
    "            self.cluster_stats[j][0] = len(p_class_index)\n",
    "            self.cluster_stats[j][1] = len(n_class_index)\n",
    "\n",
    "            n_class = cluster_pts[n_class_index]\n",
    "            p_class = cluster_pts[p_class_index]\n",
    "\n",
    "            self.negative_centers[j,:] = n_class.mean(axis=0)\n",
    "            self.positive_centers[j,:] = p_class.mean(axis=0)\n",
    "\n",
    "\n",
    "    def update_cluster(self, X, y, cluster_idx):\n",
    "        \"\"\" Update clusters in Kmeans on a batch of data \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        for i in range(n_samples):\n",
    "            self.count[cluster_idx] += 1\n",
    "            eta = 1.0 / self.count[cluster_idx]\n",
    "            updated_cluster = ((1 - eta) * self.clusters[cluster_idx] + \n",
    "                               eta * X[i])\n",
    "            self.clusters[cluster_idx] = updated_cluster\n",
    "            if y[i] == 0:\n",
    "                self.negative_centers[cluster_idx] = ((1 - eta) * self.negative_centers[cluster_idx] + \n",
    "                               eta * X[i])\n",
    "            else:\n",
    "                self.positive_centers[cluster_idx] = ((1 - eta) * self.positive_centers[cluster_idx] + \n",
    "                               eta * X[i])\n",
    "\n",
    "\n",
    "    def update_assign(self, X, y=None):\n",
    "        \"\"\" Assign samples in `X` to clusters \"\"\"\n",
    "        dis_mat = self._compute_dist(X)\n",
    "        new_labels = np.argmin(dis_mat, axis=1)\n",
    "\n",
    "#         if y is not None:\n",
    "#             for j in range(self.n_clusters):\n",
    "#                 pts_index = np.where(new_labels == j)[0]\n",
    "#                 cluster_pts = X[pts_index]        \n",
    "#                 n_class_index = np.where(y[pts_index] == 0)[0]\n",
    "#                 p_class_index = np.where(y[pts_index] == 1)[0]\n",
    "\n",
    "#                 self.cluster_stats[j][0] = len(p_class_index)\n",
    "#                 self.cluster_stats[j][1] = len(n_class_index)\n",
    "\n",
    "#                 n_class = cluster_pts[n_class_index]\n",
    "#                 p_class = cluster_pts[p_class_index]\n",
    "\n",
    "#                 self.negative_centers[j,:] = n_class.mean(axis=0)\n",
    "#                 self.positive_centers[j,:] = p_class.mean(axis=0)\n",
    "\n",
    "        return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "code_folding": [
     7
    ]
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import adjusted_rand_score as ari\n",
    "\n",
    "class batch_cac(object):    \n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.latent_dim = args.latent_dim\n",
    "        self.n_clusters = args.n_clusters\n",
    "        self.cluster_stats = np.zeros((self.n_clusters,2))\n",
    "        self.clusters = np.zeros((self.n_clusters, self.latent_dim))\n",
    "        self.positive_centers = np.zeros((self.n_clusters, self.latent_dim))\n",
    "        self.negative_centers = np.zeros((self.n_clusters, self.latent_dim))\n",
    "        self.positive_sse = np.zeros(self.n_clusters)\n",
    "        self.negative_sse = np.zeros(self.n_clusters)\n",
    "        self.count = 100 * np.ones((self.n_clusters))  # serve as learning rate\n",
    "        self.n_jobs = args.n_jobs\n",
    "\n",
    "\n",
    "    def calculate_gamma_old(self, pt, label, mu, mup, mun, p_sse, n_sse, cluster_stats, beta=1, alpha=2):\n",
    "        p, n = cluster_stats[0], cluster_stats[1]\n",
    "        if label == 0:\n",
    "            mun_new = (n/(n-1))*mun - (1/(n-1))*pt\n",
    "            mup_new = mup\n",
    "            new_n_sse = (n/(n-1))*n_sse - (1/(n-1))*np.linalg.norm(pt-mun_new)*np.linalg.norm(pt-mun)\n",
    "            new_p_sse = p_sse\n",
    "            n_new = n-1\n",
    "            p_new = p\n",
    "\n",
    "        else:\n",
    "            mup_new = (p/(p-1))*mup - (1/(p-1))*pt\n",
    "            mun_new = mun\n",
    "            new_p_sse = (p/(p-1))*p_sse - (1/(p-1))*np.linalg.norm(pt-mup_new)*np.linalg.norm(pt-mup)\n",
    "            new_n_sse = n_sse\n",
    "            p_new = p-1\n",
    "            n_new = n\n",
    "\n",
    "        mu_new = (p_new*mup_new + n_new*mun_new)/(p_new + n_new)\n",
    "        new_lin_sep = np.sum(np.square(mun_new - mup_new))\n",
    "        lin_sep = np.sum(np.square(mun - mup))\n",
    "#         new_lin_sep = np.sum(np.square(mun_new - mup_new))/(new_n_sse + new_p_sse)\n",
    "#         lin_sep = np.sum(np.square(mun - mup))/(n_sse + p_sse)\n",
    "        mu_sep = np.sum(np.square(mu - mu_new))\n",
    "#         gamma_p = -beta*np.sum(np.square(mu-pt)) - (p+n-1) * mu_sep + (p+n) * alpha*lin_sep - (p+n-1)*alpha*new_lin_sep\n",
    "        gamma_p = -np.sum(np.square(mu-pt)) - (p+n-1) * mu_sep + alpha*lin_sep - alpha*new_lin_sep\n",
    "        return gamma_p\n",
    "\n",
    "\n",
    "    def calculate_gamma_new(self, pt, label, mu, mup, mun, p_sse, n_sse, cluster_stats, beta=1, alpha=2):\n",
    "        p, n = cluster_stats[0], cluster_stats[1]\n",
    "        if label == 0:\n",
    "            mun_new = (n/(n+1))*mun + (1/(n+1))*pt\n",
    "            mup_new = mup\n",
    "            new_n_sse = (n/(n+1))*n_sse + (1/(n+1))*np.linalg.norm(pt-mun_new)*np.linalg.norm(pt-mun)\n",
    "            new_p_sse = p_sse\n",
    "            n_new = n+1\n",
    "            p_new = p\n",
    "\n",
    "        else:\n",
    "            mup_new = (p/(p+1))*mup + (1/(p+1))*pt\n",
    "            mun_new = mun\n",
    "            new_p_sse = (p/(p+1))*p_sse + (1/(p+1))*np.linalg.norm(pt-mup_new)*np.linalg.norm(pt-mup)\n",
    "            new_n_sse = n_sse\n",
    "            p_new = p+1\n",
    "            n_new = n\n",
    "\n",
    "        mu_new = (p_new*mup_new + n_new*mun_new)/(p_new + n_new)\n",
    "        new_lin_sep = np.sum(np.square(mun_new - mup_new))\n",
    "        lin_sep = np.sum(np.square(mun - mup))\n",
    "#         new_lin_sep = np.sum(np.square(mun_new - mup_new))/(new_n_sse + new_p_sse)\n",
    "#         lin_sep = np.sum(np.square(mun - mup))/(n_sse + p_sse)\n",
    "        mu_sep = np.sum(np.square(mu - mu_new))\n",
    "\n",
    "#         gamma_j = beta*np.sum(np.square(mu_new-pt)) + (p+n)*mu_sep + (p+n) * alpha*lin_sep - (p+n+1)*alpha*new_lin_sep\n",
    "        gamma_j = np.sum(np.square(mu_new-pt)) + (p+n)*mu_sep + alpha*lin_sep - alpha*new_lin_sep\n",
    "        return gamma_j\n",
    "\n",
    "    def predict_clusters(self, X_test, centers) -> np.array:\n",
    "        K = centers.shape[0]\n",
    "        dists = np.zeros(K)\n",
    "        test_labels = np.zeros(X_test.shape[0])\n",
    "\n",
    "        for pt in range(X_test.shape[0]):\n",
    "            for k in range(K):\n",
    "                min_dist = np.square(np.linalg.norm(centers[k] - X_test[pt]))\n",
    "                dists[k] = min_dist\n",
    "            test_labels[pt] = int(np.argmin(dists))\n",
    "        return test_labels.astype(int)\n",
    "\n",
    "\n",
    "    def update_cluster_centers(self, X, y, cluster_labels):\n",
    "        for j in range(self.n_clusters):\n",
    "            pts_index = np.where(cluster_labels == j)[0]\n",
    "            cluster_pts = X[pts_index]        \n",
    "            for pt in pts_index:\n",
    "                self.count[j] += 1\n",
    "                eta = 1/(self.count[j])\n",
    "                self.clusters[j,:] = (1-eta)*self.clusters[j,:] + eta*X[pt]\n",
    "\n",
    "                if y[pt] == 0:\n",
    "                    self.negative_centers[j,:] = (1-eta)*self.negative_centers[j,:] +\\\n",
    "                                                    eta*X[pt]\n",
    "                else:\n",
    "                    self.positive_centers[j,:] = (1-eta)*self.positive_centers[j,:] +\\\n",
    "                                                    eta*X[pt]\n",
    "\n",
    "            n_class_index = np.where(y[pts_index] == 0)[0]\n",
    "            p_class_index = np.where(y[pts_index] == 1)[0]\n",
    "            self.cluster_stats[j][0] = len(p_class_index)\n",
    "            self.cluster_stats[j][1] = len(n_class_index)\n",
    "            n_class = cluster_pts[n_class_index]\n",
    "            p_class = cluster_pts[p_class_index]\n",
    "            self.negative_sse[j] = np.square(np.linalg.norm(n_class - self.negative_centers[j]))\n",
    "            self.positive_sse[j] = np.square(np.linalg.norm(p_class - self.positive_centers[j]))\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def update(self, X, y, labels, beta, alpha):\n",
    "        total_iterations = 100\n",
    "        k = self.n_clusters\n",
    "        errors = np.zeros((total_iterations, k))\n",
    "        lbls = []\n",
    "        lbls.append(np.copy(labels))\n",
    "\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return cluster_stats, labels, self.clusters, self.positive_centers, self.negative_centers\n",
    "\n",
    "        old_p, old_n = np.copy(self.positive_centers), np.copy(self.negative_centers)\n",
    "\n",
    "        for iteration in range(0, total_iterations):\n",
    "            # print(cluster_stats)\n",
    "            N = X.shape[0]\n",
    "            cluster_label = []\n",
    "            for index_point in range(N):\n",
    "                distance = {}\n",
    "                pt = X[index_point]\n",
    "                pt_label = y[index_point]\n",
    "                cluster_id = labels[index_point]\n",
    "                p, n = self.cluster_stats[cluster_id][0], self.cluster_stats[cluster_id][1]\n",
    "                new_cluster = old_cluster = labels[index_point]\n",
    "                old_err = np.zeros(k)\n",
    "                # Ensure that degeneracy is not happening\n",
    "                if ((p > 2 and pt_label == 1) or (n > 2 and pt_label == 0)):\n",
    "                    for cluster_id in range(0, k):\n",
    "                        if cluster_id != old_cluster:\n",
    "                            distance[cluster_id] = self.calculate_gamma_new(pt, pt_label, self.clusters[cluster_id], self.positive_centers[cluster_id],\\\n",
    "                                                    self.negative_centers[cluster_id], self.positive_sse[cluster_id], self.negative_sse[cluster_id], self.cluster_stats[cluster_id], beta, alpha)\n",
    "                        else:\n",
    "                            distance[cluster_id] = np.infty\n",
    "\n",
    "                    old_gamma = self.calculate_gamma_old(pt, pt_label, self.clusters[old_cluster], self.positive_centers[old_cluster],\\\n",
    "                                                    self.negative_centers[old_cluster], self.positive_sse[old_cluster], self.negative_sse[old_cluster], self.cluster_stats[old_cluster], beta, alpha)\n",
    "                    # new update condition\n",
    "                    new_cluster = min(distance, key=distance.get)\n",
    "                    new_gamma = distance[new_cluster]\n",
    "\n",
    "                    if old_gamma + new_gamma < 0:\n",
    "                        # Remove point from old cluster\n",
    "                        p, n = self.cluster_stats[old_cluster] # Old cluster statistics\n",
    "                        t = p + n\n",
    "\n",
    "                        self.clusters[old_cluster] = (t/(t-1))*self.clusters[old_cluster] - (1/(t-1))*pt\n",
    "\n",
    "                        if pt_label == 0:\n",
    "                            new_mean = (n/(n-1))*self.negative_centers[old_cluster] - (1/(n-1)) * pt\n",
    "                            old_mean = self.negative_centers[old_cluster]\n",
    "                            self.negative_sse[old_cluster] = (n/(n-1))*self.negative_sse[old_cluster] - \\\n",
    "                                    (1/(n-1))*np.linalg.norm(pt-new_mean)*np.linalg.norm(pt-old_mean)\n",
    "                            self.negative_centers[old_cluster] = new_mean\n",
    "                            self.cluster_stats[old_cluster][1] -= 1\n",
    "\n",
    "                        else:\n",
    "                            new_mean = (p/(p-1))*self.positive_centers[old_cluster] - (1/(p-1)) * pt\n",
    "                            old_mean = self.positive_centers[old_cluster]\n",
    "                            self.positive_sse[old_cluster] = (p/(p-1))*self.positive_sse[old_cluster] - \\\n",
    "                                    (1/(p-1)) * np.linalg.norm(pt-new_mean)*np.linalg.norm(pt-old_mean)\n",
    "                            self.positive_centers[old_cluster] = new_mean\n",
    "                            self.cluster_stats[old_cluster][0] -= 1\n",
    "\n",
    "\n",
    "                        # Add point to new cluster\n",
    "                        p, n = self.cluster_stats[new_cluster] # New cluster statistics\n",
    "                        t = p + n\n",
    "                        self.clusters[new_cluster] = (t/(t+1))*self.clusters[new_cluster] + (1/(t+1))*pt\n",
    "\n",
    "                        if pt_label == 0:\n",
    "                            new_mean = (n/(n+1))*self.negative_centers[new_cluster] + (1/(n+1)) * pt\n",
    "                            old_mean = self.negative_centers[new_cluster]\n",
    "                            self.negative_sse[new_cluster] = (n/(n+1))*self.negative_sse[new_cluster] + \\\n",
    "                                    (1/(n+1)) * np.linalg.norm(pt-new_mean) * np.linalg.norm(pt-old_mean)\n",
    "                            self.negative_centers[new_cluster] = new_mean\n",
    "                            self.cluster_stats[new_cluster][1] += 1\n",
    "\n",
    "                        else:\n",
    "                            new_mean = (p/(p+1))*self.positive_centers[new_cluster] + (1/(p+1)) * pt\n",
    "                            old_mean = self.positive_centers[new_cluster]\n",
    "                            self.positive_sse[new_cluster] = (p/(p+1))*self.positive_sse[new_cluster] + \\\n",
    "                                    (1/(p+1)) * np.linalg.norm(pt-new_mean) * np.linalg.norm(pt-old_mean)\n",
    "                            self.positive_centers[new_cluster] = new_mean\n",
    "                            self.cluster_stats[new_cluster][0] += 1\n",
    "\n",
    "                        labels[index_point] = new_cluster\n",
    "\n",
    "\n",
    "            lbls.append(np.copy(labels))\n",
    "\n",
    "            if ((lbls[iteration] == lbls[iteration-1]).all()) and iteration > 0:\n",
    "#                 print(\"converged at itr: \", iteration)\n",
    "                break\n",
    "\n",
    "        return labels\n",
    "\n",
    "    \n",
    "    def cluster(self, X, y, beta, alpha):\n",
    "        # Update assigned cluster labels to points\n",
    "        cluster_labels = self.predict_clusters(X, self.clusters)\n",
    "#         print(\"Std of prev. labels: \", np.std(cluster_labels))\n",
    "        # Do we need this really? ... yes\n",
    "        self.update_cluster_centers(X, y, cluster_labels)\n",
    "\n",
    "        # update cluster centers\n",
    "        new_labels = self.update(X, y, cluster_labels, beta, alpha)\n",
    "#         print(\"Std of new labels: \", np.std(new_labels))\n",
    "\n",
    "        return new_labels\n",
    "\n",
    "\n",
    "    def init_cluster(self, X, y, indices=None):\n",
    "        \"\"\" Generate initial clusters using sklearn.Kmeans \"\"\"\n",
    "        \"\"\" X will be AE embeddings \"\"\"\n",
    "        model = KMeans(n_clusters=self.n_clusters,\n",
    "                       n_init=20)\n",
    "        model.fit(X)\n",
    "        self.clusters = model.cluster_centers_  # copy clusters\n",
    "        labels = model.labels_\n",
    "\n",
    "        for j in range(self.n_clusters):\n",
    "            pts_index = np.where(labels == j)[0]\n",
    "            cluster_pts = X[pts_index]\n",
    "#             assert(np.allclose(self.clusters[j,:], cluster_pts.mean(axis=0)))\n",
    "            n_class_index = np.where(y[pts_index] == 0)[0]\n",
    "            p_class_index = np.where(y[pts_index] == 1)[0]\n",
    "\n",
    "            self.cluster_stats[j][0] = len(p_class_index)\n",
    "            self.cluster_stats[j][1] = len(n_class_index)\n",
    "\n",
    "            n_class = cluster_pts[n_class_index]\n",
    "            p_class = cluster_pts[p_class_index]\n",
    "\n",
    "            self.negative_centers[j,:] = n_class.mean(axis=0)\n",
    "            self.positive_centers[j,:] = p_class.mean(axis=0)\n",
    "\n",
    "            self.negative_sse[j] = np.square(np.linalg.norm(n_class - self.negative_centers[j]))\n",
    "            self.positive_sse[j] = np.square(np.linalg.norm(p_class - self.positive_centers[j]))\n",
    "\n",
    "        # self.clusters = np.random.rand(self.n_clusters, self.latent_dim)  # copy clusters\n",
    "\n",
    "    def update_assign(self, X, target=None):\n",
    "        \"\"\" Assign samples in `X` to clusters \"\"\"\n",
    "        return self.predict_clusters(X, self.clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "def get_dataset(DATASET, base_dir):\n",
    "    if DATASET == \"cic\":\n",
    "        Xa = pd.read_csv(base_dir + \"/CIC/cic_set_a.csv\")\n",
    "        Xb = pd.read_csv(base_dir + \"/CIC/cic_set_b.csv\")\n",
    "        Xc = pd.read_csv(base_dir + \"/CIC/cic_set_c.csv\")\n",
    "\n",
    "        ya = Xa['In-hospital_death']\n",
    "        yb = Xb['In-hospital_death']\n",
    "        yc = Xc['In-hospital_death']\n",
    "\n",
    "        Xa = Xa.drop(columns=['recordid', 'Survival', 'In-hospital_death'])\n",
    "        Xb = Xb.drop(columns=['recordid', 'Survival', 'In-hospital_death'])\n",
    "        Xc = Xc.drop(columns=['recordid', 'Survival', 'In-hospital_death'])\n",
    "\n",
    "        cols = Xa.columns\n",
    "\n",
    "        scale = StandardScaler()\n",
    "        Xa = scale.fit_transform(Xa)\n",
    "        Xb = scale.fit_transform(Xb)\n",
    "        Xc = scale.fit_transform(Xc)\n",
    "\n",
    "        Xa = pd.DataFrame(Xa, columns=cols)\n",
    "        Xb = pd.DataFrame(Xb, columns=cols)\n",
    "        Xc = pd.DataFrame(Xc, columns=cols)\n",
    "\n",
    "        Xa = Xa.fillna(0)\n",
    "        Xb = Xb.fillna(0)\n",
    "        Xc = Xc.fillna(0)\n",
    "\n",
    "        X_train = pd.concat([Xa, Xb])\n",
    "        y_train = pd.concat([ya, yb])\n",
    "\n",
    "        X_test = Xc\n",
    "        y_test = yc\n",
    "\n",
    "        X = pd.concat([X_train, X_test]).to_numpy()\n",
    "        y = pd.concat([y_train, y_test]).to_numpy()\n",
    "\n",
    "    elif DATASET == \"titanic\":\n",
    "        X_train = pd.read_csv(base_dir + \"/\" + DATASET + \"/\" + \"X_train.csv\").to_numpy()\n",
    "        X_test = pd.read_csv(base_dir + \"/\" + DATASET + \"/\" + \"X_test.csv\").to_numpy()\n",
    "        y_train = pd.read_csv(base_dir + \"/\" + DATASET + \"/\" + \"y_train.csv\").to_numpy()\n",
    "        y_test = pd.read_csv(base_dir + \"/\" + DATASET + \"/\" + \"y_test.csv\").to_numpy()\n",
    "\n",
    "        X = np.vstack([X_train, X_test])\n",
    "        y = np.vstack([y_train, y_test])\n",
    "        y1 = []\n",
    "        for i in range(len(y)):\n",
    "            y1.append(y[i][0])\n",
    "        y = np.array(y1)\n",
    "        # X = pd.concat([X_train, X_test]).to_numpy()\n",
    "        # y = pd.concat([y_train, y_test]).to_numpy()\n",
    "    \n",
    "    elif DATASET == \"infant\":\n",
    "        X = pd.read_csv(base_dir + \"/\" + DATASET + \"/\" + \"X.csv\").to_numpy()\n",
    "        y = pd.read_csv(base_dir + \"/\" + DATASET + \"/\" + \"y.csv\").to_numpy()\n",
    "        y1 = []\n",
    "        \n",
    "        for i in range(len(y)):\n",
    "            y1.append(y[i][0])\n",
    "        y = np.array(y1)\n",
    "        y = y.astype(int)\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        X = enc.fit_transform(X).toarray()\n",
    "    \n",
    "    elif DATASET == \"kidney\":\n",
    "        print(\"Fetching Kidney Dataset\")\n",
    "        data = get_aki(base_dir)\n",
    "        X = pd.concat(data,axis=1).T\n",
    "        columns = X.columns\n",
    "\n",
    "        data_columns = list(columns[1:90]) + ['y'] # get the columns which have data, not mask\n",
    "        non_binary_columns = data_columns[:81] # only these columns have non-binary data fit for scaling\n",
    "\n",
    "        X = X.fillna(0)\n",
    "        X = X[data_columns]\n",
    "\n",
    "        y = X['y'].to_numpy().astype(int)\n",
    "        X = X.drop(columns=['y'])\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X[non_binary_columns] = scaler.fit_transform(np.nan_to_num(X[non_binary_columns]))\n",
    "        X = X.to_numpy()\n",
    "\n",
    "    else:\n",
    "        X = pd.read_csv(base_dir + \"/\" + DATASET + \"/\" + \"X.csv\").to_numpy()\n",
    "        y = pd.read_csv(base_dir + \"/\" + DATASET + \"/\" + \"y.csv\").to_numpy()\n",
    "        y1 = []\n",
    "        for i in range(len(y)):\n",
    "            y1.append(y[i][0])\n",
    "        y = np.array(y1)\n",
    "    return X, y\n",
    "\n",
    "def paper_synthetic(n_pts=1000, centers=4):\n",
    "    X, y = make_blobs(n_pts, centers=centers)\n",
    "    W = np.random.randn(10,2)\n",
    "    U = np.random.randn(100,10)\n",
    "    X1 = W.dot(X.T)\n",
    "    X1 = X1*(X1>0)\n",
    "    X2 = U.dot(X1)\n",
    "    X2 = X2*(X2>0)\n",
    "    return X2.T, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "code_folding": [
     147,
     186
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numbers\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "# from cac import batch_cac\n",
    "import torch.nn.functional as F\n",
    "# from kmeans import batch_KMeans\n",
    "from autoencoder import AutoEncoder\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import davies_bouldin_score as dbs, adjusted_rand_score as ari\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, f1_score, roc_auc_score, roc_curve, matthews_corrcoef as mcc\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, RidgeClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "color = ['grey', 'red', 'blue', 'pink', 'brown', 'black', 'magenta', 'purple', 'orange', 'cyan', 'olive']\n",
    "\n",
    "class DCN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(DCN, self).__init__()\n",
    "        self.args = args\n",
    "        self.beta = args.beta  # coefficient of the clustering term \n",
    "        self.lamda = args.lamda  # coefficient of the reconstruction term\n",
    "        self.device = torch.device(args.device)\n",
    "        \n",
    "        # Validation check\n",
    "        if not self.beta > 0:\n",
    "            msg = 'beta should be greater than 0 but got value = {}.'\n",
    "            raise ValueError(msg.format(self.beta))\n",
    "        \n",
    "        if not self.lamda > 0:\n",
    "            msg = 'lamda should be greater than 0 but got value = {}.'\n",
    "            raise ValueError(msg.format(self.lamda))\n",
    "        \n",
    "        if len(self.args.hidden_dims) == 0:\n",
    "            raise ValueError('No hidden layer specified.')\n",
    "        \n",
    "        if args.clustering == 'kmeans':\n",
    "            self.clustering = batch_KMeans(args)\n",
    "        elif args.clustering == 'meanshift':\n",
    "            self.clustering = batch_MeanShift(args)\n",
    "        elif args.clustering == \"cac\":\n",
    "            self.clustering = batch_cac(args)\n",
    "            self.classifier = args.classifier\n",
    "            self.cluster_classifiers = []\n",
    "            self.base_classifier = []\n",
    "        else:\n",
    "            raise RuntimeError('Error: no clustering chosen')\n",
    "            \n",
    "        self.autoencoder = AutoEncoder(args).to(self.device)\n",
    "        self.criterion  = nn.MSELoss(reduction='mean')\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                          lr=args.lr,\n",
    "                                          weight_decay=args.wd)\n",
    "\n",
    "\n",
    "    \"\"\" Compute the Equation (5) in the original paper on a data batch \"\"\"\n",
    "    def _loss(self, X, y, cluster_id):\n",
    "        batch_size = X.size()[0]\n",
    "        rec_X, y_pred = self.autoencoder(X)\n",
    "        latent_X = self.autoencoder(X, latent=True)\n",
    "        \n",
    "        # Reconstruction error\n",
    "        rec_loss = self.lamda * self.criterion(X, rec_X)\n",
    "\n",
    "        # Regularization term on clustering\n",
    "        km_loss = torch.tensor(0.).to(self.device)\n",
    "        sep_loss = torch.tensor(0.).to(self.device)\n",
    "        classification_loss = torch.tensor(0.).to(self.device)\n",
    "\n",
    "#         kmeans = KMeans(n_clusters=args.n_clusters, n_init=20)\n",
    "#         kmeans.fit(latent_X.cpu().numpy())\n",
    "#         clusters = torch.tensor(kmeans.cluster_centers_).to(device)\n",
    "#         clusters = torch.zeros((args.n_clusters, args.latent_dim)).to(self.device)\n",
    "#         positive_clusters = torch.zeros((args.n_clusters, args.latent_dim)).to(self.device)\n",
    "#         negative_clusters = torch.zeros((args.n_clusters, args.latent_dim)).to(self.device)\n",
    "#         cluster_stats = np.zeros((args.n_clusters,2))\n",
    "\n",
    "#         for j in range(self.args.n_clusters):\n",
    "#             pts_index = np.where(cluster_id == j)[0]\n",
    "#             cluster_pts = latent_X[pts_index]\n",
    "#             n_class_index = np.where(y[pts_index] == 0)[0]\n",
    "#             p_class_index = np.where(y[pts_index] == 1)[0]\n",
    "\n",
    "#             n_class = cluster_pts[n_class_index]\n",
    "#             p_class = cluster_pts[p_class_index]\n",
    "\n",
    "#             negative_clusters[j,:] = n_class.mean(axis=0)\n",
    "#             positive_clusters[j,:] = p_class.mean(axis=0)\n",
    "#             clusters[j,:] = cluster_pts.mean(axis=0)\n",
    "#             cluster_stats[j][0] = len(p_class_index)\n",
    "#             cluster_stats[j][1] = len(n_class_index)\n",
    "\n",
    "        clusters = torch.FloatTensor(self.clustering.clusters).to(self.device)\n",
    "        positive_clusters = torch.FloatTensor(self.clustering.positive_centers).to(self.device)\n",
    "        negative_clusters = torch.FloatTensor(self.clustering.negative_centers).to(self.device)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            diff_vec = latent_X[i] - clusters[cluster_id[i]]\n",
    "            sample_dist_loss = torch.matmul(diff_vec.view(1, -1),\n",
    "                                            diff_vec.view(-1, 1))\n",
    "            km_loss += 0.5 * self.beta * torch.squeeze(sample_dist_loss)\n",
    "            \n",
    "#             if self.args.clustering == \"cac\":\n",
    "#                 diff_vec = positive_clusters[cluster_id[i]] - negative_clusters[cluster_id[i]]\n",
    "            if y[i] == 0:\n",
    "                diff_vec = latent_X[i] - negative_clusters[cluster_id[i]]\n",
    "            else:\n",
    "                diff_vec = latent_X[i] - positive_clusters[cluster_id[i]]\n",
    "\n",
    "            sample_sep_loss = torch.matmul(diff_vec.view(1, -1),\n",
    "                                            diff_vec.view(-1, 1))\n",
    "            sep_loss += self.args.alpha * torch.squeeze(sample_sep_loss)\n",
    "#             sep_loss += torch.exp(self.args.alpha * torch.squeeze(sample_sep_loss))\n",
    "\n",
    "        # Individual classification loss for clusters\n",
    "#         for k in range(self.args.n_clusters):\n",
    "#             idx = np.where(cluster_id == k)[0]\n",
    "#             X_idx = X[idx]\n",
    "#             _, y_pred_idx = self.autoencoder(X_idx, latent=False, classifier_idx=k)\n",
    "#             classification_loss += self.args.gamma*nn.CrossEntropyLoss(reduction='mean')(y_pred_idx, y[idx])\n",
    "\n",
    "        \"\"\"\n",
    "        # Just for printing\n",
    "        for j in range(self.args.n_clusters):\n",
    "            diff_vec = positive_clusters[j] - negative_clusters[j]\n",
    "            sample_sep_loss = torch.matmul(diff_vec.view(1, -1),\n",
    "                                            diff_vec.view(-1, 1))\n",
    "            sample_sep_loss = torch.squeeze(sample_sep_loss)\n",
    "            print(\"Log Class Dist: \", np.log(sample_sep_loss))\n",
    "\n",
    "        print(\"Rec Loss: \", rec_loss)\n",
    "        print(\"KM Dist: \", km_loss)\n",
    "        print(\"Sep Dist: \", sep_loss)\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "        return (rec_loss + km_loss + sep_loss,\n",
    "                rec_loss,\n",
    "                sep_loss,\n",
    "                classification_loss)\n",
    "    \n",
    "    def pretrain(self, train_loader, epoch=100, verbose=True):\n",
    "        if not self.args.pretrain:\n",
    "            return\n",
    "        \n",
    "        if not isinstance(epoch, numbers.Integral):\n",
    "            msg = '`epoch` should be an integer but got value = {}'\n",
    "            raise ValueError(msg.format(epoch))\n",
    "        \n",
    "        if verbose:\n",
    "            print('========== Start pretraining ==========')\n",
    "        \n",
    "        rec_loss_list = []\n",
    "        \n",
    "        self.train()\n",
    "        for e in range(epoch):\n",
    "            for batch_idx, (data, _) in enumerate(train_loader):\n",
    "                batch_size = data.size()[0]\n",
    "                data = data.to(self.device).view(batch_size, -1)\n",
    "                rec_X, _ = self.autoencoder(data)\n",
    "                loss = self.criterion(data, rec_X)\n",
    "                if verbose and (batch_idx+1) % self.args.log_interval == 0:\n",
    "                    msg = 'Epoch: {:02d} | Batch: {:03d} | Rec-Loss: {:.3f}'\n",
    "                    print(msg.format(e, batch_idx+1, \n",
    "                                     loss.detach().cpu().numpy()))\n",
    "                    rec_loss_list.append(loss.detach().cpu().numpy())\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        self.eval()\n",
    "        \n",
    "        if verbose:\n",
    "            print('========== End pretraining ==========\\n')\n",
    "\n",
    "        self.pre_cluster(train_loader)\n",
    "                \n",
    "        return rec_loss_list\n",
    "\n",
    "\n",
    "    def pre_cluster(self, train_loader):\n",
    "        # Initialize clusters in self.clustering after pre-training\n",
    "        batch_X = []\n",
    "        batch_y = []\n",
    "        for batch_idx, (data, y) in enumerate(train_loader):\n",
    "            batch_size = data.size()[0]\n",
    "            data = data.to(self.device).view(batch_size, -1)\n",
    "            latent_X = self.autoencoder(data, latent=True)\n",
    "            batch_X.append(latent_X.detach().cpu().numpy())\n",
    "            batch_y.extend(y.detach().cpu().numpy())\n",
    "\n",
    "        X = np.vstack(batch_X)\n",
    "        y = np.array(batch_y)\n",
    "\n",
    "        self.clustering.init_cluster(X, y)\n",
    "        return None\n",
    "\n",
    "\n",
    "    def fit(self, epoch, train_loader, verbose=True):\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        cluster_ids_train = []\n",
    "        total_loss = torch.tensor(0.).to(self.device)\n",
    "        total_cac_loss = torch.tensor(0.).to(self.device)\n",
    "        total_rec_loss = torch.tensor(0.).to(self.device)\n",
    "        total_classification_loss = torch.tensor(0.).to(self.device)\n",
    "        \n",
    "        for batch_idx, (data, y) in enumerate(train_loader):\n",
    "            batch_size = data.size()[0]\n",
    "            data = data.view(batch_size, -1).to(self.device)\n",
    "\n",
    "            # Collect training data and labels for the later classifier\n",
    "            X_train.append(data.cpu().numpy())\n",
    "            y_train.extend(y.numpy())\n",
    "\n",
    "            # Get the latent features\n",
    "            with torch.no_grad():\n",
    "                latent_X = self.autoencoder(data, latent=True)\n",
    "                latent_X = latent_X.cpu().numpy()\n",
    "\n",
    "            if self.args.clustering == \"cac\":\n",
    "                cluster_id = self.clustering.cluster(latent_X, y, self.args.beta, self.args.alpha)\n",
    "\n",
    "            else:\n",
    "                # [Step-1] Update the assignment results\n",
    "                cluster_id = self.clustering.update_assign(latent_X, y)\n",
    "\n",
    "                # [Step-2] Update cluster centers in batch Clustering\n",
    "                elem_count = np.bincount(cluster_id,\n",
    "                                         minlength=self.args.n_clusters)\n",
    "\n",
    "                for k in range(self.args.n_clusters):\n",
    "                    # avoid empty slicing\n",
    "                    if elem_count[k] == 0:\n",
    "                        continue\n",
    "                    # updating the cluster center\n",
    "                    self.clustering.update_cluster(latent_X[cluster_id == k], y, k)\n",
    "\n",
    "            # [Step-3] Update the network parameters\n",
    "            loss, rec_loss, dist_loss, classification_loss = self._loss(data, y, cluster_id)\n",
    "#             print(loss)\n",
    "            total_loss += loss\n",
    "            total_cac_loss += dist_loss\n",
    "            total_rec_loss += rec_loss\n",
    "            total_classification_loss += classification_loss\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Propagate error through the reconstruction head\n",
    "            loss.backward()\n",
    "\n",
    "            # Propagate error through the classification head\n",
    "#             dist_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if verbose and (batch_idx+1) % self.args.log_interval == 0:\n",
    "                msg = 'Epoch: {:02d} | Batch: {:03d} | Loss: {:.3f} | Rec-' \\\n",
    "                      'Loss: {:.3f} | Dist-Loss: {:.3f} | Classification-Loss: {:.3f}'\n",
    "                print(msg.format(epoch, batch_idx+1, \n",
    "                                 loss.detach().cpu().numpy(),\n",
    "                                 rec_loss, dist_loss, classification_loss))\n",
    "\n",
    "        msg = 'Epoch: {:02d} Loss: {:.3f} | Rec-' \\\n",
    "              'Loss: {:.3f} | Dist-Loss: {:.3f} | Classification-Loss: {:.3f}'\n",
    "        print(msg.format(epoch, \n",
    "                         total_loss.detach().cpu().numpy(),\n",
    "                         total_rec_loss, total_cac_loss, total_classification_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "code_folding": [
     0,
     53,
     78
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    y_classifier_pred = []\n",
    "    y_classifier_pred_proba = []\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        batch_size = data.size()[0]\n",
    "        X_test.append(data)\n",
    "        data = data.view(batch_size, -1).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            latent_X = model.autoencoder(data, latent=True)\n",
    "            latent_X = latent_X.detach().to(model.device).numpy()\n",
    "\n",
    "        y_test.append(target.view(-1, 1).numpy())\n",
    "    \n",
    "    X_test = torch.vstack(X_test)\n",
    "    latent_X = model.autoencoder(X_test, latent=True)\n",
    "    X_test = latent_X.detach().to(model.device).numpy()\n",
    "    y_test = np.vstack(y_test).reshape(-1)\n",
    "    y_pred = model.clustering.update_assign(X_test).reshape(-1)\n",
    "    nmi, ari = normalized_mutual_info_score(y_test, y_pred), adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "    if model.clustering == \"cac\":\n",
    "        base_f1 = f1_score(y_test, model.base_classifier[-1].predict(X_test))\n",
    "        base_mcc = mcc(y_test, model.base_classifier[-1].predict(X_test))\n",
    "        base_auc = roc_auc_score(y_test, model.base_classifier[-1].predict_proba(X_test)[:,1])\n",
    "\n",
    "        X_cluster_test = []\n",
    "        y_cluster_test = []\n",
    "\n",
    "        for j in range(model.args.n_clusters):\n",
    "            cluster_index = np.where(y_pred == j)[0]\n",
    "            X_cluster = X_test[cluster_index]\n",
    "            y_cluster = y_test[cluster_index]\n",
    "\n",
    "            X_cluster_test.append(X_cluster)\n",
    "            y_cluster_test.extend(y_cluster)\n",
    "\n",
    "            # Select the cluster classifiers appearing in the latest iteration\n",
    "            y_classifier_pred.extend(model.cluster_classifiers[-1][j].predict(X_cluster))\n",
    "            y_classifier_pred_proba.extend(model.cluster_classifiers[-1][j].predict_proba(X_cluster)[:,1])\n",
    "\n",
    "        cac_f1 = f1_score(y_cluster_test, y_classifier_pred)\n",
    "        cac_mcc = mcc(y_cluster_test, y_classifier_pred)\n",
    "        cac_auc = roc_auc_score(y_test, y_classifier_pred_proba)\n",
    "\n",
    "        return (nmi, ari, base_f1, base_mcc, base_auc, cac_f1, cac_mcc, cac_auc)\n",
    "    else:\n",
    "        return (nmi, ari)\n",
    "    \n",
    "def solver(args, model, X_train, train_loader, test_loader):\n",
    "    rec_loss_list = model.pretrain(train_loader, epoch=args.pre_epoch)\n",
    "    nmi_list = []\n",
    "    ari_list = []\n",
    "\n",
    "    for e in range(args.epoch):\n",
    "        model.train()\n",
    "        model.fit(e, train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        out = evaluate(model, test_loader)  # evaluation on the test_loader\n",
    "        if len(out) > 2:\n",
    "            NMI, ARI, base_f1, base_mcc, base_auc, cac_f1, cac_mcc, cac_auc = out\n",
    "            print('Epoch: {:02d} | NMI: {:.3f} | ARI: {:.3f} | Base_F1: {:.3f} | Base_MCC: {:.3f} | Base_AUC: {:.3f} | CAC_F1: {:.3f} | CAC_MCC: {:.3f} | CAC_AUC: {:.3f}'.format(\n",
    "                e+1, NMI, ARI, base_f1, base_mcc, base_auc, cac_f1, cac_mcc, cac_auc))\n",
    "        else:\n",
    "            NMI, ARI = out\n",
    "            print('Epoch: {:02d} | NMI: {:.3f} | ARI: {:.3f}'.format(e+1, NMI, ARI))\n",
    "\n",
    "        nmi_list.append(NMI)\n",
    "        ari_list.append(ARI)\n",
    "        \n",
    "        \n",
    "    return rec_loss_list, nmi_list, ari_list\n",
    "\n",
    "def create_imbalanced_data_clusters(n_samples=1000, n_features=8, n_informative=5, n_classes=2,\\\n",
    "                            n_clusters = 2, frac=0.4, outer_class_sep=0.5, inner_class_sep=0.2, clus_per_class=2, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    X = np.empty(shape=n_features)\n",
    "    Y = np.empty(shape=1)\n",
    "    offsets = np.random.normal(0, outer_class_sep, size=(n_clusters, n_features))\n",
    "    for i in range(n_clusters):\n",
    "        samples = int(np.random.normal(n_samples, n_samples/10))\n",
    "        x, y = make_classification(n_samples=samples, n_features=n_features, n_informative=n_informative,\\\n",
    "                                    n_classes=n_classes, class_sep=inner_class_sep, n_clusters_per_class=clus_per_class)\n",
    "                                    # n_repeated=0, n_redundant=0)\n",
    "        x += offsets[i]\n",
    "        y_0 = np.where(y == 0)[0]\n",
    "        y_1 = np.where(y != 0)[0]\n",
    "        y_1 = np.random.choice(y_1, int(np.random.normal(frac, frac/4)*len(y_1)))\n",
    "        index = np.hstack([y_0,y_1])\n",
    "        np.random.shuffle(index)\n",
    "        x_new = x[index]\n",
    "        y_new = y[index]\n",
    "\n",
    "        X = np.vstack((X,x_new))\n",
    "        Y = np.hstack((Y,y_new))\n",
    "\n",
    "    X = pd.DataFrame(X[1:,:])\n",
    "    Y = Y[1:]\n",
    "    return X, np.array(Y).astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "params = {\n",
    "'dir' : 'synthetic',\n",
    "'dataset' : 'cic',\n",
    "\n",
    "# Training parameters\n",
    "'lr' : 0.002,\n",
    "'alpha' : 1,\n",
    "'wd' : 5e-4,\n",
    "'batch_size' : 256,\n",
    "'epoch' : 21,\n",
    "'pre_epoch' : 30,\n",
    "'pretrain' : True,\n",
    "\"load_ae\": False,\n",
    "\"classifier\": \"LR\",\n",
    "\n",
    "# Model parameters\n",
    "'lamda' : 1,\n",
    "'beta' : 1,\n",
    "'gamma' : 2,\n",
    "'hidden_dims' : [64, 32],\n",
    "'latent_dim' : 20,\n",
    "'n_clusters' : 3,\n",
    "'clustering' : 'kmeans',\n",
    "'n_classes'  : 2,\n",
    "\n",
    "# Utility parameters\n",
    "'n_jobs' : 6,\n",
    "'device' : 'cpu',\n",
    "'log_interval' : 10}\n",
    "\n",
    "class parameters(object):\n",
    "    def __init__(self, params):\n",
    "        self.dir = params['dir']\n",
    "        self.input_dim = -1\n",
    "        self.dataset = params['dataset']\n",
    "        \n",
    "        # Training parameters\n",
    "        self.lr = params['lr']\n",
    "        self.alpha = params['alpha']\n",
    "        self.wd = params['wd']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.epoch = params['epoch']\n",
    "        self.pre_epoch = params['pre_epoch']\n",
    "        self.pretrain = params['pretrain']\n",
    "        self.load_ae = params['load_ae']\n",
    "        self.classifier = params['classifier']\n",
    "\n",
    "        # Model parameters\n",
    "        self.lamda = params['lamda']\n",
    "        self.beta = params['beta']\n",
    "        self.gamma = params['gamma']\n",
    "        self.hidden_dims = params['hidden_dims']\n",
    "        self.latent_dim = params['latent_dim']\n",
    "        self.n_clusters = params['n_clusters']\n",
    "        self.clustering = params['clustering']\n",
    "        self.n_classes = params['n_classes']\n",
    "\n",
    "        # Utility parameters\n",
    "        self.n_jobs = params['n_jobs']\n",
    "        self.device = params['device']\n",
    "        self.log_interval = params['log_interval']\n",
    "\n",
    "args = parameters(params)\n",
    "datasets = ['titanic', 'magic', 'creditcard', 'adult', 'diabetes',\\\n",
    "            'cic', 'sepsis', 'synthetic', 'paper_synthetic', 'kidney', 'infant', 'wid_mortality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset: cic\n"
     ]
    }
   ],
   "source": [
    "if args.dataset in datasets:\n",
    "    base_dir = \"/Users/shivin/Document/NUS/Research/Data\"\n",
    "    print(\"Loading Dataset:\", args.dataset)\n",
    "    if args.dataset != \"kidney\":\n",
    "        if args.dataset == \"synthetic\":\n",
    "            n_feat = 45\n",
    "            X, y = create_imbalanced_data_clusters(n_samples=5000,\\\n",
    "                   n_clusters=args.n_clusters, n_features = n_feat,\\\n",
    "                   inner_class_sep=0.2, outer_class_sep=2, seed=0)\n",
    "            args.input_dim = n_feat\n",
    "\n",
    "        elif args.dataset == \"paper_synthetic\":\n",
    "            n_feat = 100\n",
    "            X, y = paper_synthetic(2500, centers=4)\n",
    "            args.input_dim = n_feat\n",
    "            print(args.input_dim)\n",
    "\n",
    "        else:\n",
    "            X, y = get_dataset(args.dataset, base_dir)\n",
    "            args.input_dim = X.shape[1]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "        args.input_dim = X_train.shape[1]\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.fit_transform(X_test)\n",
    "        X_train_data_loader = list(zip(X_train.astype(np.float32), y_train))\n",
    "        X_test_data_loader  = list(zip(X_test.astype(np.float32), y_test))\n",
    "\n",
    "    else:\n",
    "        print(\"Loading Kidney Train\")\n",
    "#         X_train, y_train = get_dataset(args.dataset, \"/Users/shivin/Document/NUS/Research/Data/aki/train\")\n",
    "        args.input_dim = X_train.shape[1]\n",
    "        print(args.input_dim)\n",
    "\n",
    "        print(\"Loading Kidney Test\")\n",
    "#         X_test, y_test = get_dataset(args.dataset, \"/Users/shivin/Document/NUS/Research/Data/aki/test\")\n",
    "\n",
    "        X_train_data_loader = list(zip(X_train.astype(np.float32), y_train))\n",
    "        X_test_data_loader  = list(zip(X_test.astype(np.float32), y_test))\n",
    "\n",
    "        \n",
    "    train_loader = torch.utils.data.DataLoader(X_train_data_loader,\n",
    "        batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(X_test_data_loader, \n",
    "        batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_kidney' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-e364be780ffe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# X_test_kidney = X_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# y_test_kidney = y_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_kidney\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train_kidney\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_kidney\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_kidney' is not defined"
     ]
    }
   ],
   "source": [
    "# X_train_kidney = X_train\n",
    "# y_train_kidney = y_train\n",
    "# X_test_kidney = X_test\n",
    "# y_test_kidney = y_test\n",
    "X_train = X_train_kidney\n",
    "y_train = y_train_kidney\n",
    "X_test = X_test_kidney\n",
    "y_test = y_test_kidney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Start pretraining ==========\n",
      "Epoch: 00 | Batch: 010 | Rec-Loss: 1.159\n",
      "Epoch: 00 | Batch: 020 | Rec-Loss: 0.887\n",
      "Epoch: 00 | Batch: 030 | Rec-Loss: 0.846\n",
      "Epoch: 01 | Batch: 010 | Rec-Loss: 0.713\n",
      "Epoch: 01 | Batch: 020 | Rec-Loss: 0.708\n",
      "Epoch: 01 | Batch: 030 | Rec-Loss: 0.805\n",
      "Epoch: 02 | Batch: 010 | Rec-Loss: 0.641\n",
      "Epoch: 02 | Batch: 020 | Rec-Loss: 0.752\n",
      "Epoch: 02 | Batch: 030 | Rec-Loss: 0.637\n",
      "Epoch: 03 | Batch: 010 | Rec-Loss: 0.597\n",
      "Epoch: 03 | Batch: 020 | Rec-Loss: 0.601\n",
      "Epoch: 03 | Batch: 030 | Rec-Loss: 0.571\n",
      "Epoch: 04 | Batch: 010 | Rec-Loss: 0.545\n",
      "Epoch: 04 | Batch: 020 | Rec-Loss: 0.563\n",
      "Epoch: 04 | Batch: 030 | Rec-Loss: 0.548\n",
      "Epoch: 05 | Batch: 010 | Rec-Loss: 0.542\n",
      "Epoch: 05 | Batch: 020 | Rec-Loss: 0.612\n",
      "Epoch: 05 | Batch: 030 | Rec-Loss: 0.517\n",
      "Epoch: 06 | Batch: 010 | Rec-Loss: 0.512\n",
      "Epoch: 06 | Batch: 020 | Rec-Loss: 0.637\n",
      "Epoch: 06 | Batch: 030 | Rec-Loss: 0.481\n",
      "Epoch: 07 | Batch: 010 | Rec-Loss: 0.491\n",
      "Epoch: 07 | Batch: 020 | Rec-Loss: 0.598\n",
      "Epoch: 07 | Batch: 030 | Rec-Loss: 0.451\n",
      "Epoch: 08 | Batch: 010 | Rec-Loss: 0.465\n",
      "Epoch: 08 | Batch: 020 | Rec-Loss: 0.470\n",
      "Epoch: 08 | Batch: 030 | Rec-Loss: 0.475\n",
      "Epoch: 09 | Batch: 010 | Rec-Loss: 0.480\n",
      "Epoch: 09 | Batch: 020 | Rec-Loss: 0.442\n",
      "Epoch: 09 | Batch: 030 | Rec-Loss: 0.472\n",
      "Epoch: 10 | Batch: 010 | Rec-Loss: 0.510\n",
      "Epoch: 10 | Batch: 020 | Rec-Loss: 0.460\n",
      "Epoch: 10 | Batch: 030 | Rec-Loss: 0.428\n",
      "Epoch: 11 | Batch: 010 | Rec-Loss: 0.456\n",
      "Epoch: 11 | Batch: 020 | Rec-Loss: 0.401\n",
      "Epoch: 11 | Batch: 030 | Rec-Loss: 0.404\n",
      "Epoch: 12 | Batch: 010 | Rec-Loss: 0.428\n",
      "Epoch: 12 | Batch: 020 | Rec-Loss: 0.412\n",
      "Epoch: 12 | Batch: 030 | Rec-Loss: 0.430\n",
      "Epoch: 13 | Batch: 010 | Rec-Loss: 0.413\n",
      "Epoch: 13 | Batch: 020 | Rec-Loss: 0.372\n",
      "Epoch: 13 | Batch: 030 | Rec-Loss: 0.397\n",
      "Epoch: 14 | Batch: 010 | Rec-Loss: 0.386\n",
      "Epoch: 14 | Batch: 020 | Rec-Loss: 0.412\n",
      "Epoch: 14 | Batch: 030 | Rec-Loss: 0.506\n",
      "Epoch: 15 | Batch: 010 | Rec-Loss: 0.391\n",
      "Epoch: 15 | Batch: 020 | Rec-Loss: 0.410\n",
      "Epoch: 15 | Batch: 030 | Rec-Loss: 0.402\n",
      "Epoch: 16 | Batch: 010 | Rec-Loss: 0.398\n",
      "Epoch: 16 | Batch: 020 | Rec-Loss: 0.366\n",
      "Epoch: 16 | Batch: 030 | Rec-Loss: 0.441\n",
      "Epoch: 17 | Batch: 010 | Rec-Loss: 0.477\n",
      "Epoch: 17 | Batch: 020 | Rec-Loss: 0.372\n",
      "Epoch: 17 | Batch: 030 | Rec-Loss: 0.370\n",
      "Epoch: 18 | Batch: 010 | Rec-Loss: 0.447\n",
      "Epoch: 18 | Batch: 020 | Rec-Loss: 0.383\n",
      "Epoch: 18 | Batch: 030 | Rec-Loss: 0.346\n",
      "Epoch: 19 | Batch: 010 | Rec-Loss: 0.349\n",
      "Epoch: 19 | Batch: 020 | Rec-Loss: 0.339\n",
      "Epoch: 19 | Batch: 030 | Rec-Loss: 0.386\n",
      "Epoch: 20 | Batch: 010 | Rec-Loss: 0.335\n",
      "Epoch: 20 | Batch: 020 | Rec-Loss: 0.373\n",
      "Epoch: 20 | Batch: 030 | Rec-Loss: 0.373\n",
      "Epoch: 21 | Batch: 010 | Rec-Loss: 0.355\n",
      "Epoch: 21 | Batch: 020 | Rec-Loss: 0.371\n",
      "Epoch: 21 | Batch: 030 | Rec-Loss: 0.384\n",
      "Epoch: 22 | Batch: 010 | Rec-Loss: 0.360\n",
      "Epoch: 22 | Batch: 020 | Rec-Loss: 0.326\n",
      "Epoch: 22 | Batch: 030 | Rec-Loss: 0.349\n",
      "Epoch: 23 | Batch: 010 | Rec-Loss: 0.402\n",
      "Epoch: 23 | Batch: 020 | Rec-Loss: 0.329\n",
      "Epoch: 23 | Batch: 030 | Rec-Loss: 0.359\n",
      "Epoch: 24 | Batch: 010 | Rec-Loss: 0.346\n",
      "Epoch: 24 | Batch: 020 | Rec-Loss: 0.462\n",
      "Epoch: 24 | Batch: 030 | Rec-Loss: 0.330\n",
      "Epoch: 25 | Batch: 010 | Rec-Loss: 0.363\n",
      "Epoch: 25 | Batch: 020 | Rec-Loss: 0.342\n",
      "Epoch: 25 | Batch: 030 | Rec-Loss: 0.379\n",
      "Epoch: 26 | Batch: 010 | Rec-Loss: 0.349\n",
      "Epoch: 26 | Batch: 020 | Rec-Loss: 0.396\n",
      "Epoch: 26 | Batch: 030 | Rec-Loss: 0.333\n",
      "Epoch: 27 | Batch: 010 | Rec-Loss: 0.356\n",
      "Epoch: 27 | Batch: 020 | Rec-Loss: 0.360\n",
      "Epoch: 27 | Batch: 030 | Rec-Loss: 0.354\n",
      "Epoch: 28 | Batch: 010 | Rec-Loss: 0.357\n",
      "Epoch: 28 | Batch: 020 | Rec-Loss: 0.325\n",
      "Epoch: 28 | Batch: 030 | Rec-Loss: 0.336\n",
      "Epoch: 29 | Batch: 010 | Rec-Loss: 0.304\n",
      "Epoch: 29 | Batch: 020 | Rec-Loss: 0.312\n",
      "Epoch: 29 | Batch: 030 | Rec-Loss: 0.437\n",
      "========== End pretraining ==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# args.input_dim = X_train.shape[1]\n",
    "model = DCN(args)\n",
    "rec_loss_list = model.pretrain(train_loader, epoch=args.pre_epoch)\n",
    "pre_trained_AE = copy.deepcopy(model.autoencoder)\n",
    "# model.autoencoder = pre_trained_AE\n",
    "# model.pre_cluster(train_loader)\n",
    "nmi_list = []\n",
    "ari_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Batch: 010 | Loss: 727.149 | Rec-Loss: 1.006 | Dist-Loss: 490.479 | Classification-Loss: 0.000\n",
      "Epoch: 00 | Batch: 020 | Loss: 302.176 | Rec-Loss: 0.890 | Dist-Loss: 206.042 | Classification-Loss: 0.000\n",
      "Epoch: 00 | Batch: 030 | Loss: 221.969 | Rec-Loss: 0.847 | Dist-Loss: 149.926 | Classification-Loss: 0.000\n",
      "Epoch: 00 Loss: 20429.719 | Rec-Loss: 32.023 | Dist-Loss: 13818.359 | Classification-Loss: 0.000\n",
      "Epoch: 01 | NMI: 0.053 | ARI: 0.034\n",
      "Epoch: 01 | NMI: 0.053 | ARI: 0.034\n",
      "Epoch: 01 | Batch: 010 | Loss: 151.323 | Rec-Loss: 1.007 | Dist-Loss: 100.899 | Classification-Loss: 0.000\n",
      "Epoch: 01 | Batch: 020 | Loss: 122.424 | Rec-Loss: 0.846 | Dist-Loss: 82.911 | Classification-Loss: 0.000\n",
      "Epoch: 01 | Batch: 030 | Loss: 95.591 | Rec-Loss: 0.910 | Dist-Loss: 64.215 | Classification-Loss: 0.000\n",
      "Epoch: 01 Loss: 4238.847 | Rec-Loss: 32.949 | Dist-Loss: 2828.642 | Classification-Loss: 0.000\n",
      "Epoch: 02 | NMI: 0.057 | ARI: 0.030\n",
      "Epoch: 02 | NMI: 0.057 | ARI: 0.030\n",
      "Epoch: 02 | Batch: 010 | Loss: 74.312 | Rec-Loss: 0.979 | Dist-Loss: 50.143 | Classification-Loss: 0.000\n",
      "Epoch: 02 | Batch: 020 | Loss: 64.285 | Rec-Loss: 0.842 | Dist-Loss: 43.033 | Classification-Loss: 0.000\n",
      "Epoch: 02 | Batch: 030 | Loss: 51.829 | Rec-Loss: 1.019 | Dist-Loss: 34.418 | Classification-Loss: 0.000\n",
      "Epoch: 02 Loss: 2227.928 | Rec-Loss: 33.228 | Dist-Loss: 1495.033 | Classification-Loss: 0.000\n",
      "Epoch: 03 | NMI: 0.064 | ARI: 0.039\n",
      "Epoch: 03 | NMI: 0.064 | ARI: 0.039\n",
      "Epoch: 03 | Batch: 010 | Loss: 51.838 | Rec-Loss: 0.959 | Dist-Loss: 34.722 | Classification-Loss: 0.000\n",
      "Epoch: 03 | Batch: 020 | Loss: 50.484 | Rec-Loss: 0.929 | Dist-Loss: 33.982 | Classification-Loss: 0.000\n",
      "Epoch: 03 | Batch: 030 | Loss: 52.008 | Rec-Loss: 0.840 | Dist-Loss: 34.300 | Classification-Loss: 0.000\n",
      "Epoch: 03 Loss: 1580.904 | Rec-Loss: 33.332 | Dist-Loss: 1058.242 | Classification-Loss: 0.000\n",
      "Epoch: 04 | NMI: 0.066 | ARI: 0.040\n",
      "Epoch: 04 | NMI: 0.066 | ARI: 0.040\n",
      "Epoch: 04 | Batch: 010 | Loss: 28.868 | Rec-Loss: 0.862 | Dist-Loss: 18.878 | Classification-Loss: 0.000\n",
      "Epoch: 04 | Batch: 020 | Loss: 28.635 | Rec-Loss: 1.015 | Dist-Loss: 19.090 | Classification-Loss: 0.000\n",
      "Epoch: 04 | Batch: 030 | Loss: 23.577 | Rec-Loss: 0.891 | Dist-Loss: 15.936 | Classification-Loss: 0.000\n",
      "Epoch: 04 Loss: 1302.486 | Rec-Loss: 33.066 | Dist-Loss: 868.973 | Classification-Loss: 0.000\n",
      "Epoch: 05 | NMI: 0.062 | ARI: 0.034\n",
      "Epoch: 05 | NMI: 0.062 | ARI: 0.034\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEVCAYAAAD91W7rAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABLJElEQVR4nO3dd3xUVd748c+Zkp6QRmihVynSQcBFsCIq2EVFcdddXV2s6z7r7uPPFcUtrrtiL6iPqNh7BymKgPQmVToSIASSkISUaef3x52ESTKTTLl3Ws779coryeTec7+TnHznzLmnCCkliqIoSuwyRToARVEUJTQqkSuKosQ4lcgVRVFinErkiqIoMU4lckVRlBinErmiKEqMU4lcCZkQ4iEhxJuRjiNWCCHGCSEOhvtcJX6pRK74RQhxnRBijRCiQghxWAjxtRDiTB3L7yKEkEIIi15lGk0IcZMQYmmk41AUlciVZgkh7gVmAX8H2gCdgOeAyREMq55YegFQFL2pRK40SQjRCngY+IOU8iMp5UkppV1K+bmU8k9ejm/01l8IsU8Ica776xHuln2ZEKJQCPFf92FL3J9L3a3+Ue7jfyOE2CaEKBFCzBNCdPYoVwoh/iCE2AnsFJonhBBHhRAnhBCbhBD9vcQ4RQixpsFj9wghPnN/PVEIsVUIUS6EKBBC3BfE7+3X7rjLhRB7hBC3ejnmr0KIY+7fz/UejycKIR4XQhxw/45eEEIk+7jOn90xlgshdgghzgk0ViX2qUSuNGcUkAR8rFN5TwJPSikzgO7Ae+7Hx7o/Z0op06SUPwohLgX+ClwOtAZ+AN5uUN6lwEigL3C+u5xeQCZwDXDcSwyfAb2FED09HrsOeMv99SvArVLKdKA/sCiI53kUuBjIAH4NPCGEGOLx87ZALtABmAa8JITo7f7Zv9zPYRDQw33Mgw0v4D5+OjDcHesFwL4gYlVinErkSnNygGNSSodO5dmBHkKIXCllhZRyRRPH3gr8Q0q5zX39vwODPFvl7p8XSymr3GWnA30A4T7vcMNCpZSVwKfAtQDuhN4HLcHXxthXCJEhpSyRUq4L9ElKKb+UUu6Wmu+B+cCvGhz2/6SUNe6ffwlcLYQQwO+Ae9zPq9z9vKd4uYwTSHTHapVS7pNS7g40ViX2qUSuNOc4kKtjH/TNaK3N7UKI1UKIi5s4tjPwpBCiVAhRChQDAq2FWuuX2i+klIuAZ4BngUIhxEtCiAwfZb+FO5GjtcY/cSd4gCuAicB+IcT3td08gRBCXCiEWCGEKHbHPhGtBV6rREp50uP7/UB7tHceKcBaj+f9jfvxeqSUu4C7gYeAo0KId4QQ7QONVYl9KpErzfkRqEbrwvDHSbREBIAQwoxHEpJS7pRSXgvkoXUhfCCESAW8LcP5C1oXR6bHR7KUcrnHMfXOk1I+JaUcCvRDe8Fo1I/vNh/tBWoQWkKv7VZBSrlaSjnZHeMnnOr+8YsQIhH4EHgcaCOlzAS+QnsRqpXlft61OgGHgGNAFdDP4zm3klKmebuWlPItKeWZaC96Eu13qrQwKpErTZJSnkDrn31WCHGpECJFCGF1tzgf83LKz0CSEOIiIYQVeADt7T8AQoipQojWUkoXUOp+2AkUAS6gm0dZLwB/EUL0c5/bSghxla9YhRDDhRAj3dc9ifYC5PTxvBzAB8C/gWzgW3cZCUKI64UQraSUdqDMVxmnLiuSPD+ABPdzLgIcQogL0frvG5rhvt6v0PrT33f/Xmaj9annuS/QQQhxgZcL9xZCnO1+4ahGewFoKlYlTqlErjRLSvlf4F60pFyE1lKejtZabXjsCeB24GWgAC2heo5imQBsEUJUoN34nCKlrHZ3azwKLHN3KZwhpfwYrYX5jhCiDNgMXNhEqBloSbAEraviOFqr2Je3gHPREqjnPYAbgH3ua/4emNpEGaPREmjDjzvRWvIlaF03nzU474j7Z4eAucDvpZTb3T/7M7ALWOGOYQHQm8YSgX+iteKPoL2D+GsTsSpxSqiNJRRFUWKbapEriqLEOJXIFUVRYpxK5IqiKDFOJXJFUZQYpxK5oihKjFOJXFEUJcapRK4oihLjVCJXFEWJcSqRK4qixDiVyBVFUWKcSuSKoigxTiVyRVGUGKcSuaIoSoxTiVxRFCXGqUSuKIoS41QiVxRFiXEqkSuKosQ4vXZGD0hubq7s0qVLJC6ttABr1649JqVstOt8OKi6rRjJV92OSCLv0qULa9asicSllRZACLE/UtdWdVsxkq+6rbpWFEVRYpxK5IqiKDFOJXJFUZQYpxK5oihKjIvIzc4WYckS+PFH6N4dOnWCIUPAon7dSmxzOBxs3LiRmpoaWrduTevWrcnMzIx0WC2eyiyBkBKefRaWLYNLLoFrrwUh6h+zZw+cdhrYbN7LePdduPpq42NVlABUVFSwePFiXC4Xv/rVr8jOzm50zMKFC1m6dCm4XAiXC2k2AyBcLrqXlTFl5kzMaWnhDl0BhJQy7BcdNmyYjLkhWq+/DtOmNX58/XoYNEj7uqICMjK0hN+U/HzYsAFycvSOUgGEEGullMMice1YrNtPP/00xcXF9R7LzMzkjjvuwGTSel/Xr1/PZ599ptXtho0XACkRdjtjx45l7Hnn1Z2n6MtX3VYt8uZMn661wn0ZPBhuvBF27YLly/0r8+BByM2F+++Hf/xDnzgVJQBVVVW88MILlJWVef15aWkpzz33HLm5uRQUFFBRUaH9wFsSr33cZGLd11/z/YoV3H333bRq1cqg6JWGVIvcG7sd/vpXePzx8FwvJwf++1/tBUEJmWqR+1ZWVsacOXMatcB9aVdQQN+tW5FCsLl/f462bev74Ia5xOUiv2NHLr38cnLUu09d+KrbKpE39M03cOGFkbv+vHlw/vmRu34cUIm8MSklb7/5JpYvvuC0LVs43K4dK0aPRjbRBXLOt98yYtUqLHY7CIHTbObTyZPZMmBAIBcGIUhOTuaee+7BarXq8GxaLl91W3VkeXr33cgmcYALLtCSuaLoRErJi88/T9Hatezq0YNlZ55JcU4OTTXh8o4cYcSqVSTY7ZgAk5RYHQ4u/fhjrDU1mByOumOF0+nzvpDVZqPX9u10XLeOf82Ygcvl0vfJKUBL7iN3ucDhgIQE2LcPRo+Gw4cjHZVmwgRwOkHdMFKCYbeDyYQ0mdi4eDFLPv2U0latkFlZABS2a0dRXl6TRfTZvh2LR7KuZXa5uGvWLBaffTab+/cHIUgrL+d468ZrlPXdvJnJn36Ky12PzU4nyy0WznzoodCfo1JP/Cbyn3+GO++EvXth/34tKZ53HqSkwGefQWVlpCNsWs+esGULJCVFOhIlyqxevZqNGzdSXFxMdXU1iYmJDB4wgBPr1pGyejWtDx/mtC1bOJmaSv/iYr6fPr1uqGAtl9msNWZ8cJlMXlvsAkipquLiL7/knEWL+HTSJHb06dNoNEur0lIu/eQTrA1eDEb8/e+sPvNMhp97bii/AqWB+EvkUkL79nDkSOOfffZZ+OMJ1p492iSirVsjHYkSJQ4ePMgrr7wCTicC6vq3q6uq+HHVKm1s9+DBMGgQ888/n6vfe482RUWUNjFhx+xwaEm7wbu/Lf37M27xYp/nlaem8vztt1OVkuJ1JEv/TZsQXl4orHY7xY89xsbWrRk4cKB/T1xpVvwl8gkTvCfxWLRtGzz4IDz8cKQjUaLAK6+8Qm5hIcdzcpANZwlLearVLQROk4l3pkyp15fdUEZ5Ode9+SYv/v73jX5Wkp3Nlv79yThxgu/HjeNYbi55R48yfvFico8d44XbbvOZxAGSamow+2jxn71oEZtuvpnqb78lyd3do4Qm/hL5/PmRjkBfjzyi9eG/+qqa4t+CrV+/noSaGqwOR+MkDl4TqjSZcCYmei3PYrMxZskSTE4nnffvZ1+3bo2O+eySSxBC4HCPNKlIT2df1660P3iQ6qQk32PKgZ29ejFm2bLGYQImp5PTN2ygqH9/Mj/6iJSRI32Wo/hH3U2LBW+8AVZr9NyMVcKusLCQkStWUBzIeGwfMzDNDgeZpaUsOO883psyhS579mD1sqSEMyGhLonXlucymznYqRMuX40KKUFKDnTsiM3HUEMhBIvGj+d4Vhall1zC0qlT/X9Oild+J3IhxKtCiKNCiM0ejz0khCgQQmxwf0w0JkwF0Pr+V6yIdBRxJxbq9qBBg+i/eTOZJSXBF+K+Iem0WDiWl4c9KYljubksGTcOe0KCluTtds6ZP5+bXnnF91ITTbTEEQKEQEjJ3Ouvx9ng2IK2bZkzbRorxozhoyuvZPbvf09RRQUfXHWVGpoYgkBa5K8BE7w8/oSUcpD74yt9wgpBg7vzcWfUKFi1KtJRxJvXiPK63bZtW5Krqui2Z0/za/kEQohTrWshcFqtLDrnHFIrKxEhXEeazRzs2JEfR43igyuu4MczzmBT//68dvPNHOjcuS7hYzazadAgtvfuzVP33aeSeZD87nSVUi4RQnQxMBZ9OJ2RjsB4I0dqC3SlpkY6krgQC3X76NGjVOTmsn7w4KZbxIHy0bf+ym9/i3A4kAkJQRctTSa+HzcOR0ICW/v21UbGeIvd/QJyIiODZ2fN4o577w36mi2VHn3k04UQm9xvTyN3C9rhgOuvj9jlw04tSBQOUVG3S0pKeOGFF1h49tnUhJBYGyVRd3+2t+PsFgtJNTX+t/4btqTd5znc8TYcx+4rvuKyMhZ/8ol/11TqhJrInwe6A4OAw8B/fB0ohLhFCLFGCLGmqKgoxMt68ec/w1tv6V9utHI6YeXKSEcRz6KibjscDmbPnk3uoUMc6tjRv4QYAG9jvQGcVivmpt7dSonJbte+drkQgLWmBjw+N3rh8POdxJL16/06TjklpEQupSyUUjqllC5gNjCiiWNfklIOk1IOa+1lOm9Idu2CJ57Qt8xYoGbHGSZa6vbixYtxFRdTmpNzql9ZL0L4fGEwORw+x4HXnptYm8iFQDgcZJaUcMayZZz/zTe+XwSaa+G7n2N5ebkfT0CpFVIiF0K08/j2MmCzr2MNs3WrNp09Aqs4RlxFRcu4JxAB0VC3v/76a5YvX44tMRG70asGNvj/cZnN2ozQJl44HGYzqeXldS8ITouF1SNGsGHIEDJLSrTFtDw5nWSWlGC2233/v7off+qxx0J5Ni1OIMMP3wZ+BHoLIQ4KIW4GHhNC/CSE2ASMB+4xKE7fhkVktdLo8ZvfRDqCmBeNdbuwsJBV7tFJdSnPyMaKt26QphZtc7no+MsvpLrXLJJmM6VZWZy+cSOti4qwWyxklpZitdnqum8SbTZunj2bO596SnsB8NE/D+AwmXCqRorfAhm1cq2Xh1/RMZbALVwIVVURDSFk3bpBcTGUlTW5iJFP770Hc+boH1cLEo11e+7cuZjsdm1xq0ivgtlwezeXi0SbjfGLFvGaR0PCZTKxfujQuu87Wixk79uHzeXSls2VkmOtW9P5wAFufOMNXrz1Vu35CXEqqXt0HxUUFNCpU6ewPMVYF9tzvv/5z0hHELwzzoAPP9Qm+TS0caO2YJY/ib2JtTSU2FV+4oQ2JyLSSdzNWlODkJLkqip67djB8NWr+XH0aJwNZngmulyMPOsszho/vtG+nVJKFp99NllTptC6qIi7n3yS7846i40DB2oLd7lXZBRoffTJyclhfIaxLbYTuRGjX8Lh8GFoasusgQO1BO3PP7GaQBGXMk6coKyJVQuD5mvz5KYIgdVu51dLltBt717KMjL4+qKL2Ou5PouU5JpM/KGJtcaFEJw9cSIn1q3j5MCBpJWXk15Wpq3kWHvT1b18rhACqeq232I3kVdUaKNVYonJBOvWNZ3EawmhbRDQ3E0uVdnjzs8//xxaAU3sdB9wEnd3eVSmpjJvgnvya20Dw6OPu2tCAlPvv9+vIlv16MGW+fMRN9/Mjj596q/n4mZyOvnikUf4zTPPBBZvCxW7ify116C6OtJRNFbbx1ebYJOTYfJkeOklSE8PrCyLRXvBSkvTP04las2fP59qH6sW+i2QpO3ZP+35Pdp65Xf/97+kVlVxIiuLzydPZq+73zorJYUx48YxJIjVC/uNGYPzmWdYOXeu95BMJrIPHAi43JYqNhK5lPDJJ/DCC1BernU7rF4d6agaM5uhXz9tA4uiIujTJ/QknJoKgwbBhg16RKhEmaqqKlatWsXOnTtJTEzUpuJXVGgNgGBaz02NNW+ilZ599ChlrVqBELQ+epT08nISqqqY9PXXWO12SEkh85//5Ibhw3ElJyN690aEOKbdfO65pL74IlabTVu0qzZMl0sbpqjebfotNhL5tGnaUq7RJjVV2zIuIUFrPefnw+efQ6dO0Lmzftd56SUY4XM+ihKjKisreeqpp6ipnQkZqkASq5TajkLuvunivDxwubDY7VTm5DBp3jxyCgsxJSZqXSm//jX87ncghK5rX5fn5THyxx9ZMXo0ZqcTKQRpFRVc9d57fPfb3+p4pfgW/Yl81aroSuJDhsC770KPHtr3x45pMeblwdCh+s68qzV8uO+fXXGF/tdTwuLtt9/2ncSNqEcNmO12HGZzXat8zJgxjD/nHMxmszYibNMmOHhQe0fobXSVDi6+/34W/+lP/OGZZziYn09KZSXtDh3i/auuYorapNlv0Z/IL7ss0hGc0rcvrF1b/7HcXJgYhqWqjx7VWvqe9wXOOgs++MD4ayu6c7lcHDx4MLCTGvZlux/ru3kze3r0oDqQ4XpC4Kjd2FsIbrjhBro13CXo9NO1DwO16diRnr/9LS/m55N/+DAuk4nDbdpw5dSpJKiNx/0W/Yn80KFIR3DKm29G7tqtW2uTnw4e1LZ+Gz4cQr0hZgCHA+6/H155Ret5+uYb6N8/0lFFny1btgQ9FLAeKdnWty8WpzO48oCEhITGSTyMhp57LkPOOYfDhw9jsVjIy8uLWCxNcR4+zCcvv8x+h4PObdty+a23IqJknH90J/Lt2yMdwSmzZsHgwZGOQuuHz8+PdBRebdumvWmpVVoKAwZoIyirq6NmbktU+O677wI/yVuSdo+7tge6KqK7dW+2WJg+fXrgsehMCEF7g7pv9DD3scfYd+KENlRSCDYXFrJ5xgxGDB7MhZdeGunwonzPzuuui+z1hYBLL4UTJ+CuuyIbS5STsn4S92S3awN6/vWv8MYUzYqLi8PSD15P7frjUpJotXLBhAk88MADpAc6LLaF2bRuHQVFRXVJ3PNj9dq1vHfllVRHeCh09LbIbTaI1LrESUnacMfzzlPNSD89/XTzx9x/P8yYoY0gjfcd+ZqycuVK/7pBguwqaaqs3r17c8EFF5CVFbk9YGLNN++8Q1VqqteFxaTZzLZ+/fjlf/+XwWefzdkXXRSRGKM3S0VyNMatt8IFF6gkHoDXX/fvuKoqbaTm8uXGxhPNvvkqxO0/3a1qk9OJKcC1ds4//3yVxAPU6sQJ3z90rxJZlZLCD6tX88Rjj0Vk1cbozVRffBG5a999d+SuHaMC7SYcM0a77dDSlJaWal+E0tJ2v60/bfNmco8dw2qzaY+7XE0udZuZmUl2dnbw122havwYPeO0WEAIyiormTlzJrbav0mYRGciLyiI3LWffx66dInc9WPUAw8Efs4998Crr+ofSzR7d+5c/5N4E8cNXL+eSV98wW9efpnz582j2+7ddNu92/vB7m6VW2+9NYiIlRHXXkvGiRP+rQfvXpL3H//4BydPnjQ+OLfo7COP1OiQjRsNHzcbzzZs0OaOBOLmm7XZ6Nd6WxE8zpSUlFB45IjWt9SEzOJizvv2W7rv3o0tIYG1Q4YAMGT9eix2O7t79KD39u0kuLtVhq1dy7C1a7FbLHx86aVsqx3v6U48Wenp/OHuu7WJPkrAzjjjDPatWYNl+3aKa7fcg0bj+Rt+//jjj/Pggw+GvJSBP6KvRb51a/iXp01Jgf37VRIP0cCBWn0OdDRbpAcnhctXX33V7ObJKRUV/G72bPps306izUZ6RQVjf/iBsT/8QEZ5OSnV1fTbsgWrl75xq8PB6B9/1L5xuUiqrKRHz57cce+9KomHaMr06Ux/+mmGbN1K+9rFvGpb6A2TeO2oFil59913wxJf9CXyl14K37XMZrjzTm2CjdqJRDdPP63VbY/NYppVXGxcPNFi165d9XfD8WLYmjVYbTZMHseYpGz0vS+VyckgJVnJyVw8bRrXXXddWFqELYEQgkvef5/fvvIKwnO4YRO/3x07doQhsmjsWgnnuip2e/jH8rYgK1c224tQJydH+3P4e3yscXnOvGyiznX85Resfox6EGh7eXqWZLNaWTVyJL07dWKK2svVMEIIrhk8mHe2bWvuwLpW+TXXXGNoTNHVIi8sDF/T7LzzVBI3mNms7aPhr9RU42KJtPf/+1+/jjual4fDj2GvDrOZyuRkahISqE5MxG6xsGTsWHb37MmkKVNCDVdpRu9rrqFHWdmpSVYNSVm3J8H2bdtYsGCBofFEV/vnqqvCd63f/S5812rBBg+GJ5/0b2KszaatWByPe0lvP3nSr4bDqhEjGLp2LRaP4WvS/eGZ3p1mMy/cdhvp5eWkVFZS0KED1SkpAKS4PyvGmvLvf/P0E0+Q/PPPHPGyvECHgweZ8M037Ojdm1U1NYwfP96wexXR1SL/6afwXWvSpPBdq4W7807IyPDv2Ndfj6510vTgCGDSzomsLF6/8UaOtGmD02TCYTaztW9fdvbogcNsxmkycaRNG+ZMm0ZFRgaHO3Rgd8+eWhKXktP69DHwmSiezGYzd9x6K7955RX6bN1av+tMCArbteOn009n7A8/8NuXX+Z9A7tXoqtFXlkZnut89VVUrhwYz3JzoazMv2M7dPBvyG6sCHTjiEP5+bx4221YbTacZjMudyvOYrdjcjqxeZugIiVCCCZHwQJOLYk5PR3hcrG3e/dGM8EdVivrhwzhwm++odWJE/Tcu5dP776byQbMhIuOFvmGDdovwejZUEJoSfzCC429jtKIn13EdYKZYBSNPvroIx5/7LGgzrUnJNQlcdASQ6Mk7n7FS01N5f6//IVE1UAJO6fVis1jqzpPDosFCSTY7bQ9dIhu336LNKCVEh2JfMgQ45tgycnaRsYqiUfE5MmBHf/oo8bEEU47duzANXcutz33nDH1232j7YwzzuCP991Hgo9kohjL+vDD5Bw71vhv7F4P58uJEylNT6dNYSHlGRm8Mnu27jFEPpF/+WV43ke/+KI28UeJmEAn/txyizFxhMu6J55g0uefk1NcjNC7jteWZzIxfvx4NVY8ku6/n4mff6593WCSkNPdvfLi7bezdMwYhq1dS5f33tM9hMgn8nAMUejeHW64wfjrKE2aOzewceIGNFzCasSCBTgsFr6cOBGp90qa7htqU6dOVS3xKNB18WLO//prWhcWIlyueiOUXBYLNquVDUOH8kt+PmOWLuUrnRcFjHwid4+1NNTPPxt/DcUvgQ5MCtf9byOYHQ5m3X03643YlFtKcnJy6N69u77lKsHp3p2szp0ZtXw5CV7u9bksFuwJCcybMIEEu52fVq3S9fKRT+Q6P6FG3nxTrSseRfr0CWziT0mJcbEYSUrJvPPPx56Q4DuJB9vd4n7bfvvttwcfoKK73v/5D5mlpTi9jRWXEuFyUZydTWVKirbbkI4in+GMHjR8/fXGlq8E7G9/8//YMK4Eqrsj7ds33RIPtpUuBO3atcOkGihRRXTsSOfjx2lfUNBoww+T08nJ1FScFgsv33wzZpdL19Erka8JRlbGyy83rmwlaNOm+X/s6afH5phy0cyaKqGaFsgvUQkb06RJXPbBB7QpLMRit2OtqUG4XNowUneuK8vMpEZKfli4UL/r6lZSsIxKthYLfPihMWUrIcnL8//Ymhr49FPjYjGcAaNVhg4dqsaLR6uHH6ZVRQWjly1DOp3YLRbtRrfnGubuxbQWL12q22Ujn8j93ewxULXDgZSolJ/v/7H/7/8ZF4eRRuTm6l+olFx88cX6l6vow33zue+2bdz+0kukV1R4Pcxqt2Ox2Th8+LAul418Ivdc11dP3boZU66ii0AmCG3dalwcRhpVVaVvgVIycPt2fctU9CUE0mzGZTKRXVxM7rFjXg+zJyTQad8+1gWyPGgTIp/I/djYNCj+rtKkREQgi0+6XBCBjclD5rRaMekVuJRkHz/OhH379ClPMYxz2jRtgTMhtJUs7XavxxV07szWNWt0uabfiVwI8aoQ4qgQYrPHY9lCiG+FEDvdn7MCjiAhQf8bnj16QNu2+pap6GrgQPjzn/0/3sh9g42q23Ly5HrL0QbFPQ1/9NKl3PrCCySpIYdRz/rf/2LLyeFYTg6nbd1K1vHjje+VCIHDYqFSSuw+En0gAsmgrwETGjx2P7BQStkTWOj+PnBTpwZ1mlcmkzbtX4l6//wnZPmZHl95xdBQXsOAup3bpUuzW7s1SwisNTWcu3Ah1t694cYbgy9LCY9WrRBbt5Jos2EC8o4d8zqCqXa8+cJ580K+pN+JXEq5BGi4fc9koHaO/Rzg0qCiuOSSoE5rJD8fNm6EXr30KU8x3JgxkY7A2LqdlJQU8jDERJsNMXUqYu1atatVjEjPzCTFfY+k144dWJtYynjdpk0hXy/UPo02UsrDAO7PPgeWCSFuEUKsEUKsKSoqqv/DUaNCDMNt3z7o31+fspSwCGTtlTCPJ9elbncfODC0GZxS0jcjQ9vLVg05jCkWux0n0G/rVvKKiur3lbv/tolVVQFtPOJL2G52SilfklIOk1IOa926df0fdugArVqFdoG0NG2TSCWmrFzp/7HR2hhtqm6fe8EF3k7wp1D6bNkCTicdf/UrnSJVwsY97FCgzeq8/o03tBvftX9793jymqQkrDpU7FATeaEQop0Wl2gHHA26pHvvDS2SUF8IlLBbsgQCGUYb5ha5LnU7JSUl6H7yCfPmkVdUhEnduI85JbfdhstkQqAl8529e5+aDFTL/X3ewYMhXy/URP4ZUDtXeBoQ/By8UMd99+wZ2vlK2AW6a1CYW+S61e26ZA5kFhf7ldRNDgeJNhuJNhsdO3YM9tJKJFRUYFu0CLPLRW2VLcnK8rmLkFmHuTSBDD98G/gR6C2EOCiEuBn4J3CeEGIncJ77++AUFAR9KgDLl4d2vhJ2q1cHdvyJE8bEYXTdttvtJFVWMnXOHJwWi1+vSC6LhbL0dI7m5LA9VmdEtVQnTpB75AgAtS/ZbQ8f9rq8LUCFDnNe/L7VJKW81sePzgk5CoBQ79wavd+noruG97ybU1ZmTA+a0XXbbrdz9YcfcqJVK6oTE/1+azF36lRqUlNZ+8UXDB85Uo9QlHAoKMDkbo3bzWasTic9d+4ks7SU4zk52os52rsul9lMeSCLD/kQ+ZmdtQK56+VLLE7/a8EC3VOkQwdj4jBaSkUFXfbtoygvD7u/I0+EoCwzE4ATsbj8Y0vmHnboEoLlY8YgAbvVynVvvsmATZu0HYSkxOJwYK2uppMOy4kEMPjLYLt3h16GqvAxJTs7sFb5iy9qMzxjaRluW2UlSdXVuEwmWhcVYa2p8T+ZuyUbFJtikCFDAKhMTua7s84irayMvd27U56ezoFOneoqsC0hAYvNxq5duzh+/Dg5OTlBXzI6/iX06BZJSgpsULISUXPmBJ6Qb78dWreGvXuNickIa5YupSQrC4fFQv8tW0iw272/FWmiETKud28DI1T0JGtqOPLII5RkZrKvc2cwmdjVowdb+vfnQJcu9Su9yYQzIQFcLp555hnee++9oDebiI5E3sSsJ7/9+9+hl6EY7ttvtS7im26CwsLAzy8uhuHDdQ/LMJv37UOaTHxx8cUgJdc3sWyzcDq1SSMe/8xml4sB110XjlCVEJ0YOhSSkmjz73+TVVqK3WIBKdlx2mk+z6nblFtKtm3bxrJly4K6dnQk8vT00M5PTa1bB1iJXjU1cP75oZdz/Djs2BF6OeGQ5V5MZlu/fsy56SY2Dhrk/WanEGSWlnLR559zzTvv1CVzS0oKQk10i3q2Bx8kY926unHj0mTiy0suAZMJaTbX/c2Fy9VoGzhMJszuxxYtWhTU9aMjkYeqsjK2mmkt1P/8j35lxcquQWlpaXX/xIc6dGDl6NHeE7mUZJWUMGjTJrru3Utn93K12dnZYYxWCZblkUfw/KuWtGqFt7FJ0r1Oea8G68o7zWaQMsa7VkIlJfzyS6SjUJqh56ZNu3bpV5aROgQw1KbXzz8DIKQkxz1x6OjR4CdLK+HTMGmnVFXh8nETqKxVK6744AMySks9CnCX4HBQE0RXc3wkcoBHH410BEozJk3Sr6yGy/VEq/5+LuJmcjrpu21b3fdH2rQBIXA6HJw8edKo8BSdSCHwbEsn1dSQ5CMh5x09iklKTm8wd6bj/v103bsXV6DjcommRG61hnb+Rx/pE4diGD3vR69dq19ZRjL5Gprj8c9qttkYuHEj6eXlFGVnM//88znk0ZJfsWKF0WEqIRINNpYVUnLOggVYG4zIM9ntjP3+e8xOJymVlad+ICWTP/+cyZ9/TkIQuTB6Evno0aGdLyW8+qo+sSiGsFrhu+/0KUuP+WORkl5aSp/t20msqiLl5ElO276dcd99x9rBg3nx9ttZO2xYvV3Xly5dqssuMopxxIwZuEaMQHJqWv6Q9eu59JNPyDp2DFwuhMuFWUreu+Yalo0eza4ePbQDpQQh+Pzii7FWV1O2YUPA14+eRK7HehJ/+lPoZSiGOussfTaTMGrdFUO5b2S1P3yYpKoq7AkJVCcmsqN3b566806+uvhin2uxbNy4MdzRKgEyr1zJifR0dnbrVpfMT9u6td5IFntCAg6rlUXnnsue2hmd7lUQ93fpwrN33MHxIMblRk8i1+M/s6Qk9DIUw6Wmhl5GTE7idSfoHb17s2HIEFxmMy6LBXtiIk6r1efNMYADBw6EK0olBBVpaVg8lgo50q4dFenpp8aLu8mGS9oCCEFlWhqLgvhbR08i79Qp9DK8jbc9cgRmzoS77oLFi2M0A8SXJuZHtAze/ombYWkwa1lKyYH9+1n88ccsX7CAsrIyPSNUgrS9b1+67N9fN4qlKikJ4S3nNFEHjgXRII2eOe2vvx56P3lKSv3v586tv7HzU09pn9u1g8cfh2uvjd5tZ+JYLK2VoofU1NT6I0981bkm6mIrj2UfpZR8+NRTlG3YQLtDh9jTvj0/vfMOFRkZ5A0ezIQJE2i0C5cSFrt79eLshQvrvu9QUFC3ybK/EnysW96U6PmXGjUq9Pfc7tXiAG0Fshtu8H7c4cMwbRo8+GBo11OCoseboiDqesTcfPPNIZeRlpZW9/W2L7/Evnw5x3JyWDVqFAe7dOFIfj4VGRkc2LqV2bNn02hfXCUsTqal4fBI3Ik2G2OWLvW/0gvBgAEDAr5u9CRygC1bQjv/7LNPff3SS03/8hwOrVXuOShfCYvBg0MvY+LE0MsIl6ysLDJC3DwgPz+/7uuN332H1W6nymPWaO1bdUdiIna7Peip3kpoehcWYmqQd85asoRkz6GGvrjPGzZsWMDXja5E3rlzaOd7Vt5jx5o/Xgj46afArmG367PIVwPffXeq20wIaNNGW3kgHunx6/vkk9h6DZ4yZUpI53tO3U4uKmJ7374+12yB4G6O2u32oKeIN2XJX//K43/8I3//61959vbb2fGf/+h+jWhx1vLlWBpO6JGSVPdmzF5JWZfEcwoL+fyzzwK+bnQlcoC//CX4cz0rrz8rxlVVQXW1f297PvlEWyo3IUH7XJtxn3gi6HBBu3x2NowfX//xo0e1nqZ4uzfrcMB99+lT1vvv61NOOLRr1y6k87/++uu6r/u2bq2NemiCw+HA5XI1m5illMydO5cZDz3E3x99lEceeogn7r6b5+65hyr3BgnBOlJQwOvXX8/xbdtIrq7GnpjIsbw83ikrY7VelSCa/PADqV7Wjjienc1xX/csGvx9SnJz6TB3bsCXjr5E/ve/QyjbWtVOcz7tNPCnr+n887W7b7WJOTERXngBFiyAkydh+3bt8csu896UvPde6NcvqIxbWVzNP5IfZEVJT46Qx1yuoyt76h3zz+B3QY1KL76obdmmh1mz9CknXO6///6gz/VsYfe87z5yjh1rss7ZbDYeefhhHp4xg4cffJB//fnPvPyvf7Fz504OHz6MlJLFixfz8MMPs2vnzrr6L00myjIzOZaezpt33MHOnTuDivfLO+9k8X33Udi+Pfs7d+b0jRu58t13MblcIASLTKb4a6VcfTUSbWegWnazmZVnnKHtCuSLx1txl9nMpkGDkP70KHgWYcRbqeYMGzZMrlmzpumDxo2D778PvPCePcG9+BBOZ2Q2m0hM1JrYP/2kDX+sHVealgZvvQVjx0JuLicdVlKoqhuq5MREGRkM4CcK0PpEU1K015N40bEjHDyoT1mJido7moaEEGullIF3NOqgubrtdDqZOXNm3Wy+QEydOpXu7uWaS9ev58lPP/W/jIb/50LUf8xbOU6n1shxxyqEIMNqpV1KCid++QVRXo5EW5q1Z1ISY//xDzbt2sWi//s/ymtXbXSXa7bZ6LF7NznHj7P8zDNBSv52772gw8bDUeGXX3B27szerl3JLSqiVXl53f+1zWrlSJs2zLnpJlx+5CNrTQ1TR42i0yWXNPqZr7odfS3yWt99p/2nBmrnTq0fG7RKaMRuvc2pqYFvvoGCgvr7iFZUaCtHZWYiHY56SRzAjItUTnIvp/oQ9Zg8Ey1cLv2SOBhyq8JwZrOZ2267rXEi9cMHH3xQ9/WJ7OzAXgg8b8B4ntfUmHbPd6ruRaFO2GxsLy3lcHo6hzp04HD79hzKz+f73Fwe+c9/+PTTT7Uk3qBcZ0ICu3v0oKvnlo7xVLm/+45fOnbks0mTSK2q/3+dYLfTtrCQfn4O5hDA9gBvAEVvIofg7/b9618wdKhWEaN0LnfttN2GErAzlh/qvtdj5YJoYcSIOG8t8miXl5fHNVdeqb2yBZDMq6ur2bt3LzNnzuS1116jy969JDd1E605zb0QeI6I8XzMM0k3fIFo6oVBSopzc0FKMouLvU/gi1HVH3/M9+PGMXbJEiwNN45AS+Z9GqxB7ovTbGZ7gJOCojuRm0yBb7UO8MgjsG6d/vGEgQR2oi2mk58PubmRjUdPc+boX2asDpfu068fV0yaRF5BQUDJ/PXXX8fpdNLhl1+oSE6mKpbuiAtBSWYmJoeDG++5J9LR6Mo0fz4dDxxg4MaNXhtoLiGoSm5mG20psdpsOIWgJK4SOQT1FlSXzZwjxI6FJ5nOyy/H314ZRsy/iuUFL/sPG8ZNDzwQ1LlFOTkcc69Z3qgF7DGcLWBSBtd48qPctIoKdnTvzl9nzCCra1f9rxEpLhd2p5Mzly3D6tmV6sFpNrNu8OBm/y4Xfv55Xc4LZIOJ6E/ktfxteiUl+V9mFE3Pl4ATwcv8huXOM9FhMmBUmTvXmD7tmTP1LzOckjt3ZkD79vWTr49EbPboirDVDoFtqDYRCxFw1w0AQpBWXt5oHe2QuFua6U4ndz7/fL3nEQ9+HjWK5OpqrD6WGpaA1eFgwrx53tdd8bDo3HPr1rDYGMBGzLGTyHNztW3XPafhN9Szpzbe21+dOmnN3ltuCTW6oEmPj83ZY7nd9UJcrkXia7WEUDkc0GCjlZhz+S23cOkZZ2gzAms/GiTpkSNH1l8/palGiBBcfPHF3HrbbafKDEBFRgYdfvlF2xA41Na9lKSVlnLOhAn85umngysnmq1eTZf16zHh/Z4XnFqffOXIkTT5mxSCCo9RPN/88EMTB9cXPYtm+SMvr/FStU4n7N+vjU7JydEqT1KSf3fBKiq0jugXX9Rml0RgGVwBMHw4YtUqBob96uGxZ4+x3bh33BHcSNVoMnDCBAZOmFDvsZqaGk6ePElGRgYWi4UjR47w4osvNl+YENhsNtq2bcv0u+/mqaeeCmy4oxAU5Odzx6xZPHPXXTj82bGmtnzPP7TLxS033EC7nj39u24Mkvff32TldgiBSUpcQJ9t2yjo0IFSzw21G/5dan+HQiClpKqqiuTm+taJpRa5L2YzdOumJXHQfhGHD/s3ftxz2vTx4zB8uDExemM2wz33aH+0VavCd90IOH7c2PJjvUXuS2JiItnZ2XVL2LZt25bzzjvv1AG+EojLRU938szKymL69OmIAO812a1W7BZLszNI65ESHA7y7HZuuflm/vbww3GdxAGw2bB6GaUiAZvFgjSZMKEl2tO2beOWl14i3XMknRAkVlfT2mMzCVNteUJwzM+JQbHVIvdXZqY2lnzOHLjpJu/HpKTAQw+d+l4ILaGWlRk79vyhh+BvfzOu/Cg0dKix5UfRrQ7DjR49mlGjRvHUE09Q6m1orZQMGjqUXI/hTjk5OTz4t7+xad48Pv7xR+1BP4Ye7uneXRtF0VSjqPbFQQjuu+8+UuNpbLgfis85h+ylSxs9brdYMEmJyeViS9++bOnXjwSbjYEbNjBq+XLmX3ghAMLppNf27Vz85Zcc6NSJd6ZMYfjKlezs04fjOTmN1qH3JfZb5E2ZNg3ee6/x45dcovW3exvbl5Gh3STyZ6OL1FRthIw/62hYrVrXTQtL4mD8+uPxMjnQX0II7rznHnp360b7/fux1tRgdjhIrazkiquuYtKkSV7PO/2CC/jD9Olai6+Zvu9fjR3LaYsXa+O9fR3nfrxP58787W9/a3FJHOCku9vJ8zdUnZDAquHDsVmtvHXddXw6eTLb+vVj48CBvH399RTXdq1IidXhYNz33yOA8vR0uu7dS8aJE5y5ZAnC5fJ75Ep8tsg9XXWVVuEqKrSZov709wmh9bs7HJCVpZ3bUE6Otuyu1aod27WrNpPTm1degd/8JrTnofgU4LIUcUEIwZRp03DdcAN2u52EhASt+6QZubm5/L+ZM9n344989P77lKel1X+lFYIhQ4Ywfvx4hBBcOnMmn/3lLxzOz2+0XVma08n0Bx4gMZgZ2HEi2z2MUgAOk4nSrCzenDoVKQRZpaX80qkT9trfj3vPznUey9QOXb0ah9XKE/fcg9NsxmGxsK9LF9odOYI0mSgvKIAuXZqNI/4TeS2Phfn9ZrFAebm2FOHzz8OhQ3DuuXD55fVnpVmt2tzz0lJtKuaxY9CnD/To0fK2w/GhXTvt1oURYnF2p15MJlNQibTLqFHcO2oURTt2sGXtWuytW3P66aeTl5dX7wWhTZs2/O7VVykuLqa4uBin00nHjh1JabgbVwuVdvnlOITALCUmKfnwsss4kZkJQrB26FBsXv42dfcdhGDjwIH83KePNlnI/bjLbOZQ+/aAdrPUHy0nkYciL8+/LpHMzNC3q4tTixYZt1dnnA1LDqvWvXszrnfvZo/Lzs4m23O0haJJSoIzzsC+ejW7evbkSPv2dQl5X5cuWjdtw42XPb6vSUz0OifAabGAlOS4E3pzVHNRCYsTJ4zbnq1vX2PKVRR/WHr0YMnYsdrMTXeSzj9wgGvefRezj5meQN09hqb29Mz3cx17XRK5EGKfEOInIcQGIUQz69MqLVGbNsaNJX/jDWPKBVW3FT907MhZP/yA2T2TNqO0lOvmzuVA586nxoVLiclurzd7t/XRo7Q/dMjrJCHhdJJ/4AAmP7vN9OxaGS+lbIG3nRR/dOmi3UrwMYs5JP366V9mA6puK76NH0/Nk0+SVlYGLhdD16xh8fjxbBgypN7QTSEEuUePYktM5Lq33iKrpASXycTR1q0brVUuTSaOBbBinupaUcKmBQ9uUOKZ2czWwYPZ1aMHJpeLzNJS1g8dir1BX6LTYqEyNZXr3nqL3KIiEux2kmpq6HTwIHfNmkV2g5lz1QEM59QrkUtgvhBirRAicguXKFFtxAhjyjV4QpCq20rTBg3CZjaTXF2Ny2JhT7duPhfHqklKIqukBHODn6dXVHDOggWnHpCSpAAWANQrkY+RUg4BLgT+IIQY2/AAIcQtQog1Qog1RbG6iLQSEiM2T/dnWkCIVN1WmpaVRc+BA0mqria1tJTMkhLvidy9CqTLy5BkAWR4bGYrgPENd2Rvgi6JXEp5yP35KPAx0KjtJaV8SUo5TEo5rLWvHaWVuObnSKqAPP+8/mV6UnVb8Ueb3r3pt2kTN73xBmN+/JH0sjJt9cgGHBaL15EsdrOZ9LKyumSff+AAwwNY+ynkRC6ESBVCpNd+DZwPbA61XCX+fPihvuWlp2Pouu2qbit+e+45kux20svLsToclGZlNV6jRggcVivfnnceNqu1brSKSwgsTicplZX037SJ877+mvPPPdevmbq19Bi10gb42H1RC/CWlPIbHcpV4syRI/qW52WtIr2puq34paaoCFebNpzIymJHr15Y7Xbvi41JyeqRIzmal8fIlStJLy8nvayMtIoK9nfuzPbTTiO9pobh118f0PVDTuRSyj0Qt0tpKzrSM5G3bw+nn65fed6ouq3468cBA9jTsSOHO3TAZTLhgnprjXc8cIALv/yStoWF1CQksHrECN6/6ioA7nzySaqTkvjksstwWixkz5gR8PXVFH0lbDZu1Kec5OSY3VtbiVNLR43SZmh62T81r7CQqW+8QYJ7EkWSzcbIFStIPXmSby68kLSKCp645x4q09Lo2asXwzwW1fKXGkeuhI1ea6K8+aY2U1RRooXPNduFYMzSpVga3PhMcDgY8NNPWG02Pp08mcr0dMwuF9dee21Q11eJXAmbsY0G7gUnglusKopXbWr7DaWk44EDnLV4sdbqLi+nbWGhtndqA06zmaySEjYP1HrvXEKw74svgrq+6lpRwkavvXePH4eTJ7V9PRQl4nbsYOTy5Xw+eTJXfPghPXfuxOK+2Xn2woUcz87GCWwYMoQfxo6lIi2NNoWFnL1gAcW1+wwLoW0P9/TT2sY3AVKJXAmLwkJtuXa9lJSoRK5EB/usWfTcuZNxixbRc+fOur7w2r03c4qLWXbmmSwdO7Zu2v6hDh146/rrT62vIiWWmhosR48GFYPqWlHC4rPP9C2vQwd9y1OUYB1fsIAjeXmcuWxZXRL3ZLdY+H78+EZrr7ga3DRqd+AABSNHBhWDSuSK4X74AW67Td8yW9KGy0qUkhKmT+eYxcKOJhbFP5mWdmpXIE+ej0nJZV99xY4gx9SqRK4YqqYGJk2CptbXD4bLpW95ihKwTz+F116jOCuLgvx8r+uKAyRXVnpP5LWkREiJAFz79gUViuojVwy1eLExSVdthapE3EsvYa+poTw9naK8PKqTkvilUyfmTZhAeUYG6WVl9Nqxg42DB3t/C+necCK5shKnxUJRbi6tg1wFTiVyxVA1NfqXqdalUqKCe9fv1MpK0svL+WriRLYOGFCXtEuzs1l1xhleJwkBtCop4YbXX2fz6aez9Fe/Iqe4mEHTpgUVimrXKIYaMQI8VufUxRq14ZoSBWSPHriAXy1bxvhvv2Vr//6Nk3YTXSplmZksGzOGBJuNrnv2UNKuHd382AjbG9UiVwwjJeTn61vmWWdBp076lqkogbJ/8w2W2bNJQFs7vDwry/878O7jpBCsHzaMjvv2ceUHH5C0e3fQ8agWuWKYBx7Qv39cr0lFihIK5xVXAFoSB6gKYDefeoTgYKdOzPn970kIYd0JlcgVwzz7rL7lmUwwYIC+ZSpKMBIrK+uSeEH79qw44wzvB/rY8q3eISYTg847L6R4VNeKYhj3vSDdTJmib3mKEiyJ1hqXwEdXXIGjdrJPw8TtsZRtU8acfXZI8agWuWKYbt30LW/OHH3LU5RglWZkIIHyjAxKMzJO/UCI+h/NjZOVkt5CYApx81mVyBXD3HmnfmX97nfga6VQRQm3w927sz8/n0Xjxp1aLyVQ7tb7lAcfDDkelcgVwwS4W1WTnnxSv7IUJVQJl13G1xddxBaPcePBsFZW6rLehGrjKIZJT9evrORk/cpSlFA5Bw+muKYGh0eXSFp5OZ337aM6OZm9Xbs2WhSrHndrfFS/frrEoxK5EvW6dIl0BIpSn+v4ccxOZ10iH7t4MWcuXapt94a2Y9AbN95IYdu23gsQAqRk2OWX6xKP6lpRop4OXYiKoqu2I0bgcPeNd92zhzHLl2N1Okmy2Uiy2UiprOS6uXObnUiRrtPbVpXIFUOlpIRexo03hl6Gougpu08fuu/ahcVuZ+jq1VgbrEMugMSaGvIPHvRegJTkd+yoWzwqkSuGCnLpiDrDhum3abOi6EYIrnz/fdJPnCC5qkobUy4Ex3NyKHMPR5RCkGCz+Sziuuuu0y0c1UeuGOrFF7WFs4I1f75+sSiKnmoGDcJqt/PTgAE4rFa+uOQSqpOSkEKQd/Qol370Eb94WxhISsxSkqzjHXzVIlcMNXx48Of26AFZWfrFoih6qnzvPSw2GxsHDuT9q6+mPCMDe0ICDquVw23bMvuWWxpt71Y7WuU2nfsLVSJXDNe5c3Dn/fyzvnEoip5at28PJhPSZKq78VlLms3YExJIO3HC40FtI4lRffuS0727rrGorhXFUE4n7N8f+Hnbtql9OZXoJr78kkPt25+aju9FamUlZpeLE5mZpFRUIC0Wzr/6at1jUS1yxVBVVYGfc+aZ0KeP/rEoip5ce/cinE5SKyq8r3LovvnZZd8+TE4nNYmJ3P3b3xoSi2qRK4YK9H5Obi788IMxsSiKnn4uL0darVSZTJidTm0yUIOWuSMhgaN5ebhMJiZfdBEJPXoYEotqkSuGWrjQ/2PbtIEjR4yLRVH0tH7nTgBcFgsJNTWYnU6vx9mtViZefDGDfK1ZrgOVyBVD+ZoP4c3hw2rMuBI7POdsVqWmYnE4Gh1jtdmQ6ekMD2X4lh9UIlcMdc01/h+nbm4qsSTJasXkMQXfbrFgdjiw2mwIpxOLzYbFbuemBx4wPJYW10fuXLGCn//yF46ZTOSlp9Pz2WcxdegQ6bDiVmpq88fcdhs895zxscQzKSXff/45K9evR0jJqN69OXPKFIR6dTTMpKeeYt/DD1ORng5CaOuSu4cYtiopoTQzkz/89a+kpaUZHkuLSuSH7ruP2Skp2lbsbsmzZnHHJZeQPHZsBCOLbxUV4K0u/+EP8Nhj+qzH0pJJKXn44YfJLC5m2E8/kWizsXP/fpZt3sz9M2eqtzoGsbZqxWWDBvHVihWU5OQAkF5WRreiIgbefjudx4wJWywtJpHLmhpmp6Y2GvNZlZrKq2+/zR9UIjdMaqrWUNm6Fb79FiZOhJ49Ix1V/Hjx2Wfpt2kTkz/7DOFyYXa5GLFqFdt79+b7zp0565ZbIh1i3Op2ww1MnzqVyq+/xlVWRtoVV0CI27YFQ5c+ciHEBCHEDiHELiHE/XqUqbcv/vIX7wP3heBYmzbNLjephK5vX7jrrthK4rFQt0sKCpj82WdYHQ4sLhcCSLDb6b1jBwWLF0c6vPgnBCkTJ5I2ZUpEkjjokMiFEGbgWeBCoC9wrRCib6jl6m1bMz8vOnYsLHEosSNW6nbXvXvrNjTwlGi3M2Dz5ghEpISbHi3yEcAuKeUeKaUNeAeYrEO5urHb7VR57nTthcPHGFClRYv6ur1o0SJcPnZqd4HXBK/EHz0SeQfgF4/vD7ofq0cIcYsQYo0QYk1RUZEOl/XfrFmzmlwPAaBNmzbhC0iJFVFdt6WU/LBkCXu7dkV4mSLusFrZMGhQ2OJRIkePRO4tOzaqVVLKl6SUw6SUw1q3bq3DZf1TVFREZWWl7wOkBCEw+WjVKC1aVNftx//9b0CbBv7eNddgs1qpsVqxWyzYLRZWDx9OqUrkLYIeo1YOAp57FuUDh3QoVxfPNTdAWQ3NUnyL2rotpcRWWgru9a73dO/Of+69l9O2byfRZmNXjx4U5+QwYdSoyAaqhIUeiXw10FMI0RUoAKYA+u1hFAKnn/3eatKE4kPU1u3nnnsOR0JCvYaILTmZjYMHa9+4u1oymrk3pMSHkBO5lNIhhJgOzAPMwKtSyi0hR6aDt956y6/jstQ2NIoX0Vy3j/k5yqpt27YGR6JEA10mBEkpvwK+0qMsPe3Zs8ev4zIzM40NRIlZ0Vq3gaa7Bd0/q6mpCVMwSiSpO3zA0aNHIx2CovjNbrf7feyhQ1HRpa8YTCVyoKKiItIhKIrfFixY4PexNpvNwEiUaBG3iVx623pJUeLA9u3b/T5WDattGdRfGUhMTIx0CIriN3MAszV79eplYCRKtIjbRB7IkMKrDdjVWlGM0ieAnanVjfyWIW4Tub8SExPp1q1bpMNQFL/l5+f7ddy5555rcCRKtGjxibxv36hbzE5RmuRvv/fo0aMNjkSJFnGbyF1+ri9+4sQJgyNRFH3t27fPr+P8ndmsxL64TeT+ToTo3bu3wZEoir4KCwv9Oi6Qm6JKbIvbRJ6UlOTXcUOHDjU4EkXRV4J7oazmjlFrCLUccZvI/a3E5eXlBkeiKPoa5MfStGoiUMsSt4kcIDk5udlj5s+fH4ZIFEU/PXr08Ou4QKbyK7EtbhN5TU0NVVVVzR63bVtzu3kqSnRZtGiRX8etWLHC4EiUaBG3iXz58uWRDkFRDLFq1Sq/jlu7dq3BkSjRIm4T+fr16/0+Vr0FVWKJGlqrNBS3iTyQm5gOh8PASBRFUYwVt4k8EPv37490CIpiCLUKaMugEjlQVlYW6RAUxRBqdmfLELeJPJBNZ+fNm+fXCBdFiTUffvhhpENQwiBuE3kgCwa5XC6eeuop1XpR4s727dtZsmRJpMNQDBa3iXzIkCEBHV9dXc33339vUDSKop8uXboEdPzixYvVTM84F7eJ3Gq1BnzO0qVLDYhEUfTVunXrgM+ZN2+eAZEo0SJuE3kwpJS88MILkQ5DUZrkz6JZDa1bt46dO3caEI0SDVQib6CwsJD33nsv0mEoik+dO3cO6ry33npL3dSPUyqRe7Ft2zb27NkT6TAUxSt/FoPz5bHHHvN7ZqgSO1Qi9+GNN95QU/eVqBTqvIfHHntMp0iUaKESeRP+/ve/+73TkKKES9euXUM6v6amhtmzZ+sUjRINLJEOwEhdunTxe39DX/75z38C2g2m0047jXPPPZe0tDQdolOU4ITStVLr0KFDzJgxAyEErVq1YvTo0QwbNkztKhSj4rpFfsUVV+hWls1mY+PGjcyePVuNyVUiLj8/X5dypJSUlpYyf/58FixYoEuZSvjFdSI3ouV88uTJgJbIVRQjXHrppbqW53A4WLlypRrVEqPiOpEDdOrUSdfynE4nCxcu5OTJk7qWqyiByMnJ0b1Mp9PJl19+qXu5ivHiPpGfffbZupdpt9t5//332bt3r1omVImYVq1a6V7mtm3bWLJkCaWlpbqXrRgn7hN5sJMnmrN//37efvttnnnmmYA2sVAUvVxyySW6l+lyuVi8eDHPPPMM33zzjWqoxIi4T+QASUlJhpRrt9spLi7m9ddfN6R8RWlK9+7dDSvb6XSycuVKNm7caNg1FP2ElMiFEA8JIQqEEBvcHxP1CkxPI0eONLT8Y8eOMWPGDLVHYhyJlbpttE8//ZTZs2erJZ6jnB4t8ieklIPcH1/pUJ7uxo4dG5brzJo1izfeeENV+vgR9XX7zDPPNPwahw4dYubMmeqeUBSL6wlBtUwmE927d2f37t2GX2vPnj3MnDmTtLQ0evXqxaZNm+o2d77ooosYNmyY4TEoLcc555wTtuWXa7sQu3fvjsvlYu/evQCkpKRwyy23GHLzVfGPHi3y6UKITUKIV4UQWTqUZ4ipU6eG9XoVFRWsW7euLokDfPnll8yYMUNN+48dMVG3J04Mb6/P7t2765I4QGVlJbNmzeLtt98OaxzKKc0mciHEAiHEZi8fk4Hnge7AIOAw8J8myrlFCLFGCLGmqKhIr/gDEi1T6x9//PFIh6AQP3U7Wt7l/fzzz6xcuTLSYbRIQq8+LyFEF+ALKWX/5o4dNmyYXLNmjS7XDUR1dTX/+te/wn5db5KTk7n66qsD3rZLaZ4QYq2UUrfsFgt1+4MPPmDLli1hv643Xbp04brrrgtqly6lab7qdqijVtp5fHsZsDmU8oyWlJTEWWedFekwAKiqqmLOnDk88sgjkQ5F8SLW6vYVV1wRHQteScm+vXv5+6OPslBtLxc2ofaRPyaE+EkIsQkYD9yjQ0yGGjduHJdffnmkw6jjcrl49NFHIx2G0lhM1W0hBP/7v/9Lt27dIh1I3cfSH39k788/RzaeFiKkRC6lvEFKOUBKebqUcpKU8rBegRlpwIAB/M///E9Qex8aweFwEKn7Bop3sVi3zWYzN9xwg+4LaoXi9TffjHQILUKLmNnpTXJyMr///e+D2pHcCG+99VakQ1DixMCBA3VdwjloQoDJpJawCIMWm8gBsrKyuP322yMdBgClpaUcOnQo0mEocaJ///5R04X4f//3f2qfUIO16ERea9KkSZEOAYDZs2czY8aMSIehxIkBAwZEfpKOEJSUlPDII4+wa9euyMYSx1QiBwYPHsyUKVMiHUadN1W/oqKTO+64g4EDB0Y6DADmzp0b6RDilkrkbr179+bBBx+MirGv4VhKQGkZzGYzl156Kb/+9a8jHQoAL7/8cqRDiEsqkXsQQvCnP/2JzMzMSIeiKLrq1KkT1157baTDoKCgINIhxCWVyBuwWq3cdddd3HPPPYZtSqEokdCrVy8efPBBrrzySiyWyKyXl5ycHJHrxrsWsfphMDIyMrjpppuQUrJx40YWL15MWVlZWK4dLWvCKPFHCEG/fv3o168fNpuNzz//nO3bt9db3M1If/zjH8NynZZGJfJmCCEYNGgQgwYNAuDIkSMcPHiQjRs3cvDgwWbP79ChAxMmTKC4uJjly5dTWFjY7DmqsivhkJCQUDfe3OFwsGfPHgoKClizZg2VlZXNnj9u3Di6du3Kvn37WLZsGTabrcnjBw4ciNls1iV2pT7dFs0KRKQWFjJKZWUl+/fvZ+/evTgcDrp06UJeXh6pqamkp6c3ea7L5eKDDz6gtLSU66+/ntTU1DBFHb/0XjQrEPFWt4uKitizZw9HjhwhOTmZPn36kJiYSHZ2drMDA44ePcoHH3xAp06duPjii8MUcXzzVbdVIlfijkrkSrwyZPVDRVEUJfJUIlcURYlxKpEriqLEOJXIFUVRYpxK5IqiKDEuIqNWhBBFwP6wX1hfucCxSAcRRrH0fDtLKSOy0Hwc1O1Y+jvrJZaes9e6HZFEHg+EEGsiNcQtElra822pWuLfOR6es+paURRFiXEqkSuKosQ4lciD91KkAwizlvZ8W6qW+HeO+ees+sgVRVFinGqRK4qixDiVyAMkhJgghNghhNglhLg/0vEYQQjxqhDiqBBis8dj2UKIb4UQO92fsyIZo6I/Vbdjt26rRB4AIYQZeBa4EOgLXCuE6BvZqAzxGjChwWP3AwullD2Bhe7vlTih6nZs122VyAMzAtglpdwjpbQB7wCTIxyT7qSUS4DiBg9PBua4v54DXBrOmBTDqbqticm6rRJ5YDoAv3h8f9D9WEvQRkp5GMD9OS/C8Sj6UnWb2K3bKpEHRnh5TA37UeKBqtsxTCXywBwEOnp8nw8cilAs4VYohGgH4P58NMLxKPpSdZvYrdsqkQdmNdBTCNFVCJEATAE+i3BM4fIZMM399TTg0wjGouhP1W1NTNZtNSEoQEKIicAswAy8KqV8NLIR6U8I8TYwDm1VuELgb8AnwHtAJ+AAcJWUsuFNIyWGqbodu3VbJXJFUZQYp7pWFEVRYpxK5IqiKDFOJXJFUZQYpxK5oihKjFOJXFEUJcapRK4oihLjVCJXFEWJcSqRK4qixLj/D/M1qxR3TduKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Batch: 010 | Loss: 32.619 | Rec-Loss: 0.883 | Dist-Loss: 21.755 | Classification-Loss: 0.000\n",
      "Epoch: 05 | Batch: 020 | Loss: 31.114 | Rec-Loss: 0.858 | Dist-Loss: 21.136 | Classification-Loss: 0.000\n",
      "Epoch: 05 | Batch: 030 | Loss: 20.514 | Rec-Loss: 0.866 | Dist-Loss: 13.675 | Classification-Loss: 0.000\n",
      "Epoch: 05 Loss: 1026.476 | Rec-Loss: 33.336 | Dist-Loss: 684.455 | Classification-Loss: 0.000\n",
      "Epoch: 06 | NMI: 0.063 | ARI: 0.035\n",
      "Epoch: 06 | NMI: 0.063 | ARI: 0.035\n",
      "Epoch: 06 | Batch: 010 | Loss: 25.860 | Rec-Loss: 0.954 | Dist-Loss: 17.025 | Classification-Loss: 0.000\n",
      "Epoch: 06 | Batch: 020 | Loss: 14.027 | Rec-Loss: 0.897 | Dist-Loss: 9.796 | Classification-Loss: 0.000\n",
      "Epoch: 06 | Batch: 030 | Loss: 30.111 | Rec-Loss: 0.972 | Dist-Loss: 20.140 | Classification-Loss: 0.000\n",
      "Epoch: 06 Loss: 924.010 | Rec-Loss: 32.915 | Dist-Loss: 616.273 | Classification-Loss: 0.000\n",
      "Epoch: 07 | NMI: 0.063 | ARI: 0.032\n",
      "Epoch: 07 | NMI: 0.063 | ARI: 0.032\n",
      "Epoch: 07 | Batch: 010 | Loss: 33.375 | Rec-Loss: 0.754 | Dist-Loss: 22.329 | Classification-Loss: 0.000\n",
      "Epoch: 07 | Batch: 020 | Loss: 41.111 | Rec-Loss: 0.925 | Dist-Loss: 27.590 | Classification-Loss: 0.000\n",
      "Epoch: 07 | Batch: 030 | Loss: 15.042 | Rec-Loss: 0.894 | Dist-Loss: 10.058 | Classification-Loss: 0.000\n",
      "Epoch: 07 Loss: 819.129 | Rec-Loss: 32.523 | Dist-Loss: 545.343 | Classification-Loss: 0.000\n",
      "Epoch: 08 | NMI: 0.062 | ARI: 0.035\n",
      "Epoch: 08 | NMI: 0.062 | ARI: 0.035\n",
      "Epoch: 08 | Batch: 010 | Loss: 17.381 | Rec-Loss: 0.982 | Dist-Loss: 11.805 | Classification-Loss: 0.000\n",
      "Epoch: 08 | Batch: 020 | Loss: 15.179 | Rec-Loss: 0.903 | Dist-Loss: 10.107 | Classification-Loss: 0.000\n",
      "Epoch: 08 | Batch: 030 | Loss: 25.557 | Rec-Loss: 0.879 | Dist-Loss: 16.770 | Classification-Loss: 0.000\n",
      "Epoch: 08 Loss: 734.290 | Rec-Loss: 32.493 | Dist-Loss: 486.543 | Classification-Loss: 0.000\n",
      "Epoch: 09 | NMI: 0.063 | ARI: 0.033\n",
      "Epoch: 09 | NMI: 0.063 | ARI: 0.033\n",
      "Epoch: 09 | Batch: 010 | Loss: 10.188 | Rec-Loss: 0.877 | Dist-Loss: 6.617 | Classification-Loss: 0.000\n",
      "Epoch: 09 | Batch: 020 | Loss: 33.790 | Rec-Loss: 0.941 | Dist-Loss: 22.127 | Classification-Loss: 0.000\n",
      "Epoch: 09 | Batch: 030 | Loss: 22.182 | Rec-Loss: 0.818 | Dist-Loss: 14.850 | Classification-Loss: 0.000\n",
      "Epoch: 09 Loss: 657.147 | Rec-Loss: 32.905 | Dist-Loss: 435.648 | Classification-Loss: 0.000\n",
      "Epoch: 10 | NMI: 0.059 | ARI: 0.032\n",
      "Epoch: 10 | NMI: 0.059 | ARI: 0.032\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Batch: 010 | Loss: 11.498 | Rec-Loss: 0.840 | Dist-Loss: 7.827 | Classification-Loss: 0.000\n",
      "Epoch: 10 | Batch: 020 | Loss: 18.503 | Rec-Loss: 0.913 | Dist-Loss: 12.273 | Classification-Loss: 0.000\n",
      "Epoch: 10 | Batch: 030 | Loss: 11.800 | Rec-Loss: 1.009 | Dist-Loss: 7.668 | Classification-Loss: 0.000\n",
      "Epoch: 10 Loss: 563.065 | Rec-Loss: 32.615 | Dist-Loss: 371.382 | Classification-Loss: 0.000\n",
      "Epoch: 11 | NMI: 0.061 | ARI: 0.036\n",
      "Epoch: 11 | NMI: 0.061 | ARI: 0.036\n",
      "Epoch: 11 | Batch: 010 | Loss: 23.528 | Rec-Loss: 1.058 | Dist-Loss: 15.378 | Classification-Loss: 0.000\n",
      "Epoch: 11 | Batch: 020 | Loss: 16.103 | Rec-Loss: 0.861 | Dist-Loss: 10.634 | Classification-Loss: 0.000\n",
      "Epoch: 11 | Batch: 030 | Loss: 10.859 | Rec-Loss: 0.841 | Dist-Loss: 7.059 | Classification-Loss: 0.000\n",
      "Epoch: 11 Loss: 510.796 | Rec-Loss: 32.772 | Dist-Loss: 335.907 | Classification-Loss: 0.000\n",
      "Epoch: 12 | NMI: 0.060 | ARI: 0.032\n",
      "Epoch: 12 | NMI: 0.060 | ARI: 0.032\n",
      "Epoch: 12 | Batch: 010 | Loss: 30.626 | Rec-Loss: 0.878 | Dist-Loss: 20.207 | Classification-Loss: 0.000\n",
      "Epoch: 12 | Batch: 020 | Loss: 6.755 | Rec-Loss: 0.902 | Dist-Loss: 4.363 | Classification-Loss: 0.000\n",
      "Epoch: 12 | Batch: 030 | Loss: 9.419 | Rec-Loss: 1.014 | Dist-Loss: 6.313 | Classification-Loss: 0.000\n",
      "Epoch: 12 Loss: 506.236 | Rec-Loss: 32.679 | Dist-Loss: 333.216 | Classification-Loss: 0.000\n",
      "Epoch: 13 | NMI: 0.061 | ARI: 0.032\n",
      "Epoch: 13 | NMI: 0.061 | ARI: 0.032\n",
      "Epoch: 13 | Batch: 010 | Loss: 7.796 | Rec-Loss: 0.943 | Dist-Loss: 5.034 | Classification-Loss: 0.000\n",
      "Epoch: 13 | Batch: 020 | Loss: 19.747 | Rec-Loss: 1.039 | Dist-Loss: 13.303 | Classification-Loss: 0.000\n",
      "Epoch: 13 | Batch: 030 | Loss: 8.550 | Rec-Loss: 0.901 | Dist-Loss: 5.767 | Classification-Loss: 0.000\n",
      "Epoch: 13 Loss: 467.712 | Rec-Loss: 32.647 | Dist-Loss: 306.063 | Classification-Loss: 0.000\n",
      "Epoch: 14 | NMI: 0.061 | ARI: 0.033\n",
      "Epoch: 14 | NMI: 0.061 | ARI: 0.033\n",
      "Epoch: 14 | Batch: 010 | Loss: 8.610 | Rec-Loss: 0.924 | Dist-Loss: 5.577 | Classification-Loss: 0.000\n",
      "Epoch: 14 | Batch: 020 | Loss: 7.681 | Rec-Loss: 0.827 | Dist-Loss: 4.989 | Classification-Loss: 0.000\n",
      "Epoch: 14 | Batch: 030 | Loss: 20.921 | Rec-Loss: 0.970 | Dist-Loss: 13.778 | Classification-Loss: 0.000\n",
      "Epoch: 14 Loss: 410.608 | Rec-Loss: 32.525 | Dist-Loss: 267.665 | Classification-Loss: 0.000\n",
      "Epoch: 15 | NMI: 0.060 | ARI: 0.033\n",
      "Epoch: 15 | NMI: 0.060 | ARI: 0.033\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Batch: 010 | Loss: 12.519 | Rec-Loss: 0.830 | Dist-Loss: 8.136 | Classification-Loss: 0.000\n",
      "Epoch: 15 | Batch: 020 | Loss: 4.448 | Rec-Loss: 0.824 | Dist-Loss: 2.912 | Classification-Loss: 0.000\n",
      "Epoch: 15 | Batch: 030 | Loss: 7.328 | Rec-Loss: 0.885 | Dist-Loss: 4.703 | Classification-Loss: 0.000\n",
      "Epoch: 15 Loss: 363.994 | Rec-Loss: 32.547 | Dist-Loss: 236.524 | Classification-Loss: 0.000\n",
      "Epoch: 16 | NMI: 0.060 | ARI: 0.032\n",
      "Epoch: 16 | NMI: 0.060 | ARI: 0.032\n",
      "Epoch: 16 | Batch: 010 | Loss: 6.074 | Rec-Loss: 0.900 | Dist-Loss: 4.116 | Classification-Loss: 0.000\n",
      "Epoch: 16 | Batch: 020 | Loss: 28.513 | Rec-Loss: 0.831 | Dist-Loss: 18.772 | Classification-Loss: 0.000\n",
      "Epoch: 16 | Batch: 030 | Loss: 8.432 | Rec-Loss: 0.879 | Dist-Loss: 5.360 | Classification-Loss: 0.000\n",
      "Epoch: 16 Loss: 394.954 | Rec-Loss: 32.335 | Dist-Loss: 256.505 | Classification-Loss: 0.000\n",
      "Epoch: 17 | NMI: 0.058 | ARI: 0.031\n",
      "Epoch: 17 | NMI: 0.058 | ARI: 0.031\n",
      "Epoch: 17 | Batch: 010 | Loss: 11.912 | Rec-Loss: 1.031 | Dist-Loss: 7.667 | Classification-Loss: 0.000\n",
      "Epoch: 17 | Batch: 020 | Loss: 14.150 | Rec-Loss: 0.920 | Dist-Loss: 9.274 | Classification-Loss: 0.000\n",
      "Epoch: 17 | Batch: 030 | Loss: 4.021 | Rec-Loss: 0.889 | Dist-Loss: 2.349 | Classification-Loss: 0.000\n",
      "Epoch: 17 Loss: 373.885 | Rec-Loss: 33.112 | Dist-Loss: 241.168 | Classification-Loss: 0.000\n",
      "Epoch: 18 | NMI: 0.061 | ARI: 0.033\n",
      "Epoch: 18 | NMI: 0.061 | ARI: 0.033\n",
      "Epoch: 18 | Batch: 010 | Loss: 16.429 | Rec-Loss: 0.813 | Dist-Loss: 11.098 | Classification-Loss: 0.000\n",
      "Epoch: 18 | Batch: 020 | Loss: 9.723 | Rec-Loss: 0.914 | Dist-Loss: 6.456 | Classification-Loss: 0.000\n",
      "Epoch: 18 | Batch: 030 | Loss: 4.594 | Rec-Loss: 0.824 | Dist-Loss: 2.713 | Classification-Loss: 0.000\n",
      "Epoch: 18 Loss: 300.468 | Rec-Loss: 32.656 | Dist-Loss: 192.302 | Classification-Loss: 0.000\n",
      "Epoch: 19 | NMI: 0.061 | ARI: 0.034\n",
      "Epoch: 19 | NMI: 0.061 | ARI: 0.034\n",
      "Epoch: 19 | Batch: 010 | Loss: 27.462 | Rec-Loss: 0.856 | Dist-Loss: 18.000 | Classification-Loss: 0.000\n",
      "Epoch: 19 | Batch: 020 | Loss: 5.233 | Rec-Loss: 0.876 | Dist-Loss: 3.205 | Classification-Loss: 0.000\n",
      "Epoch: 19 | Batch: 030 | Loss: 5.322 | Rec-Loss: 1.014 | Dist-Loss: 3.200 | Classification-Loss: 0.000\n",
      "Epoch: 19 Loss: 324.761 | Rec-Loss: 32.309 | Dist-Loss: 208.262 | Classification-Loss: 0.000\n",
      "Epoch: 20 | NMI: 0.059 | ARI: 0.031\n",
      "Epoch: 20 | NMI: 0.059 | ARI: 0.031\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Batch: 010 | Loss: 7.572 | Rec-Loss: 1.101 | Dist-Loss: 4.716 | Classification-Loss: 0.000\n",
      "Epoch: 20 | Batch: 020 | Loss: 7.150 | Rec-Loss: 0.931 | Dist-Loss: 4.308 | Classification-Loss: 0.000\n",
      "Epoch: 20 | Batch: 030 | Loss: 3.230 | Rec-Loss: 0.858 | Dist-Loss: 1.901 | Classification-Loss: 0.000\n",
      "Epoch: 20 Loss: 302.666 | Rec-Loss: 31.952 | Dist-Loss: 193.350 | Classification-Loss: 0.000\n",
      "Epoch: 21 | NMI: 0.061 | ARI: 0.033\n",
      "Epoch: 21 | NMI: 0.061 | ARI: 0.033\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model.args = args\n",
    "# model.clustering = initial_clustering\n",
    "reducer = umap.UMAP()\n",
    "for e in range(args.epoch):\n",
    "    # Print training set\n",
    "    if e%5 == 0:\n",
    "        out = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "        cluster_id = model.clustering.update_assign(out.cpu().detach().numpy())\n",
    "        \n",
    "        X2 = reducer.fit_transform(out.cpu().detach().numpy())\n",
    "#         X2 = out.cpu().detach().numpy()\n",
    "\n",
    "#         X_centers = reducer.transform(model.clustering.clusters)\n",
    "\n",
    "        c_clusters = [color[int(cluster_id[i])] for i in range(len(cluster_id))]\n",
    "        c_labels = [color[int(y_train[i])] for i in range(len(cluster_id))]\n",
    "#         plt.scatter(latent_X[:,0], latent_X[:,1], color=c_train); plt.show()\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.suptitle('Clusters vs Labels')\n",
    "        ax1.scatter(X2[:,0], X2[:,1], color=c_clusters)\n",
    "#         ax1.plot(X_centers[0], marker='x', markersize=3, color=\"green\")\n",
    "#         ax1.plot(X_centers[1], marker='x', markersize=3, color=\"green\")\n",
    "\n",
    "        ax2.scatter(X2[:,0], X2[:,1], color=c_labels)\n",
    "        plt.show()\n",
    "\n",
    "#         # Print testset\n",
    "#         out = model.autoencoder(torch.FloatTensor(np.array(X_test)).to(args.device), latent=True)\n",
    "#         test_cluster_id = model.clustering.update_assign(out.cpu().detach().numpy())\n",
    "#         X_t = reducer.transform(out.cpu().detach().numpy())\n",
    "#         X_t = out.cpu().detach().numpy()\n",
    "\n",
    "#         c_clusters = [color[int(test_cluster_id[i])] for i in range(len(test_cluster_id))]\n",
    "#         c_test = [color[int(y_test[i])] for i in range(len(y_test))]\n",
    "\n",
    "#         figure = plt.figure()\n",
    "#         fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#         fig.suptitle('CAC Testing Clusters vs Testing Embeddings')\n",
    "#         ax1.scatter(X_t[:,0], X_t[:,1], color=c_clusters)\n",
    "#         ax2.scatter(X_t[:,0], X_t[:,1], color=c_test)\n",
    "#         plt.show()\n",
    "\n",
    "    model.train()\n",
    "    model.fit(e, train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    out = evaluate(model, test_loader)  # evaluation on the test_loader\n",
    "    if len(out) > 2:\n",
    "        NMI, ARI, base_f1, base_mcc, base_auc, cac_f1, cac_mcc, cac_auc = out\n",
    "        print('Epoch: {:02d} | NMI: {:.3f} | ARI: {:.3f} | Base_F1: {:.3f} | Base_MCC: {:.3f} | Base_AUC: {:.3f} | CAC_F1: {:.3f} | CAC_MCC: {:.3f} | CAC_AUC: {:.3f}'.format(\n",
    "                e+1, NMI, ARI, base_f1, base_mcc, base_auc, cac_f1, cac_mcc, cac_auc))\n",
    "    else:\n",
    "        NMI, ARI = out\n",
    "        print('Epoch: {:02d} | NMI: {:.3f} | ARI: {:.3f}'.format(\n",
    "                e+1, NMI, ARI))\n",
    "    nmi_list.append(NMI)\n",
    "    ari_list.append(ARI)\n",
    "    print('Epoch: {:02d} | NMI: {:.3f} | ARI: {:.3f}'.format(\n",
    "            e+1, NMI, ARI))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(NNClassifier, self).__init__()\n",
    "        self.args = args\n",
    "        self.input_dim = args.latent_dim\n",
    "        self.n_classes = args.n_classes\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "#         self.train_loss = None\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, args.n_classes)\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.classifier.parameters(), lr=args.lr)\n",
    "#         self.dropout = nn.Dropout(p=0.1)\n",
    "#         self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "#         self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.classifier(inputs)\n",
    "\n",
    "    def fit(self, X_batch, y_batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.classifier.train()\n",
    "        y_pred = self.forward(X_batch.detach())\n",
    "        train_loss = self.criterion(y_pred, y_batch)\n",
    "        train_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return y_pred.detach().numpy(), train_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot(latent_X, y_train, latents_test, y_test, plot_test=False):\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    cluster_id_train = model.clustering.update_assign(latents_X.cpu().detach().numpy())\n",
    "    X2 = reducer.fit_transform(latents_X.cpu().detach().numpy())\n",
    "\n",
    "    print(\"Training data\")\n",
    "\n",
    "    c_clusters = [color[int(cluster_id_train[i])] for i in range(len(cluster_id_train))]\n",
    "    c_labels = [color[int(y_train[i])] for i in range(len(cluster_id_train))]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle('Clusters vs Labels')\n",
    "    ax1.scatter(X2[:,0], X2[:,1], color=c_clusters)\n",
    "    ax2.scatter(X2[:,0], X2[:,1], color=c_labels)\n",
    "    plt.show()\n",
    "    \n",
    "    if plot_test:\n",
    "        X2 = reducer.transform(latents_test.cpu().detach().numpy())\n",
    "        cluster_id_test = model.clustering.update_assign(latents_test.cpu().detach().numpy())\n",
    "        c_clusters = [color[int(cluster_id_test[i])] for i in range(len(cluster_id_test))]\n",
    "        c_labels = [color[int(y_test[i])] for i in range(len(cluster_id_test))]\n",
    "\n",
    "        print(\"Test data\")\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.suptitle('Clusters vs Labels')\n",
    "        ax1.scatter(X2[:,0], X2[:,1], color=c_clusters)\n",
    "        ax2.scatter(X2[:,0], X2[:,1], color=c_labels)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch wise local network training (Soft clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 1.27622 | Train F1: 0.012 | Train Acc: 0.609| Test F1: 0.000 | Test Acc: 0.717 | Test Loss: 1.297\n",
      "Epoch 002: | Train Loss: 1.10736 | Train F1: 0.000 | Train Acc: 0.709| Test F1: 0.000 | Test Acc: 0.719 | Test Loss: 1.297\n",
      "Epoch 003: | Train Loss: 1.10676 | Train F1: 0.000 | Train Acc: 0.714| Test F1: 0.000 | Test Acc: 0.721 | Test Loss: 1.297\n",
      "Epoch 004: | Train Loss: 1.10669 | Train F1: 0.000 | Train Acc: 0.719| Test F1: 0.000 | Test Acc: 0.724 | Test Loss: 1.296\n",
      "Epoch 005: | Train Loss: 1.10657 | Train F1: 0.000 | Train Acc: 0.725| Test F1: 0.000 | Test Acc: 0.727 | Test Loss: 1.296\n",
      "Epoch 006: | Train Loss: 1.10642 | Train F1: 0.000 | Train Acc: 0.732| Test F1: 0.000 | Test Acc: 0.731 | Test Loss: 1.296\n",
      "Epoch 007: | Train Loss: 1.10623 | Train F1: 0.000 | Train Acc: 0.741| Test F1: 0.000 | Test Acc: 0.735 | Test Loss: 1.296\n",
      "Epoch 008: | Train Loss: 1.10601 | Train F1: 0.000 | Train Acc: 0.750| Test F1: 0.000 | Test Acc: 0.740 | Test Loss: 1.295\n",
      "Epoch 009: | Train Loss: 1.10577 | Train F1: 0.000 | Train Acc: 0.757| Test F1: 0.000 | Test Acc: 0.744 | Test Loss: 1.295\n",
      "Epoch 010: | Train Loss: 1.10551 | Train F1: 0.000 | Train Acc: 0.765| Test F1: 0.000 | Test Acc: 0.749 | Test Loss: 1.295\n",
      "Epoch 011: | Train Loss: 1.10522 | Train F1: 0.000 | Train Acc: 0.771| Test F1: 0.000 | Test Acc: 0.753 | Test Loss: 1.294\n",
      "Epoch 012: | Train Loss: 1.10489 | Train F1: 0.000 | Train Acc: 0.777| Test F1: 0.000 | Test Acc: 0.757 | Test Loss: 1.293\n",
      "Epoch 013: | Train Loss: 1.10454 | Train F1: 0.000 | Train Acc: 0.782| Test F1: 0.000 | Test Acc: 0.761 | Test Loss: 1.293\n",
      "Epoch 014: | Train Loss: 1.10419 | Train F1: 0.000 | Train Acc: 0.785| Test F1: 0.000 | Test Acc: 0.765 | Test Loss: 1.292\n",
      "Epoch 015: | Train Loss: 1.10372 | Train F1: 0.000 | Train Acc: 0.788| Test F1: 0.000 | Test Acc: 0.768 | Test Loss: 1.291\n",
      "Epoch 016: | Train Loss: 1.10320 | Train F1: 0.000 | Train Acc: 0.789| Test F1: 0.000 | Test Acc: 0.768 | Test Loss: 1.291\n",
      "Epoch 017: | Train Loss: 1.10266 | Train F1: 0.000 | Train Acc: 0.789| Test F1: 0.000 | Test Acc: 0.771 | Test Loss: 1.291\n",
      "Epoch 018: | Train Loss: 1.10206 | Train F1: 0.000 | Train Acc: 0.791| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.291\n",
      "Epoch 019: | Train Loss: 1.10130 | Train F1: 0.000 | Train Acc: 0.793| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.291\n",
      "Epoch 020: | Train Loss: 1.10041 | Train F1: 0.000 | Train Acc: 0.793| Test F1: 0.000 | Test Acc: 0.776 | Test Loss: 1.291\n",
      "Epoch 021: | Train Loss: 1.09950 | Train F1: 0.000 | Train Acc: 0.794| Test F1: 0.000 | Test Acc: 0.778 | Test Loss: 1.292\n",
      "Epoch 022: | Train Loss: 1.09796 | Train F1: 0.000 | Train Acc: 0.794| Test F1: 0.000 | Test Acc: 0.779 | Test Loss: 1.292\n",
      "Epoch 023: | Train Loss: 1.09595 | Train F1: 0.000 | Train Acc: 0.794| Test F1: 0.000 | Test Acc: 0.779 | Test Loss: 1.290\n",
      "Epoch 024: | Train Loss: 1.09210 | Train F1: 0.000 | Train Acc: 0.795| Test F1: 0.000 | Test Acc: 0.781 | Test Loss: 1.285\n",
      "Epoch 025: | Train Loss: 1.08678 | Train F1: 0.000 | Train Acc: 0.793| Test F1: 0.000 | Test Acc: 0.779 | Test Loss: 1.279\n",
      "Epoch 026: | Train Loss: 1.07682 | Train F1: 0.000 | Train Acc: 0.793| Test F1: 0.018 | Test Acc: 0.780 | Test Loss: 1.278\n",
      "Epoch 027: | Train Loss: 1.07517 | Train F1: 0.000 | Train Acc: 0.792| Test F1: 0.027 | Test Acc: 0.778 | Test Loss: 1.274\n",
      "Epoch 028: | Train Loss: 1.06499 | Train F1: 0.000 | Train Acc: 0.793| Test F1: 0.049 | Test Acc: 0.779 | Test Loss: 1.282\n",
      "Epoch 029: | Train Loss: 1.06278 | Train F1: 0.000 | Train Acc: 0.793| Test F1: 0.049 | Test Acc: 0.778 | Test Loss: 1.277\n",
      "Epoch 030: | Train Loss: 1.05991 | Train F1: 0.000 | Train Acc: 0.792| Test F1: 0.049 | Test Acc: 0.776 | Test Loss: 1.276\n",
      "Epoch 031: | Train Loss: 1.05587 | Train F1: 0.002 | Train Acc: 0.793| Test F1: 0.048 | Test Acc: 0.778 | Test Loss: 1.273\n",
      "Epoch 032: | Train Loss: 1.06702 | Train F1: 0.000 | Train Acc: 0.792| Test F1: 0.057 | Test Acc: 0.780 | Test Loss: 1.279\n",
      "Epoch 033: | Train Loss: 1.05381 | Train F1: 0.000 | Train Acc: 0.792| Test F1: 0.048 | Test Acc: 0.777 | Test Loss: 1.277\n",
      "Epoch 034: | Train Loss: 1.05528 | Train F1: 0.000 | Train Acc: 0.793| Test F1: 0.049 | Test Acc: 0.780 | Test Loss: 1.283\n",
      "Epoch 035: | Train Loss: 1.05794 | Train F1: 0.000 | Train Acc: 0.793| Test F1: 0.044 | Test Acc: 0.783 | Test Loss: 1.276\n",
      "Epoch 036: | Train Loss: 1.05268 | Train F1: 0.000 | Train Acc: 0.794| Test F1: 0.048 | Test Acc: 0.782 | Test Loss: 1.276\n",
      "Epoch 037: | Train Loss: 1.04959 | Train F1: 0.002 | Train Acc: 0.794| Test F1: 0.057 | Test Acc: 0.781 | Test Loss: 1.278\n",
      "Epoch 038: | Train Loss: 1.04838 | Train F1: 0.002 | Train Acc: 0.795| Test F1: 0.057 | Test Acc: 0.782 | Test Loss: 1.277\n",
      "Epoch 039: | Train Loss: 1.04785 | Train F1: 0.002 | Train Acc: 0.795| Test F1: 0.048 | Test Acc: 0.782 | Test Loss: 1.277\n",
      "Epoch 040: | Train Loss: 1.04764 | Train F1: 0.002 | Train Acc: 0.795| Test F1: 0.052 | Test Acc: 0.782 | Test Loss: 1.278\n",
      "Epoch 041: | Train Loss: 1.04645 | Train F1: 0.002 | Train Acc: 0.795| Test F1: 0.052 | Test Acc: 0.782 | Test Loss: 1.278\n",
      "Epoch 042: | Train Loss: 1.04591 | Train F1: 0.002 | Train Acc: 0.795| Test F1: 0.052 | Test Acc: 0.782 | Test Loss: 1.277\n",
      "Epoch 043: | Train Loss: 1.04532 | Train F1: 0.002 | Train Acc: 0.795| Test F1: 0.048 | Test Acc: 0.781 | Test Loss: 1.279\n",
      "Epoch 044: | Train Loss: 1.04519 | Train F1: 0.002 | Train Acc: 0.796| Test F1: 0.056 | Test Acc: 0.782 | Test Loss: 1.279\n",
      "Epoch 045: | Train Loss: 1.04454 | Train F1: 0.002 | Train Acc: 0.796| Test F1: 0.052 | Test Acc: 0.781 | Test Loss: 1.279\n",
      "Epoch 046: | Train Loss: 1.04372 | Train F1: 0.002 | Train Acc: 0.796| Test F1: 0.060 | Test Acc: 0.781 | Test Loss: 1.280\n",
      "Epoch 047: | Train Loss: 1.04402 | Train F1: 0.002 | Train Acc: 0.796| Test F1: 0.056 | Test Acc: 0.781 | Test Loss: 1.279\n",
      "Epoch 048: | Train Loss: 1.04289 | Train F1: 0.002 | Train Acc: 0.796| Test F1: 0.060 | Test Acc: 0.781 | Test Loss: 1.280\n",
      "Epoch 049: | Train Loss: 1.04318 | Train F1: 0.002 | Train Acc: 0.796| Test F1: 0.060 | Test Acc: 0.781 | Test Loss: 1.278\n",
      "Epoch 050: | Train Loss: 1.04383 | Train F1: 0.002 | Train Acc: 0.797| Test F1: 0.060 | Test Acc: 0.781 | Test Loss: 1.280\n",
      "Epoch 051: | Train Loss: 1.04157 | Train F1: 0.002 | Train Acc: 0.797| Test F1: 0.060 | Test Acc: 0.781 | Test Loss: 1.285\n",
      "Epoch 052: | Train Loss: 1.04073 | Train F1: 0.002 | Train Acc: 0.797| Test F1: 0.060 | Test Acc: 0.781 | Test Loss: 1.281\n",
      "Epoch 053: | Train Loss: 1.04100 | Train F1: 0.002 | Train Acc: 0.797| Test F1: 0.060 | Test Acc: 0.781 | Test Loss: 1.279\n",
      "Epoch 054: | Train Loss: 1.04046 | Train F1: 0.002 | Train Acc: 0.797| Test F1: 0.056 | Test Acc: 0.781 | Test Loss: 1.277\n",
      "Epoch 055: | Train Loss: 1.04009 | Train F1: 0.002 | Train Acc: 0.797| Test F1: 0.060 | Test Acc: 0.781 | Test Loss: 1.277\n",
      "Epoch 056: | Train Loss: 1.03935 | Train F1: 0.002 | Train Acc: 0.797| Test F1: 0.052 | Test Acc: 0.781 | Test Loss: 1.277\n",
      "Epoch 057: | Train Loss: 1.03895 | Train F1: 0.002 | Train Acc: 0.797| Test F1: 0.052 | Test Acc: 0.782 | Test Loss: 1.275\n",
      "Epoch 058: | Train Loss: 1.03857 | Train F1: 0.002 | Train Acc: 0.796| Test F1: 0.052 | Test Acc: 0.781 | Test Loss: 1.276\n",
      "Epoch 059: | Train Loss: 1.03840 | Train F1: 0.002 | Train Acc: 0.797| Test F1: 0.053 | Test Acc: 0.782 | Test Loss: 1.274\n",
      "Epoch 060: | Train Loss: 1.03801 | Train F1: 0.002 | Train Acc: 0.797| Test F1: 0.053 | Test Acc: 0.782 | Test Loss: 1.274\n",
      "Epoch 061: | Train Loss: 1.03751 | Train F1: 0.002 | Train Acc: 0.797| Test F1: 0.048 | Test Acc: 0.782 | Test Loss: 1.272\n",
      "Epoch 062: | Train Loss: 1.03754 | Train F1: 0.002 | Train Acc: 0.797| Test F1: 0.053 | Test Acc: 0.782 | Test Loss: 1.272\n",
      "Epoch 063: | Train Loss: 1.03696 | Train F1: 0.002 | Train Acc: 0.798| Test F1: 0.049 | Test Acc: 0.782 | Test Loss: 1.269\n",
      "Epoch 064: | Train Loss: 1.03711 | Train F1: 0.002 | Train Acc: 0.798| Test F1: 0.048 | Test Acc: 0.782 | Test Loss: 1.271\n",
      "Epoch 065: | Train Loss: 1.03628 | Train F1: 0.002 | Train Acc: 0.798| Test F1: 0.048 | Test Acc: 0.782 | Test Loss: 1.271\n",
      "Epoch 066: | Train Loss: 1.03600 | Train F1: 0.000 | Train Acc: 0.798| Test F1: 0.049 | Test Acc: 0.782 | Test Loss: 1.268\n",
      "Epoch 067: | Train Loss: 1.03649 | Train F1: 0.002 | Train Acc: 0.798| Test F1: 0.049 | Test Acc: 0.782 | Test Loss: 1.269\n",
      "Epoch 068: | Train Loss: 1.03580 | Train F1: 0.002 | Train Acc: 0.799| Test F1: 0.049 | Test Acc: 0.782 | Test Loss: 1.269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 069: | Train Loss: 1.03617 | Train F1: 0.002 | Train Acc: 0.799| Test F1: 0.053 | Test Acc: 0.781 | Test Loss: 1.269\n",
      "Epoch 070: | Train Loss: 1.03601 | Train F1: 0.000 | Train Acc: 0.799| Test F1: 0.049 | Test Acc: 0.782 | Test Loss: 1.266\n",
      "Epoch 071: | Train Loss: 1.03687 | Train F1: 0.000 | Train Acc: 0.799| Test F1: 0.049 | Test Acc: 0.782 | Test Loss: 1.268\n",
      "Epoch 072: | Train Loss: 1.03587 | Train F1: 0.000 | Train Acc: 0.799| Test F1: 0.049 | Test Acc: 0.782 | Test Loss: 1.266\n",
      "Epoch 073: | Train Loss: 1.03689 | Train F1: 0.000 | Train Acc: 0.800| Test F1: 0.049 | Test Acc: 0.782 | Test Loss: 1.266\n",
      "Epoch 074: | Train Loss: 1.03614 | Train F1: 0.000 | Train Acc: 0.800| Test F1: 0.049 | Test Acc: 0.783 | Test Loss: 1.265\n",
      "Epoch 075: | Train Loss: 1.03542 | Train F1: 0.000 | Train Acc: 0.800| Test F1: 0.049 | Test Acc: 0.782 | Test Loss: 1.266\n",
      "Epoch 076: | Train Loss: 1.03438 | Train F1: 0.000 | Train Acc: 0.799| Test F1: 0.049 | Test Acc: 0.783 | Test Loss: 1.265\n",
      "Epoch 077: | Train Loss: 1.03367 | Train F1: 0.000 | Train Acc: 0.799| Test F1: 0.049 | Test Acc: 0.783 | Test Loss: 1.266\n",
      "Epoch 078: | Train Loss: 1.03396 | Train F1: 0.000 | Train Acc: 0.800| Test F1: 0.049 | Test Acc: 0.783 | Test Loss: 1.267\n",
      "Epoch 079: | Train Loss: 1.03316 | Train F1: 0.000 | Train Acc: 0.800| Test F1: 0.049 | Test Acc: 0.783 | Test Loss: 1.266\n",
      "Epoch 080: | Train Loss: 1.03319 | Train F1: 0.000 | Train Acc: 0.800| Test F1: 0.049 | Test Acc: 0.783 | Test Loss: 1.267\n",
      "Epoch 081: | Train Loss: 1.03288 | Train F1: 0.000 | Train Acc: 0.800| Test F1: 0.049 | Test Acc: 0.783 | Test Loss: 1.267\n",
      "Epoch 082: | Train Loss: 1.03290 | Train F1: 0.000 | Train Acc: 0.800| Test F1: 0.049 | Test Acc: 0.783 | Test Loss: 1.266\n",
      "Epoch 083: | Train Loss: 1.03246 | Train F1: 0.000 | Train Acc: 0.801| Test F1: 0.049 | Test Acc: 0.783 | Test Loss: 1.267\n",
      "Epoch 084: | Train Loss: 1.03244 | Train F1: 0.000 | Train Acc: 0.801| Test F1: 0.049 | Test Acc: 0.783 | Test Loss: 1.267\n",
      "Epoch 085: | Train Loss: 1.03197 | Train F1: 0.000 | Train Acc: 0.801| Test F1: 0.049 | Test Acc: 0.783 | Test Loss: 1.266\n",
      "Epoch 086: | Train Loss: 1.03187 | Train F1: 0.002 | Train Acc: 0.801| Test F1: 0.049 | Test Acc: 0.784 | Test Loss: 1.266\n",
      "Epoch 087: | Train Loss: 1.03326 | Train F1: 0.000 | Train Acc: 0.801| Test F1: 0.049 | Test Acc: 0.783 | Test Loss: 1.268\n",
      "Epoch 088: | Train Loss: 1.03258 | Train F1: 0.002 | Train Acc: 0.801| Test F1: 0.053 | Test Acc: 0.783 | Test Loss: 1.267\n",
      "Epoch 089: | Train Loss: 1.03082 | Train F1: 0.002 | Train Acc: 0.801| Test F1: 0.053 | Test Acc: 0.783 | Test Loss: 1.265\n",
      "Epoch 090: | Train Loss: 1.03097 | Train F1: 0.002 | Train Acc: 0.801| Test F1: 0.053 | Test Acc: 0.783 | Test Loss: 1.266\n",
      "Epoch 091: | Train Loss: 1.03121 | Train F1: 0.002 | Train Acc: 0.801| Test F1: 0.053 | Test Acc: 0.783 | Test Loss: 1.267\n",
      "Epoch 092: | Train Loss: 1.03114 | Train F1: 0.002 | Train Acc: 0.802| Test F1: 0.053 | Test Acc: 0.783 | Test Loss: 1.268\n",
      "Epoch 093: | Train Loss: 1.03128 | Train F1: 0.002 | Train Acc: 0.801| Test F1: 0.053 | Test Acc: 0.783 | Test Loss: 1.266\n",
      "Epoch 094: | Train Loss: 1.03081 | Train F1: 0.002 | Train Acc: 0.801| Test F1: 0.053 | Test Acc: 0.783 | Test Loss: 1.265\n",
      "Epoch 095: | Train Loss: 1.03062 | Train F1: 0.002 | Train Acc: 0.801| Test F1: 0.057 | Test Acc: 0.783 | Test Loss: 1.267\n",
      "Epoch 096: | Train Loss: 1.03012 | Train F1: 0.002 | Train Acc: 0.801| Test F1: 0.057 | Test Acc: 0.783 | Test Loss: 1.265\n",
      "Epoch 097: | Train Loss: 1.03022 | Train F1: 0.002 | Train Acc: 0.801| Test F1: 0.057 | Test Acc: 0.783 | Test Loss: 1.264\n",
      "Epoch 098: | Train Loss: 1.03026 | Train F1: 0.002 | Train Acc: 0.802| Test F1: 0.053 | Test Acc: 0.783 | Test Loss: 1.265\n",
      "Epoch 099: | Train Loss: 1.02981 | Train F1: 0.002 | Train Acc: 0.802| Test F1: 0.049 | Test Acc: 0.783 | Test Loss: 1.265\n",
      "Epoch 100: | Train Loss: 1.02955 | Train F1: 0.000 | Train Acc: 0.801| Test F1: 0.040 | Test Acc: 0.784 | Test Loss: 1.262\n",
      "Epoch 101: | Train Loss: 1.02949 | Train F1: 0.000 | Train Acc: 0.802| Test F1: 0.040 | Test Acc: 0.784 | Test Loss: 1.265\n",
      "Epoch 102: | Train Loss: 1.02998 | Train F1: 0.002 | Train Acc: 0.801| Test F1: 0.040 | Test Acc: 0.783 | Test Loss: 1.268\n",
      "Epoch 103: | Train Loss: 1.02938 | Train F1: 0.002 | Train Acc: 0.802| Test F1: 0.040 | Test Acc: 0.784 | Test Loss: 1.267\n",
      "Epoch 104: | Train Loss: 1.02908 | Train F1: 0.002 | Train Acc: 0.802| Test F1: 0.040 | Test Acc: 0.784 | Test Loss: 1.264\n",
      "Epoch 105: | Train Loss: 1.02914 | Train F1: 0.002 | Train Acc: 0.802| Test F1: 0.040 | Test Acc: 0.784 | Test Loss: 1.264\n",
      "Epoch 106: | Train Loss: 1.02901 | Train F1: 0.002 | Train Acc: 0.802| Test F1: 0.040 | Test Acc: 0.784 | Test Loss: 1.263\n",
      "Epoch 107: | Train Loss: 1.02930 | Train F1: 0.002 | Train Acc: 0.802| Test F1: 0.040 | Test Acc: 0.784 | Test Loss: 1.264\n",
      "Epoch 108: | Train Loss: 1.03041 | Train F1: 0.002 | Train Acc: 0.802| Test F1: 0.045 | Test Acc: 0.784 | Test Loss: 1.263\n",
      "Epoch 109: | Train Loss: 1.02935 | Train F1: 0.002 | Train Acc: 0.802| Test F1: 0.040 | Test Acc: 0.784 | Test Loss: 1.264\n",
      "Epoch 110: | Train Loss: 1.02858 | Train F1: 0.002 | Train Acc: 0.802| Test F1: 0.049 | Test Acc: 0.783 | Test Loss: 1.264\n",
      "Epoch 111: | Train Loss: 1.02873 | Train F1: 0.002 | Train Acc: 0.802| Test F1: 0.040 | Test Acc: 0.783 | Test Loss: 1.263\n",
      "Epoch 112: | Train Loss: 1.02851 | Train F1: 0.002 | Train Acc: 0.802| Test F1: 0.040 | Test Acc: 0.783 | Test Loss: 1.262\n",
      "Epoch 113: | Train Loss: 1.02843 | Train F1: 0.002 | Train Acc: 0.802| Test F1: 0.040 | Test Acc: 0.784 | Test Loss: 1.263\n",
      "Epoch 114: | Train Loss: 1.02841 | Train F1: 0.002 | Train Acc: 0.803| Test F1: 0.049 | Test Acc: 0.784 | Test Loss: 1.264\n",
      "Epoch 115: | Train Loss: 1.02823 | Train F1: 0.000 | Train Acc: 0.803| Test F1: 0.041 | Test Acc: 0.784 | Test Loss: 1.261\n",
      "Epoch 116: | Train Loss: 1.02854 | Train F1: 0.000 | Train Acc: 0.802| Test F1: 0.040 | Test Acc: 0.784 | Test Loss: 1.263\n",
      "Epoch 117: | Train Loss: 1.02836 | Train F1: 0.002 | Train Acc: 0.803| Test F1: 0.040 | Test Acc: 0.784 | Test Loss: 1.263\n",
      "Epoch 118: | Train Loss: 1.02803 | Train F1: 0.002 | Train Acc: 0.803| Test F1: 0.040 | Test Acc: 0.784 | Test Loss: 1.262\n",
      "Epoch 119: | Train Loss: 1.02800 | Train F1: 0.002 | Train Acc: 0.803| Test F1: 0.045 | Test Acc: 0.785 | Test Loss: 1.263\n",
      "Epoch 120: | Train Loss: 1.02773 | Train F1: 0.002 | Train Acc: 0.803| Test F1: 0.036 | Test Acc: 0.786 | Test Loss: 1.263\n",
      "Epoch 121: | Train Loss: 1.02785 | Train F1: 0.002 | Train Acc: 0.803| Test F1: 0.045 | Test Acc: 0.785 | Test Loss: 1.263\n",
      "Epoch 122: | Train Loss: 1.02768 | Train F1: 0.002 | Train Acc: 0.803| Test F1: 0.036 | Test Acc: 0.786 | Test Loss: 1.262\n",
      "Epoch 123: | Train Loss: 1.02812 | Train F1: 0.002 | Train Acc: 0.801| Test F1: 0.045 | Test Acc: 0.785 | Test Loss: 1.263\n",
      "Epoch 124: | Train Loss: 1.02790 | Train F1: 0.000 | Train Acc: 0.803| Test F1: 0.041 | Test Acc: 0.784 | Test Loss: 1.261\n",
      "Epoch 125: | Train Loss: 1.02767 | Train F1: 0.000 | Train Acc: 0.802| Test F1: 0.036 | Test Acc: 0.784 | Test Loss: 1.262\n",
      "Epoch 126: | Train Loss: 1.02787 | Train F1: 0.002 | Train Acc: 0.803| Test F1: 0.036 | Test Acc: 0.785 | Test Loss: 1.263\n",
      "Epoch 127: | Train Loss: 1.02744 | Train F1: 0.002 | Train Acc: 0.803| Test F1: 0.040 | Test Acc: 0.786 | Test Loss: 1.263\n",
      "Epoch 128: | Train Loss: 1.02720 | Train F1: 0.002 | Train Acc: 0.803| Test F1: 0.032 | Test Acc: 0.786 | Test Loss: 1.262\n",
      "Epoch 129: | Train Loss: 1.02728 | Train F1: 0.002 | Train Acc: 0.803| Test F1: 0.040 | Test Acc: 0.785 | Test Loss: 1.263\n",
      "Epoch 130: | Train Loss: 1.02720 | Train F1: 0.002 | Train Acc: 0.803| Test F1: 0.036 | Test Acc: 0.786 | Test Loss: 1.262\n",
      "Epoch 131: | Train Loss: 1.02719 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.031 | Test Acc: 0.786 | Test Loss: 1.263\n",
      "Epoch 132: | Train Loss: 1.02709 | Train F1: 0.000 | Train Acc: 0.803| Test F1: 0.027 | Test Acc: 0.786 | Test Loss: 1.261\n",
      "Epoch 133: | Train Loss: 1.02716 | Train F1: 0.000 | Train Acc: 0.803| Test F1: 0.027 | Test Acc: 0.786 | Test Loss: 1.261\n",
      "Epoch 134: | Train Loss: 1.02732 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.786 | Test Loss: 1.262\n",
      "Epoch 135: | Train Loss: 1.02682 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.787 | Test Loss: 1.263\n",
      "Epoch 136: | Train Loss: 1.02670 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.786 | Test Loss: 1.263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137: | Train Loss: 1.02680 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.036 | Test Acc: 0.786 | Test Loss: 1.264\n",
      "Epoch 138: | Train Loss: 1.02692 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.785 | Test Loss: 1.263\n",
      "Epoch 139: | Train Loss: 1.02679 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.036 | Test Acc: 0.786 | Test Loss: 1.263\n",
      "Epoch 140: | Train Loss: 1.02731 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.027 | Test Acc: 0.786 | Test Loss: 1.262\n",
      "Epoch 141: | Train Loss: 1.02664 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.036 | Test Acc: 0.786 | Test Loss: 1.263\n",
      "Epoch 142: | Train Loss: 1.02669 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.786 | Test Loss: 1.263\n",
      "Epoch 143: | Train Loss: 1.02638 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.787 | Test Loss: 1.264\n",
      "Epoch 144: | Train Loss: 1.02691 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.786 | Test Loss: 1.261\n",
      "Epoch 145: | Train Loss: 1.02668 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.036 | Test Acc: 0.786 | Test Loss: 1.263\n",
      "Epoch 146: | Train Loss: 1.02595 | Train F1: 0.000 | Train Acc: 0.804| Test F1: 0.027 | Test Acc: 0.786 | Test Loss: 1.262\n",
      "Epoch 147: | Train Loss: 1.02609 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.787 | Test Loss: 1.262\n",
      "Epoch 148: | Train Loss: 1.02574 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.787 | Test Loss: 1.264\n",
      "Epoch 149: | Train Loss: 1.02663 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.787 | Test Loss: 1.264\n",
      "Epoch 150: | Train Loss: 1.02621 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.031 | Test Acc: 0.787 | Test Loss: 1.261\n",
      "Epoch 151: | Train Loss: 1.02598 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.786 | Test Loss: 1.264\n",
      "Epoch 152: | Train Loss: 1.02541 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.036 | Test Acc: 0.786 | Test Loss: 1.264\n",
      "Epoch 153: | Train Loss: 1.02661 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.786 | Test Loss: 1.262\n",
      "Epoch 154: | Train Loss: 1.02532 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.036 | Test Acc: 0.786 | Test Loss: 1.262\n",
      "Epoch 155: | Train Loss: 1.02640 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.786 | Test Loss: 1.259\n",
      "Epoch 156: | Train Loss: 1.02556 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.036 | Test Acc: 0.786 | Test Loss: 1.263\n",
      "Epoch 157: | Train Loss: 1.02632 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.786 | Test Loss: 1.260\n",
      "Epoch 158: | Train Loss: 1.02591 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.786 | Test Loss: 1.263\n",
      "Epoch 159: | Train Loss: 1.02520 | Train F1: 0.000 | Train Acc: 0.804| Test F1: 0.027 | Test Acc: 0.786 | Test Loss: 1.262\n",
      "Epoch 160: | Train Loss: 1.02466 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.785 | Test Loss: 1.261\n",
      "Epoch 161: | Train Loss: 1.02483 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.784 | Test Loss: 1.260\n",
      "Epoch 162: | Train Loss: 1.02548 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.785 | Test Loss: 1.259\n",
      "Epoch 163: | Train Loss: 1.02507 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.785 | Test Loss: 1.262\n",
      "Epoch 164: | Train Loss: 1.02479 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.036 | Test Acc: 0.785 | Test Loss: 1.265\n",
      "Epoch 165: | Train Loss: 1.02684 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.785 | Test Loss: 1.261\n",
      "Epoch 166: | Train Loss: 1.02494 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.785 | Test Loss: 1.263\n",
      "Epoch 167: | Train Loss: 1.02468 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.040 | Test Acc: 0.786 | Test Loss: 1.265\n",
      "Epoch 168: | Train Loss: 1.02413 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.036 | Test Acc: 0.786 | Test Loss: 1.263\n",
      "Epoch 169: | Train Loss: 1.02450 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.040 | Test Acc: 0.786 | Test Loss: 1.258\n",
      "Epoch 170: | Train Loss: 1.02493 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.040 | Test Acc: 0.786 | Test Loss: 1.261\n",
      "Epoch 171: | Train Loss: 1.02426 | Train F1: 0.000 | Train Acc: 0.804| Test F1: 0.027 | Test Acc: 0.785 | Test Loss: 1.259\n",
      "Epoch 172: | Train Loss: 1.02422 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.785 | Test Loss: 1.262\n",
      "Epoch 173: | Train Loss: 1.02393 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.032 | Test Acc: 0.786 | Test Loss: 1.261\n",
      "Epoch 174: | Train Loss: 1.02438 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.027 | Test Acc: 0.786 | Test Loss: 1.260\n",
      "Epoch 175: | Train Loss: 1.02433 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.027 | Test Acc: 0.786 | Test Loss: 1.261\n",
      "Epoch 176: | Train Loss: 1.02519 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.031 | Test Acc: 0.786 | Test Loss: 1.269\n",
      "Epoch 177: | Train Loss: 1.02706 | Train F1: 0.002 | Train Acc: 0.804| Test F1: 0.027 | Test Acc: 0.786 | Test Loss: 1.276\n",
      "Epoch 178: | Train Loss: 1.02431 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.027 | Test Acc: 0.786 | Test Loss: 1.265\n",
      "Epoch 179: | Train Loss: 1.02381 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.040 | Test Acc: 0.785 | Test Loss: 1.264\n",
      "Epoch 180: | Train Loss: 1.02637 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.045 | Test Acc: 0.785 | Test Loss: 1.281\n",
      "Epoch 181: | Train Loss: 1.02404 | Train F1: 0.000 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.784 | Test Loss: 1.262\n",
      "Epoch 182: | Train Loss: 1.02420 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.032 | Test Acc: 0.785 | Test Loss: 1.263\n",
      "Epoch 183: | Train Loss: 1.02322 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.040 | Test Acc: 0.785 | Test Loss: 1.264\n",
      "Epoch 184: | Train Loss: 1.02478 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.040 | Test Acc: 0.785 | Test Loss: 1.266\n",
      "Epoch 185: | Train Loss: 1.02712 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.036 | Test Acc: 0.785 | Test Loss: 1.269\n",
      "Epoch 186: | Train Loss: 1.02298 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.040 | Test Acc: 0.786 | Test Loss: 1.266\n",
      "Epoch 187: | Train Loss: 1.02361 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.032 | Test Acc: 0.786 | Test Loss: 1.262\n",
      "Epoch 188: | Train Loss: 1.02349 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.036 | Test Acc: 0.786 | Test Loss: 1.264\n",
      "Epoch 189: | Train Loss: 1.02332 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.027 | Test Acc: 0.786 | Test Loss: 1.261\n",
      "Epoch 190: | Train Loss: 1.02389 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.041 | Test Acc: 0.785 | Test Loss: 1.263\n",
      "Epoch 191: | Train Loss: 1.02328 | Train F1: 0.000 | Train Acc: 0.804| Test F1: 0.032 | Test Acc: 0.784 | Test Loss: 1.264\n",
      "Epoch 192: | Train Loss: 1.02306 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.032 | Test Acc: 0.785 | Test Loss: 1.262\n",
      "Epoch 193: | Train Loss: 1.02291 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.041 | Test Acc: 0.785 | Test Loss: 1.265\n",
      "Epoch 194: | Train Loss: 1.02359 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.036 | Test Acc: 0.786 | Test Loss: 1.264\n",
      "Epoch 195: | Train Loss: 1.02504 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.032 | Test Acc: 0.786 | Test Loss: 1.262\n",
      "Epoch 196: | Train Loss: 1.02357 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.036 | Test Acc: 0.786 | Test Loss: 1.257\n",
      "Epoch 197: | Train Loss: 1.02360 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.036 | Test Acc: 0.786 | Test Loss: 1.263\n",
      "Epoch 198: | Train Loss: 1.02392 | Train F1: 0.002 | Train Acc: 0.805| Test F1: 0.027 | Test Acc: 0.786 | Test Loss: 1.264\n"
     ]
    }
   ],
   "source": [
    "# Training separate models\n",
    "args.lr = 0.02\n",
    "classifiers = [NNClassifier(args) for _ in range(args.n_clusters)]\n",
    "\n",
    "# optimizers = [torch.optim.Adam(classifiers[i].classifier.parameters(), lr=args.lr) for i in range(args.n_clusters)]\n",
    "EPOCHS = 200\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "\n",
    "\n",
    "latents_X = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id_train = model.clustering.update_assign(latents_X.cpu().detach().numpy())\n",
    "X_latents_data_loader = list(zip(latents_X, cluster_id_train, y_train))\n",
    "train_loader_latents = torch.utils.data.DataLoader(X_latents_data_loader,\n",
    "    batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "latents_test = model.autoencoder(torch.FloatTensor(np.array(X_test)).to(args.device), latent=True)\n",
    "cluster_id_test = model.clustering.update_assign(latents_test.cpu().detach().numpy())\n",
    "# plot(latents_X, y_train, latents_test, y_test)\n",
    "\n",
    "\n",
    "for e in range(1, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    alpha = (1-e/EPOCHS)\n",
    "    acc = 0\n",
    "#     plot(latents_X, y_train, latents_test, y_test)\n",
    "\n",
    "    for X_batch, cluster_batch, y_batch in train_loader_latents:\n",
    "        q = 1.0 / (1.0 + torch.sum(\n",
    "            torch.pow(X_batch.detach().unsqueeze(1) - model.clustering.clusters, 2), 2) / args.alpha)\n",
    "        q = q.pow((args.alpha + 1.0) / 2.0)\n",
    "        q = (q.t() / torch.sum(q, 1)).t()\n",
    "\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        new_y_batch = []\n",
    "        y_pred = np.zeros((len(X_batch), 2))\n",
    "        for k in range(args.n_clusters):\n",
    "#             idx = np.where(cluster_batch == k)[0]\n",
    "            y_pred_idx, loss = classifiers[k].fit(X_batch, y_batch)\n",
    "            y_pred += q[:,k].numpy().reshape(-1,1)*y_pred_idx\n",
    "#             new_y_batch.append(y_batch.detach().numpy())\n",
    "            epoch_loss += loss\n",
    "        y_pred = y_pred/args.n_clusters\n",
    "#         print(y_pred)\n",
    "#         y_pred = np.hstack(y_pred)\n",
    "#         new_y_batch = np.hstack(new_y_batch)\n",
    "        \n",
    "        f1 = f1_score(np.argmax(y_pred, axis=1), y_batch)\n",
    "        acc = roc_auc_score(y_batch, y_pred[:,1])\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_f1 += f1.item()\n",
    "\n",
    "    test_preds = []\n",
    "    test_loss = 0.0\n",
    "    new_y_test = []\n",
    "    \n",
    "    # Epoch Testing\n",
    "#     for k in range(args.n_clusters):\n",
    "#         classifiers[k].classifier.eval()\n",
    "#         idx = np.where(cluster_id_test == k)[0]\n",
    "#         latents_idx = latents_test[idx]\n",
    "#         y_pred_idx = classifiers[k](latents_idx)\n",
    "#         test_loss += nn.CrossEntropyLoss(reduction='mean')(y_pred_idx, torch.tensor(y_test[idx]).to(device))\n",
    "#         test_preds.append(y_pred_idx.detach().numpy())\n",
    "#         new_y_test.append(y_test[idx])\n",
    "    q = 1.0 / (1.0 + torch.sum(\n",
    "        torch.pow(latents_test.detach().unsqueeze(1) - model.clustering.clusters, 2), 2) / args.alpha)\n",
    "    q = q.pow((args.alpha + 1.0) / 2.0)\n",
    "    q = (q.t() / torch.sum(q, 1)).t()\n",
    "    \n",
    "    y_pred = np.zeros((len(X_test), 2))\n",
    "    for k in range(args.n_clusters):\n",
    "        classifiers[k].classifier.eval()\n",
    "#         idx = np.where(cluster_id_test == k)[0]\n",
    "        latents_idx = latents_test\n",
    "        y_pred_idx = classifiers[k](latents_idx)\n",
    "#         print(q.shape, y_pred_idx.shape)\n",
    "        y_pred += q[:,k].numpy().reshape(-1,1)*y_pred_idx.detach().numpy()\n",
    "\n",
    "        test_loss += nn.CrossEntropyLoss(reduction='mean')(torch.tensor(y_pred).to(device), torch.tensor(y_test).to(device))\n",
    "#         test_preds.append(y_pred)\n",
    "#         new_y_test.append(y_test[idx])\n",
    "#     test_preds = np.vstack(test_preds)\n",
    "#     new_y_test = np.hstack(new_y_test).reshape(-1)\n",
    "    test_f1 = f1_score(np.argmax(y_pred, axis=1), y_test)\n",
    "    y_pred = y_pred/args.n_clusters\n",
    "    test_acc = roc_auc_score(y_test, y_pred[:,1])\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {epoch_loss/len(train_loader):.5f} | Train F1: {epoch_f1/len(train_loader):.3f} | Train Acc: {epoch_acc/len(train_loader):.3f}| Test F1: {test_f1:.3f} | Test Acc: {test_acc:.3f} | Test Loss: {test_loss:.3f}')\n",
    "\n",
    "out = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id = model.clustering.update_assign(out.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Local Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 1.35185 | Train F1: 0.376 | Train Acc: 0.496| Test F1: 0.321 | Test Acc: 0.782 | Test Loss: 1.358\n",
      "Epoch 002: | Train Loss: 1.35211 | Train F1: 0.376 | Train Acc: 0.496| Test F1: 0.319 | Test Acc: 0.782 | Test Loss: 1.356\n",
      "Epoch 003: | Train Loss: 1.35155 | Train F1: 0.376 | Train Acc: 0.496| Test F1: 0.321 | Test Acc: 0.783 | Test Loss: 1.357\n",
      "Epoch 004: | Train Loss: 1.35170 | Train F1: 0.376 | Train Acc: 0.496| Test F1: 0.321 | Test Acc: 0.783 | Test Loss: 1.356\n",
      "Epoch 005: | Train Loss: 1.35102 | Train F1: 0.376 | Train Acc: 0.496| Test F1: 0.321 | Test Acc: 0.783 | Test Loss: 1.357\n",
      "Epoch 006: | Train Loss: 1.35163 | Train F1: 0.377 | Train Acc: 0.496| Test F1: 0.321 | Test Acc: 0.783 | Test Loss: 1.355\n",
      "Epoch 007: | Train Loss: 1.35074 | Train F1: 0.377 | Train Acc: 0.496| Test F1: 0.321 | Test Acc: 0.783 | Test Loss: 1.357\n",
      "Epoch 008: | Train Loss: 1.35151 | Train F1: 0.377 | Train Acc: 0.496| Test F1: 0.321 | Test Acc: 0.784 | Test Loss: 1.354\n",
      "Epoch 009: | Train Loss: 1.35077 | Train F1: 0.377 | Train Acc: 0.496| Test F1: 0.321 | Test Acc: 0.784 | Test Loss: 1.355\n",
      "Epoch 010: | Train Loss: 1.35111 | Train F1: 0.377 | Train Acc: 0.496| Test F1: 0.321 | Test Acc: 0.783 | Test Loss: 1.354\n",
      "Epoch 011: | Train Loss: 1.35058 | Train F1: 0.377 | Train Acc: 0.496| Test F1: 0.321 | Test Acc: 0.784 | Test Loss: 1.354\n",
      "Epoch 012: | Train Loss: 1.35064 | Train F1: 0.378 | Train Acc: 0.496| Test F1: 0.321 | Test Acc: 0.784 | Test Loss: 1.353\n",
      "Epoch 013: | Train Loss: 1.35016 | Train F1: 0.378 | Train Acc: 0.496| Test F1: 0.321 | Test Acc: 0.784 | Test Loss: 1.355\n",
      "Epoch 014: | Train Loss: 1.35027 | Train F1: 0.379 | Train Acc: 0.496| Test F1: 0.321 | Test Acc: 0.784 | Test Loss: 1.354\n",
      "Epoch 015: | Train Loss: 1.34974 | Train F1: 0.379 | Train Acc: 0.496| Test F1: 0.321 | Test Acc: 0.784 | Test Loss: 1.355\n",
      "Epoch 016: | Train Loss: 1.34997 | Train F1: 0.379 | Train Acc: 0.497| Test F1: 0.321 | Test Acc: 0.783 | Test Loss: 1.354\n",
      "Epoch 017: | Train Loss: 1.34947 | Train F1: 0.379 | Train Acc: 0.497| Test F1: 0.321 | Test Acc: 0.783 | Test Loss: 1.355\n",
      "Epoch 018: | Train Loss: 1.34943 | Train F1: 0.380 | Train Acc: 0.497| Test F1: 0.322 | Test Acc: 0.784 | Test Loss: 1.355\n",
      "Epoch 019: | Train Loss: 1.34949 | Train F1: 0.380 | Train Acc: 0.497| Test F1: 0.322 | Test Acc: 0.784 | Test Loss: 1.353\n",
      "Epoch 020: | Train Loss: 1.34916 | Train F1: 0.381 | Train Acc: 0.497| Test F1: 0.322 | Test Acc: 0.784 | Test Loss: 1.354\n",
      "Epoch 021: | Train Loss: 1.34929 | Train F1: 0.382 | Train Acc: 0.497| Test F1: 0.321 | Test Acc: 0.784 | Test Loss: 1.354\n",
      "Epoch 022: | Train Loss: 1.34937 | Train F1: 0.381 | Train Acc: 0.497| Test F1: 0.321 | Test Acc: 0.784 | Test Loss: 1.352\n",
      "Epoch 023: | Train Loss: 1.34891 | Train F1: 0.381 | Train Acc: 0.497| Test F1: 0.321 | Test Acc: 0.784 | Test Loss: 1.353\n",
      "Epoch 024: | Train Loss: 1.34913 | Train F1: 0.381 | Train Acc: 0.497| Test F1: 0.321 | Test Acc: 0.784 | Test Loss: 1.351\n",
      "Epoch 025: | Train Loss: 1.34858 | Train F1: 0.381 | Train Acc: 0.497| Test F1: 0.321 | Test Acc: 0.784 | Test Loss: 1.352\n",
      "Epoch 026: | Train Loss: 1.34849 | Train F1: 0.381 | Train Acc: 0.497| Test F1: 0.321 | Test Acc: 0.784 | Test Loss: 1.353\n",
      "Epoch 027: | Train Loss: 1.34876 | Train F1: 0.381 | Train Acc: 0.497| Test F1: 0.320 | Test Acc: 0.784 | Test Loss: 1.351\n",
      "Epoch 028: | Train Loss: 1.34828 | Train F1: 0.382 | Train Acc: 0.497| Test F1: 0.320 | Test Acc: 0.784 | Test Loss: 1.351\n",
      "Epoch 029: | Train Loss: 1.34797 | Train F1: 0.382 | Train Acc: 0.497| Test F1: 0.321 | Test Acc: 0.784 | Test Loss: 1.352\n",
      "Epoch 030: | Train Loss: 1.34797 | Train F1: 0.382 | Train Acc: 0.497| Test F1: 0.321 | Test Acc: 0.784 | Test Loss: 1.352\n",
      "Epoch 031: | Train Loss: 1.34787 | Train F1: 0.382 | Train Acc: 0.497| Test F1: 0.320 | Test Acc: 0.784 | Test Loss: 1.351\n",
      "Epoch 032: | Train Loss: 1.34784 | Train F1: 0.382 | Train Acc: 0.497| Test F1: 0.320 | Test Acc: 0.784 | Test Loss: 1.351\n",
      "Epoch 033: | Train Loss: 1.34763 | Train F1: 0.383 | Train Acc: 0.497| Test F1: 0.319 | Test Acc: 0.784 | Test Loss: 1.351\n",
      "Epoch 034: | Train Loss: 1.34731 | Train F1: 0.383 | Train Acc: 0.497| Test F1: 0.320 | Test Acc: 0.784 | Test Loss: 1.352\n",
      "Epoch 035: | Train Loss: 1.34741 | Train F1: 0.382 | Train Acc: 0.497| Test F1: 0.320 | Test Acc: 0.784 | Test Loss: 1.352\n",
      "Epoch 036: | Train Loss: 1.34736 | Train F1: 0.382 | Train Acc: 0.497| Test F1: 0.320 | Test Acc: 0.785 | Test Loss: 1.350\n",
      "Epoch 037: | Train Loss: 1.34674 | Train F1: 0.383 | Train Acc: 0.497| Test F1: 0.320 | Test Acc: 0.785 | Test Loss: 1.352\n",
      "Epoch 038: | Train Loss: 1.34719 | Train F1: 0.383 | Train Acc: 0.497| Test F1: 0.320 | Test Acc: 0.785 | Test Loss: 1.350\n",
      "Epoch 039: | Train Loss: 1.34670 | Train F1: 0.383 | Train Acc: 0.497| Test F1: 0.321 | Test Acc: 0.785 | Test Loss: 1.350\n",
      "Epoch 040: | Train Loss: 1.34645 | Train F1: 0.383 | Train Acc: 0.497| Test F1: 0.321 | Test Acc: 0.784 | Test Loss: 1.351\n",
      "Epoch 041: | Train Loss: 1.34670 | Train F1: 0.383 | Train Acc: 0.497| Test F1: 0.321 | Test Acc: 0.785 | Test Loss: 1.350\n",
      "Epoch 042: | Train Loss: 1.34620 | Train F1: 0.383 | Train Acc: 0.497| Test F1: 0.322 | Test Acc: 0.785 | Test Loss: 1.350\n",
      "Epoch 043: | Train Loss: 1.34648 | Train F1: 0.383 | Train Acc: 0.497| Test F1: 0.320 | Test Acc: 0.785 | Test Loss: 1.349\n",
      "Epoch 044: | Train Loss: 1.34599 | Train F1: 0.385 | Train Acc: 0.497| Test F1: 0.321 | Test Acc: 0.785 | Test Loss: 1.350\n",
      "Epoch 045: | Train Loss: 1.34576 | Train F1: 0.384 | Train Acc: 0.497| Test F1: 0.320 | Test Acc: 0.785 | Test Loss: 1.351\n",
      "Epoch 046: | Train Loss: 1.34628 | Train F1: 0.384 | Train Acc: 0.497| Test F1: 0.320 | Test Acc: 0.785 | Test Loss: 1.348\n",
      "Epoch 047: | Train Loss: 1.34551 | Train F1: 0.384 | Train Acc: 0.497| Test F1: 0.320 | Test Acc: 0.786 | Test Loss: 1.349\n",
      "Epoch 048: | Train Loss: 1.34577 | Train F1: 0.384 | Train Acc: 0.497| Test F1: 0.320 | Test Acc: 0.785 | Test Loss: 1.348\n",
      "Epoch 049: | Train Loss: 1.34518 | Train F1: 0.384 | Train Acc: 0.497| Test F1: 0.320 | Test Acc: 0.786 | Test Loss: 1.350\n",
      "Epoch 050: | Train Loss: 1.34565 | Train F1: 0.384 | Train Acc: 0.497| Test F1: 0.322 | Test Acc: 0.786 | Test Loss: 1.348\n",
      "Epoch 051: | Train Loss: 1.34488 | Train F1: 0.384 | Train Acc: 0.497| Test F1: 0.322 | Test Acc: 0.786 | Test Loss: 1.350\n",
      "Epoch 052: | Train Loss: 1.34553 | Train F1: 0.385 | Train Acc: 0.497| Test F1: 0.323 | Test Acc: 0.786 | Test Loss: 1.347\n",
      "Epoch 053: | Train Loss: 1.34474 | Train F1: 0.384 | Train Acc: 0.497| Test F1: 0.323 | Test Acc: 0.786 | Test Loss: 1.349\n",
      "Epoch 054: | Train Loss: 1.34490 | Train F1: 0.384 | Train Acc: 0.497| Test F1: 0.323 | Test Acc: 0.786 | Test Loss: 1.349\n",
      "Epoch 055: | Train Loss: 1.34503 | Train F1: 0.385 | Train Acc: 0.497| Test F1: 0.323 | Test Acc: 0.786 | Test Loss: 1.347\n",
      "Epoch 056: | Train Loss: 1.34465 | Train F1: 0.384 | Train Acc: 0.497| Test F1: 0.323 | Test Acc: 0.786 | Test Loss: 1.348\n",
      "Epoch 057: | Train Loss: 1.34477 | Train F1: 0.384 | Train Acc: 0.497| Test F1: 0.324 | Test Acc: 0.786 | Test Loss: 1.347\n",
      "Epoch 058: | Train Loss: 1.34433 | Train F1: 0.384 | Train Acc: 0.496| Test F1: 0.325 | Test Acc: 0.786 | Test Loss: 1.347\n",
      "Epoch 059: | Train Loss: 1.34448 | Train F1: 0.384 | Train Acc: 0.496| Test F1: 0.327 | Test Acc: 0.786 | Test Loss: 1.347\n",
      "Epoch 060: | Train Loss: 1.34394 | Train F1: 0.384 | Train Acc: 0.496| Test F1: 0.327 | Test Acc: 0.786 | Test Loss: 1.348\n",
      "Epoch 061: | Train Loss: 1.34409 | Train F1: 0.384 | Train Acc: 0.496| Test F1: 0.327 | Test Acc: 0.787 | Test Loss: 1.347\n",
      "Epoch 062: | Train Loss: 1.34385 | Train F1: 0.383 | Train Acc: 0.496| Test F1: 0.327 | Test Acc: 0.787 | Test Loss: 1.348\n",
      "Epoch 063: | Train Loss: 1.34401 | Train F1: 0.383 | Train Acc: 0.496| Test F1: 0.327 | Test Acc: 0.787 | Test Loss: 1.347\n",
      "Epoch 064: | Train Loss: 1.34403 | Train F1: 0.383 | Train Acc: 0.496| Test F1: 0.327 | Test Acc: 0.787 | Test Loss: 1.346\n",
      "Epoch 065: | Train Loss: 1.34336 | Train F1: 0.383 | Train Acc: 0.496| Test F1: 0.327 | Test Acc: 0.787 | Test Loss: 1.348\n",
      "Epoch 066: | Train Loss: 1.34387 | Train F1: 0.383 | Train Acc: 0.496| Test F1: 0.327 | Test Acc: 0.787 | Test Loss: 1.346\n",
      "Epoch 067: | Train Loss: 1.34358 | Train F1: 0.383 | Train Acc: 0.496| Test F1: 0.328 | Test Acc: 0.787 | Test Loss: 1.345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 068: | Train Loss: 1.34304 | Train F1: 0.384 | Train Acc: 0.496| Test F1: 0.328 | Test Acc: 0.787 | Test Loss: 1.347\n",
      "Epoch 069: | Train Loss: 1.34336 | Train F1: 0.383 | Train Acc: 0.496| Test F1: 0.328 | Test Acc: 0.787 | Test Loss: 1.345\n",
      "Epoch 070: | Train Loss: 1.34301 | Train F1: 0.384 | Train Acc: 0.496| Test F1: 0.329 | Test Acc: 0.787 | Test Loss: 1.346\n",
      "Epoch 071: | Train Loss: 1.34334 | Train F1: 0.384 | Train Acc: 0.496| Test F1: 0.329 | Test Acc: 0.787 | Test Loss: 1.344\n",
      "Epoch 072: | Train Loss: 1.34279 | Train F1: 0.384 | Train Acc: 0.496| Test F1: 0.330 | Test Acc: 0.787 | Test Loss: 1.345\n",
      "Epoch 073: | Train Loss: 1.34280 | Train F1: 0.384 | Train Acc: 0.496| Test F1: 0.330 | Test Acc: 0.787 | Test Loss: 1.345\n",
      "Epoch 074: | Train Loss: 1.34274 | Train F1: 0.384 | Train Acc: 0.496| Test F1: 0.330 | Test Acc: 0.787 | Test Loss: 1.345\n",
      "Epoch 075: | Train Loss: 1.34262 | Train F1: 0.384 | Train Acc: 0.496| Test F1: 0.330 | Test Acc: 0.788 | Test Loss: 1.345\n",
      "Epoch 076: | Train Loss: 1.34230 | Train F1: 0.385 | Train Acc: 0.496| Test F1: 0.331 | Test Acc: 0.788 | Test Loss: 1.345\n",
      "Epoch 077: | Train Loss: 1.34236 | Train F1: 0.385 | Train Acc: 0.496| Test F1: 0.331 | Test Acc: 0.788 | Test Loss: 1.345\n",
      "Epoch 078: | Train Loss: 1.34252 | Train F1: 0.385 | Train Acc: 0.496| Test F1: 0.331 | Test Acc: 0.788 | Test Loss: 1.344\n",
      "Epoch 079: | Train Loss: 1.34205 | Train F1: 0.386 | Train Acc: 0.496| Test F1: 0.333 | Test Acc: 0.788 | Test Loss: 1.344\n",
      "Epoch 080: | Train Loss: 1.34199 | Train F1: 0.386 | Train Acc: 0.496| Test F1: 0.334 | Test Acc: 0.789 | Test Loss: 1.344\n",
      "Epoch 081: | Train Loss: 1.34195 | Train F1: 0.387 | Train Acc: 0.496| Test F1: 0.335 | Test Acc: 0.789 | Test Loss: 1.344\n",
      "Epoch 082: | Train Loss: 1.34211 | Train F1: 0.387 | Train Acc: 0.496| Test F1: 0.335 | Test Acc: 0.789 | Test Loss: 1.343\n",
      "Epoch 083: | Train Loss: 1.34198 | Train F1: 0.386 | Train Acc: 0.496| Test F1: 0.336 | Test Acc: 0.789 | Test Loss: 1.342\n",
      "Epoch 084: | Train Loss: 1.34137 | Train F1: 0.387 | Train Acc: 0.496| Test F1: 0.337 | Test Acc: 0.789 | Test Loss: 1.344\n",
      "Epoch 085: | Train Loss: 1.34206 | Train F1: 0.386 | Train Acc: 0.496| Test F1: 0.338 | Test Acc: 0.789 | Test Loss: 1.341\n",
      "Epoch 086: | Train Loss: 1.34140 | Train F1: 0.387 | Train Acc: 0.496| Test F1: 0.340 | Test Acc: 0.789 | Test Loss: 1.342\n",
      "Epoch 087: | Train Loss: 1.34156 | Train F1: 0.387 | Train Acc: 0.496| Test F1: 0.341 | Test Acc: 0.790 | Test Loss: 1.342\n",
      "Epoch 088: | Train Loss: 1.34122 | Train F1: 0.388 | Train Acc: 0.495| Test F1: 0.342 | Test Acc: 0.790 | Test Loss: 1.342\n",
      "Epoch 089: | Train Loss: 1.34107 | Train F1: 0.387 | Train Acc: 0.496| Test F1: 0.344 | Test Acc: 0.790 | Test Loss: 1.342\n",
      "Epoch 090: | Train Loss: 1.34127 | Train F1: 0.388 | Train Acc: 0.495| Test F1: 0.345 | Test Acc: 0.790 | Test Loss: 1.341\n",
      "Epoch 091: | Train Loss: 1.34080 | Train F1: 0.389 | Train Acc: 0.495| Test F1: 0.346 | Test Acc: 0.790 | Test Loss: 1.342\n",
      "Epoch 092: | Train Loss: 1.34122 | Train F1: 0.389 | Train Acc: 0.496| Test F1: 0.346 | Test Acc: 0.790 | Test Loss: 1.340\n",
      "Epoch 093: | Train Loss: 1.34069 | Train F1: 0.389 | Train Acc: 0.495| Test F1: 0.346 | Test Acc: 0.790 | Test Loss: 1.341\n",
      "Epoch 094: | Train Loss: 1.34077 | Train F1: 0.389 | Train Acc: 0.495| Test F1: 0.346 | Test Acc: 0.790 | Test Loss: 1.340\n",
      "Epoch 095: | Train Loss: 1.34050 | Train F1: 0.389 | Train Acc: 0.495| Test F1: 0.346 | Test Acc: 0.790 | Test Loss: 1.341\n",
      "Epoch 096: | Train Loss: 1.34050 | Train F1: 0.389 | Train Acc: 0.496| Test F1: 0.350 | Test Acc: 0.790 | Test Loss: 1.341\n",
      "Epoch 097: | Train Loss: 1.34028 | Train F1: 0.390 | Train Acc: 0.495| Test F1: 0.350 | Test Acc: 0.790 | Test Loss: 1.341\n",
      "Epoch 098: | Train Loss: 1.34070 | Train F1: 0.391 | Train Acc: 0.496| Test F1: 0.351 | Test Acc: 0.790 | Test Loss: 1.339\n",
      "Epoch 099: | Train Loss: 1.33997 | Train F1: 0.390 | Train Acc: 0.495| Test F1: 0.351 | Test Acc: 0.791 | Test Loss: 1.340\n"
     ]
    }
   ],
   "source": [
    "# Training separate models\n",
    "args.lr = 0.02\n",
    "# classifiers = [NNClassifier(args) for _ in range(args.n_clusters)]\n",
    "\n",
    "# optimizers = [torch.optim.Adam(classifiers[i].classifier.parameters(), lr=args.lr) for i in range(args.n_clusters)]\n",
    "EPOCHS = 100\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "\n",
    "\n",
    "latents_X = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id_train = model.clustering.update_assign(latents_X.cpu().detach().numpy())\n",
    "\n",
    "X_latents_data_loader = list(zip(latents_X, cluster_id_train, y_train))\n",
    "train_loader_latents = torch.utils.data.DataLoader(X_latents_data_loader,\n",
    "    batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "latents_test = model.autoencoder(torch.FloatTensor(np.array(X_test)).to(args.device), latent=True)\n",
    "cluster_id_test = model.clustering.update_assign(latents_test.cpu().detach().numpy())\n",
    "# plot(latents_X, y_train, latents_test, y_test)\n",
    "\n",
    "for e in range(1, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    alpha = (1-e/EPOCHS)\n",
    "    acc = 0\n",
    "#     plot(latents_X, y_train, latents_test, y_test)\n",
    "\n",
    "    for X_batch, cluster_batch, y_batch in train_loader_latents:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        new_y_batch = []\n",
    "        y_pred = []\n",
    "        for k in range(args.n_clusters):\n",
    "            idx = np.where(cluster_batch == k)[0]\n",
    "            y_pred_idx, loss = classifiers[k].fit(X_batch[idx], y_batch[idx])\n",
    "#             print(\"F1 Cluster: \", k, f1_score(np.argmax(y_pred_idx, axis=1), y_batch[idx]))\n",
    "#             print(\"Cluster: \", k, len(idx))\n",
    "            new_y_batch.append(y_batch[idx])\n",
    "            y_pred.append(y_pred_idx)\n",
    "            epoch_loss += loss\n",
    "\n",
    "        y_pred = np.vstack(y_pred)\n",
    "        new_y_batch = np.hstack(new_y_batch)\n",
    "#         print(y_pred.shape, new_y_batch, len(X_batch))\n",
    "        f1 = f1_score(np.argmax(y_pred, axis=1), new_y_batch)\n",
    "        acc = roc_auc_score(y_batch, y_pred[:,1])\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_f1 += f1.item()\n",
    "\n",
    "    test_preds = []\n",
    "    test_loss = 0.0\n",
    "    new_y_test = []\n",
    "    \n",
    "    # Epoch Testing\n",
    "    for k in range(args.n_clusters):\n",
    "        classifiers[k].classifier.eval()\n",
    "        idx = np.where(cluster_id_test == k)[0]\n",
    "        latents_idx = latents_test[idx]\n",
    "        y_pred_idx = classifiers[k](latents_idx)\n",
    "        test_loss += nn.CrossEntropyLoss(reduction='mean')(y_pred_idx, torch.tensor(y_test[idx]).to(device))\n",
    "        test_preds.append(y_pred_idx.detach().numpy())\n",
    "        new_y_test.append(y_test[idx])\n",
    "\n",
    "    test_preds = np.vstack(test_preds)\n",
    "    new_y_test = np.hstack(new_y_test).reshape(-1)\n",
    "    test_f1 = f1_score(np.argmax(test_preds, axis=1), new_y_test)\n",
    "    test_acc = roc_auc_score(new_y_test, test_preds[:,1])\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {epoch_loss/len(train_loader):.5f} | Train F1: {epoch_f1/len(train_loader):.3f} | Train Acc: {epoch_acc/len(train_loader):.3f}| Test F1: {test_f1:.3f} | Test Acc: {test_acc:.3f} | Test Loss: {test_loss:.3f}')\n",
    "\n",
    "out = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id = model.clustering.update_assign(out.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Clustering No Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 0.02288 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.189 | Test Acc: 0.752 | Test Loss: 2.181\n",
      "Epoch 002: | Train Loss: 0.02270 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.189 | Test Acc: 0.768 | Test Loss: 2.165\n",
      "Epoch 003: | Train Loss: 0.02254 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.189 | Test Acc: 0.755 | Test Loss: 2.150\n",
      "Epoch 004: | Train Loss: 0.02238 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.253 | Test Acc: 0.755 | Test Loss: 2.134\n",
      "Epoch 005: | Train Loss: 0.02222 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.253 | Test Acc: 0.756 | Test Loss: 2.119\n",
      "Epoch 006: | Train Loss: 0.02206 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.757 | Test Loss: 2.104\n",
      "Epoch 007: | Train Loss: 0.02191 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.756 | Test Loss: 2.089\n",
      "Epoch 008: | Train Loss: 0.02175 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.756 | Test Loss: 2.074\n",
      "Epoch 009: | Train Loss: 0.02159 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.762 | Test Loss: 2.059\n",
      "Epoch 010: | Train Loss: 0.02144 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.762 | Test Loss: 2.044\n",
      "Epoch 011: | Train Loss: 0.02128 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.762 | Test Loss: 2.029\n",
      "Epoch 012: | Train Loss: 0.02113 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.771 | Test Loss: 2.015\n",
      "Epoch 013: | Train Loss: 0.02099 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.765 | Test Loss: 2.001\n",
      "Epoch 014: | Train Loss: 0.02084 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.254 | Test Acc: 0.762 | Test Loss: 1.987\n",
      "Epoch 015: | Train Loss: 0.02070 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.008 | Test Acc: 0.762 | Test Loss: 1.973\n",
      "Epoch 016: | Train Loss: 0.02055 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.004 | Test Acc: 0.763 | Test Loss: 1.958\n",
      "Epoch 017: | Train Loss: 0.02040 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.001 | Test Acc: 0.763 | Test Loss: 1.944\n",
      "Epoch 018: | Train Loss: 0.02026 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.762 | Test Loss: 1.930\n",
      "Epoch 019: | Train Loss: 0.02011 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.761 | Test Loss: 1.915\n",
      "Epoch 020: | Train Loss: 0.01996 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.761 | Test Loss: 1.901\n",
      "Epoch 021: | Train Loss: 0.01981 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.762 | Test Loss: 1.886\n",
      "Epoch 022: | Train Loss: 0.01966 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.763 | Test Loss: 1.871\n",
      "Epoch 023: | Train Loss: 0.01950 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.764 | Test Loss: 1.856\n",
      "Epoch 024: | Train Loss: 0.01935 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.765 | Test Loss: 1.841\n",
      "Epoch 025: | Train Loss: 0.01920 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.765 | Test Loss: 1.827\n",
      "Epoch 026: | Train Loss: 0.01905 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.766 | Test Loss: 1.812\n",
      "Epoch 027: | Train Loss: 0.01889 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.766 | Test Loss: 1.797\n",
      "Epoch 028: | Train Loss: 0.01874 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.767 | Test Loss: 1.782\n",
      "Epoch 029: | Train Loss: 0.01859 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.767 | Test Loss: 1.767\n",
      "Epoch 030: | Train Loss: 0.01844 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.767 | Test Loss: 1.753\n",
      "Epoch 031: | Train Loss: 0.01829 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.768 | Test Loss: 1.739\n",
      "Epoch 032: | Train Loss: 0.01814 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.768 | Test Loss: 1.724\n",
      "Epoch 033: | Train Loss: 0.01800 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.768 | Test Loss: 1.711\n",
      "Epoch 034: | Train Loss: 0.01785 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.769 | Test Loss: 1.697\n",
      "Epoch 035: | Train Loss: 0.01771 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.769 | Test Loss: 1.684\n",
      "Epoch 036: | Train Loss: 0.01758 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.769 | Test Loss: 1.671\n",
      "Epoch 037: | Train Loss: 0.01745 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.769 | Test Loss: 1.659\n",
      "Epoch 038: | Train Loss: 0.01732 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.770 | Test Loss: 1.647\n",
      "Epoch 039: | Train Loss: 0.01719 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.770 | Test Loss: 1.635\n",
      "Epoch 040: | Train Loss: 0.01708 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.770 | Test Loss: 1.624\n",
      "Epoch 041: | Train Loss: 0.01696 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.770 | Test Loss: 1.613\n",
      "Epoch 042: | Train Loss: 0.01685 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.770 | Test Loss: 1.603\n",
      "Epoch 043: | Train Loss: 0.01674 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.771 | Test Loss: 1.593\n",
      "Epoch 044: | Train Loss: 0.01664 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.771 | Test Loss: 1.583\n",
      "Epoch 045: | Train Loss: 0.01654 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.771 | Test Loss: 1.574\n",
      "Epoch 046: | Train Loss: 0.01645 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.771 | Test Loss: 1.565\n",
      "Epoch 047: | Train Loss: 0.01636 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.557\n",
      "Epoch 048: | Train Loss: 0.01627 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.549\n",
      "Epoch 049: | Train Loss: 0.01619 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.541\n",
      "Epoch 050: | Train Loss: 0.01611 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.534\n",
      "Epoch 051: | Train Loss: 0.01603 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.527\n",
      "Epoch 052: | Train Loss: 0.01596 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.520\n",
      "Epoch 053: | Train Loss: 0.01589 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.513\n",
      "Epoch 054: | Train Loss: 0.01582 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.507\n",
      "Epoch 055: | Train Loss: 0.01575 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.501\n",
      "Epoch 056: | Train Loss: 0.01569 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.496\n",
      "Epoch 057: | Train Loss: 0.01563 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.491\n",
      "Epoch 058: | Train Loss: 0.01558 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.486\n",
      "Epoch 059: | Train Loss: 0.01553 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.482\n",
      "Epoch 060: | Train Loss: 0.01548 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.477\n",
      "Epoch 061: | Train Loss: 0.01544 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.474\n",
      "Epoch 062: | Train Loss: 0.01540 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.470\n",
      "Epoch 063: | Train Loss: 0.01536 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.467\n",
      "Epoch 064: | Train Loss: 0.01533 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.465\n",
      "Epoch 065: | Train Loss: 0.01530 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.462\n",
      "Epoch 066: | Train Loss: 0.01527 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.460\n",
      "Epoch 067: | Train Loss: 0.01525 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 068: | Train Loss: 0.01522 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.457\n",
      "Epoch 069: | Train Loss: 0.01521 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.455\n",
      "Epoch 070: | Train Loss: 0.01519 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.454\n",
      "Epoch 071: | Train Loss: 0.01518 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.453\n",
      "Epoch 072: | Train Loss: 0.01517 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.453\n",
      "Epoch 073: | Train Loss: 0.01516 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.452\n",
      "Epoch 074: | Train Loss: 0.01515 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.451\n",
      "Epoch 075: | Train Loss: 0.01514 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.451\n",
      "Epoch 076: | Train Loss: 0.01514 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.451\n",
      "Epoch 077: | Train Loss: 0.01513 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 078: | Train Loss: 0.01513 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 079: | Train Loss: 0.01513 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 080: | Train Loss: 0.01513 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 081: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 082: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 083: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 084: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 085: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 086: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 087: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 088: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 089: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 090: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 091: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.776 | Test Loss: 1.450\n",
      "Epoch 092: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 093: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 094: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 095: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.776 | Test Loss: 1.450\n",
      "Epoch 096: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.776 | Test Loss: 1.450\n",
      "Epoch 097: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 098: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 099: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 100: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 101: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 102: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 103: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 104: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 105: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 106: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 107: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 108: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 109: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.450\n",
      "Epoch 110: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 111: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 112: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 113: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 114: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 115: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 116: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 117: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 118: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 119: | Train Loss: 0.01512 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 120: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 121: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 122: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 123: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 124: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 125: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 126: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 127: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 128: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 129: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 130: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 131: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 132: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.775 | Test Loss: 1.449\n",
      "Epoch 133: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.449\n",
      "Epoch 134: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.449\n",
      "Epoch 135: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.449\n",
      "Epoch 136: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 138: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 139: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 140: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.771 | Test Loss: 1.448\n",
      "Epoch 141: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.771 | Test Loss: 1.448\n",
      "Epoch 142: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.771 | Test Loss: 1.448\n",
      "Epoch 143: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 144: | Train Loss: 0.01511 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 145: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 146: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 147: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 148: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 149: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 150: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 151: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 152: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 153: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.448\n",
      "Epoch 154: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 155: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 156: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 157: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 158: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 159: | Train Loss: 0.01510 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 160: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 161: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 162: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 163: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 164: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 165: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.447\n",
      "Epoch 166: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.446\n",
      "Epoch 167: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.446\n",
      "Epoch 168: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.446\n",
      "Epoch 169: | Train Loss: 0.01509 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.446\n",
      "Epoch 170: | Train Loss: 0.01508 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.446\n",
      "Epoch 171: | Train Loss: 0.01508 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.446\n",
      "Epoch 172: | Train Loss: 0.01508 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.446\n",
      "Epoch 173: | Train Loss: 0.01508 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.772 | Test Loss: 1.446\n",
      "Epoch 174: | Train Loss: 0.01508 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.446\n",
      "Epoch 175: | Train Loss: 0.01508 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.445\n",
      "Epoch 176: | Train Loss: 0.01508 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.445\n",
      "Epoch 177: | Train Loss: 0.01507 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.445\n",
      "Epoch 178: | Train Loss: 0.01507 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.445\n",
      "Epoch 179: | Train Loss: 0.01507 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.445\n",
      "Epoch 180: | Train Loss: 0.01507 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.445\n",
      "Epoch 181: | Train Loss: 0.01507 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.444\n",
      "Epoch 182: | Train Loss: 0.01507 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.444\n",
      "Epoch 183: | Train Loss: 0.01506 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.444\n",
      "Epoch 184: | Train Loss: 0.01506 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.444\n",
      "Epoch 185: | Train Loss: 0.01506 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.444\n",
      "Epoch 186: | Train Loss: 0.01506 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.443\n",
      "Epoch 187: | Train Loss: 0.01505 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.443\n",
      "Epoch 188: | Train Loss: 0.01505 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.443\n",
      "Epoch 189: | Train Loss: 0.01505 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.443\n",
      "Epoch 190: | Train Loss: 0.01505 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.442\n",
      "Epoch 191: | Train Loss: 0.01505 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.442\n",
      "Epoch 192: | Train Loss: 0.01504 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.442\n",
      "Epoch 193: | Train Loss: 0.01504 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.442\n",
      "Epoch 194: | Train Loss: 0.01504 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.441\n",
      "Epoch 195: | Train Loss: 0.01503 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.441\n",
      "Epoch 196: | Train Loss: 0.01503 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.441\n",
      "Epoch 197: | Train Loss: 0.01503 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.441\n",
      "Epoch 198: | Train Loss: 0.01503 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.440\n",
      "Epoch 199: | Train Loss: 0.01502 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.440\n",
      "Epoch 200: | Train Loss: 0.01502 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.440\n",
      "Epoch 201: | Train Loss: 0.01502 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.439\n",
      "Epoch 202: | Train Loss: 0.01501 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.439\n",
      "Epoch 203: | Train Loss: 0.01501 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204: | Train Loss: 0.01500 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.438\n",
      "Epoch 205: | Train Loss: 0.01500 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.438\n",
      "Epoch 206: | Train Loss: 0.01500 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.773 | Test Loss: 1.437\n",
      "Epoch 207: | Train Loss: 0.01499 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.437\n",
      "Epoch 208: | Train Loss: 0.01499 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.437\n",
      "Epoch 209: | Train Loss: 0.01498 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.436\n",
      "Epoch 210: | Train Loss: 0.01498 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.436\n",
      "Epoch 211: | Train Loss: 0.01498 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.435\n",
      "Epoch 212: | Train Loss: 0.01497 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.434\n",
      "Epoch 213: | Train Loss: 0.01496 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.434\n",
      "Epoch 214: | Train Loss: 0.01495 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.433\n",
      "Epoch 215: | Train Loss: 0.01495 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.433\n",
      "Epoch 216: | Train Loss: 0.01494 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.432\n",
      "Epoch 217: | Train Loss: 0.01494 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.000 | Test Acc: 0.774 | Test Loss: 1.431\n",
      "Epoch 218: | Train Loss: 0.01493 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.008 | Test Acc: 0.774 | Test Loss: 1.431\n",
      "Epoch 219: | Train Loss: 0.01492 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.006 | Test Acc: 0.774 | Test Loss: 1.430\n",
      "Epoch 220: | Train Loss: 0.01492 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.010 | Test Acc: 0.774 | Test Loss: 1.429\n",
      "Epoch 221: | Train Loss: 0.01491 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.022 | Test Acc: 0.774 | Test Loss: 1.429\n",
      "Epoch 222: | Train Loss: 0.01490 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.016 | Test Acc: 0.774 | Test Loss: 1.428\n",
      "Epoch 223: | Train Loss: 0.01490 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.043 | Test Acc: 0.774 | Test Loss: 1.427\n",
      "Epoch 224: | Train Loss: 0.01489 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.052 | Test Acc: 0.774 | Test Loss: 1.427\n",
      "Epoch 225: | Train Loss: 0.01488 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.048 | Test Acc: 0.774 | Test Loss: 1.426\n",
      "Epoch 226: | Train Loss: 0.01487 | Train F1: 0.000 | Train Acc: 0.008| Test F1: 0.080 | Test Acc: 0.774 | Test Loss: 1.425\n",
      "Epoch 227: | Train Loss: 0.01487 | Train F1: 0.001 | Train Acc: 0.008| Test F1: 0.071 | Test Acc: 0.774 | Test Loss: 1.424\n",
      "Epoch 228: | Train Loss: 0.01486 | Train F1: 0.001 | Train Acc: 0.008| Test F1: 0.108 | Test Acc: 0.774 | Test Loss: 1.424\n",
      "Epoch 229: | Train Loss: 0.01485 | Train F1: 0.001 | Train Acc: 0.008| Test F1: 0.115 | Test Acc: 0.774 | Test Loss: 1.423\n",
      "Epoch 230: | Train Loss: 0.01484 | Train F1: 0.001 | Train Acc: 0.008| Test F1: 0.114 | Test Acc: 0.774 | Test Loss: 1.422\n",
      "Epoch 231: | Train Loss: 0.01483 | Train F1: 0.001 | Train Acc: 0.008| Test F1: 0.162 | Test Acc: 0.774 | Test Loss: 1.421\n",
      "Epoch 232: | Train Loss: 0.01483 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.129 | Test Acc: 0.774 | Test Loss: 1.421\n",
      "Epoch 233: | Train Loss: 0.01482 | Train F1: 0.001 | Train Acc: 0.008| Test F1: 0.197 | Test Acc: 0.774 | Test Loss: 1.420\n",
      "Epoch 234: | Train Loss: 0.01481 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.162 | Test Acc: 0.775 | Test Loss: 1.419\n",
      "Epoch 235: | Train Loss: 0.01480 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.200 | Test Acc: 0.775 | Test Loss: 1.418\n",
      "Epoch 236: | Train Loss: 0.01479 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.218 | Test Acc: 0.775 | Test Loss: 1.417\n",
      "Epoch 237: | Train Loss: 0.01478 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.197 | Test Acc: 0.775 | Test Loss: 1.417\n",
      "Epoch 238: | Train Loss: 0.01478 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.237 | Test Acc: 0.775 | Test Loss: 1.416\n",
      "Epoch 239: | Train Loss: 0.01477 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.220 | Test Acc: 0.775 | Test Loss: 1.415\n",
      "Epoch 240: | Train Loss: 0.01476 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.238 | Test Acc: 0.775 | Test Loss: 1.414\n",
      "Epoch 241: | Train Loss: 0.01475 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.238 | Test Acc: 0.775 | Test Loss: 1.414\n",
      "Epoch 242: | Train Loss: 0.01474 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.234 | Test Acc: 0.775 | Test Loss: 1.413\n",
      "Epoch 243: | Train Loss: 0.01473 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.245 | Test Acc: 0.775 | Test Loss: 1.412\n",
      "Epoch 244: | Train Loss: 0.01473 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.238 | Test Acc: 0.775 | Test Loss: 1.411\n",
      "Epoch 245: | Train Loss: 0.01472 | Train F1: 0.002 | Train Acc: 0.008| Test F1: 0.248 | Test Acc: 0.775 | Test Loss: 1.410\n",
      "Epoch 246: | Train Loss: 0.01471 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.244 | Test Acc: 0.775 | Test Loss: 1.410\n",
      "Epoch 247: | Train Loss: 0.01470 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.247 | Test Acc: 0.775 | Test Loss: 1.409\n",
      "Epoch 248: | Train Loss: 0.01469 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.252 | Test Acc: 0.775 | Test Loss: 1.408\n",
      "Epoch 249: | Train Loss: 0.01469 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.248 | Test Acc: 0.775 | Test Loss: 1.407\n",
      "Epoch 250: | Train Loss: 0.01468 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.256 | Test Acc: 0.775 | Test Loss: 1.407\n",
      "Epoch 251: | Train Loss: 0.01467 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.250 | Test Acc: 0.775 | Test Loss: 1.406\n",
      "Epoch 252: | Train Loss: 0.01466 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.255 | Test Acc: 0.775 | Test Loss: 1.405\n",
      "Epoch 253: | Train Loss: 0.01466 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.257 | Test Acc: 0.775 | Test Loss: 1.404\n",
      "Epoch 254: | Train Loss: 0.01465 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.253 | Test Acc: 0.775 | Test Loss: 1.404\n",
      "Epoch 255: | Train Loss: 0.01464 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.266 | Test Acc: 0.775 | Test Loss: 1.403\n",
      "Epoch 256: | Train Loss: 0.01463 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.253 | Test Acc: 0.775 | Test Loss: 1.402\n",
      "Epoch 257: | Train Loss: 0.01463 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.268 | Test Acc: 0.775 | Test Loss: 1.402\n",
      "Epoch 258: | Train Loss: 0.01462 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.260 | Test Acc: 0.775 | Test Loss: 1.401\n",
      "Epoch 259: | Train Loss: 0.01461 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.266 | Test Acc: 0.775 | Test Loss: 1.400\n",
      "Epoch 260: | Train Loss: 0.01460 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.272 | Test Acc: 0.776 | Test Loss: 1.399\n",
      "Epoch 261: | Train Loss: 0.01460 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.263 | Test Acc: 0.776 | Test Loss: 1.399\n",
      "Epoch 262: | Train Loss: 0.01459 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.281 | Test Acc: 0.776 | Test Loss: 1.398\n",
      "Epoch 263: | Train Loss: 0.01458 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.264 | Test Acc: 0.776 | Test Loss: 1.397\n",
      "Epoch 264: | Train Loss: 0.01458 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.281 | Test Acc: 0.776 | Test Loss: 1.397\n",
      "Epoch 265: | Train Loss: 0.01457 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.276 | Test Acc: 0.776 | Test Loss: 1.396\n",
      "Epoch 266: | Train Loss: 0.01456 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.276 | Test Acc: 0.776 | Test Loss: 1.395\n",
      "Epoch 267: | Train Loss: 0.01456 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.286 | Test Acc: 0.776 | Test Loss: 1.395\n",
      "Epoch 268: | Train Loss: 0.01455 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.272 | Test Acc: 0.776 | Test Loss: 1.394\n",
      "Epoch 269: | Train Loss: 0.01454 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.291 | Test Acc: 0.776 | Test Loss: 1.394\n",
      "Epoch 270: | Train Loss: 0.01454 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.279 | Test Acc: 0.776 | Test Loss: 1.393\n",
      "Epoch 271: | Train Loss: 0.01453 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.287 | Test Acc: 0.776 | Test Loss: 1.392\n",
      "Epoch 272: | Train Loss: 0.01453 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.290 | Test Acc: 0.776 | Test Loss: 1.392\n",
      "Epoch 273: | Train Loss: 0.01452 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.281 | Test Acc: 0.776 | Test Loss: 1.391\n",
      "Epoch 274: | Train Loss: 0.01451 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.295 | Test Acc: 0.776 | Test Loss: 1.391\n",
      "Epoch 275: | Train Loss: 0.01451 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.281 | Test Acc: 0.776 | Test Loss: 1.390\n",
      "Epoch 276: | Train Loss: 0.01450 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.297 | Test Acc: 0.776 | Test Loss: 1.390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277: | Train Loss: 0.01450 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.288 | Test Acc: 0.776 | Test Loss: 1.389\n",
      "Epoch 278: | Train Loss: 0.01449 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.293 | Test Acc: 0.776 | Test Loss: 1.389\n",
      "Epoch 279: | Train Loss: 0.01449 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.295 | Test Acc: 0.776 | Test Loss: 1.388\n",
      "Epoch 280: | Train Loss: 0.01448 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.290 | Test Acc: 0.776 | Test Loss: 1.388\n",
      "Epoch 281: | Train Loss: 0.01448 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.304 | Test Acc: 0.776 | Test Loss: 1.387\n",
      "Epoch 282: | Train Loss: 0.01447 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.289 | Test Acc: 0.776 | Test Loss: 1.387\n",
      "Epoch 283: | Train Loss: 0.01447 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.312 | Test Acc: 0.776 | Test Loss: 1.386\n",
      "Epoch 284: | Train Loss: 0.01446 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.293 | Test Acc: 0.777 | Test Loss: 1.386\n",
      "Epoch 285: | Train Loss: 0.01446 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.303 | Test Acc: 0.777 | Test Loss: 1.385\n",
      "Epoch 286: | Train Loss: 0.01445 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.299 | Test Acc: 0.777 | Test Loss: 1.385\n",
      "Epoch 287: | Train Loss: 0.01445 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.299 | Test Acc: 0.777 | Test Loss: 1.384\n",
      "Epoch 288: | Train Loss: 0.01444 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.313 | Test Acc: 0.777 | Test Loss: 1.384\n",
      "Epoch 289: | Train Loss: 0.01444 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.296 | Test Acc: 0.777 | Test Loss: 1.384\n",
      "Epoch 290: | Train Loss: 0.01444 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.322 | Test Acc: 0.777 | Test Loss: 1.383\n",
      "Epoch 291: | Train Loss: 0.01443 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.295 | Test Acc: 0.777 | Test Loss: 1.383\n",
      "Epoch 292: | Train Loss: 0.01443 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.324 | Test Acc: 0.777 | Test Loss: 1.383\n",
      "Epoch 293: | Train Loss: 0.01442 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.299 | Test Acc: 0.777 | Test Loss: 1.382\n",
      "Epoch 294: | Train Loss: 0.01442 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.317 | Test Acc: 0.777 | Test Loss: 1.382\n",
      "Epoch 295: | Train Loss: 0.01442 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.318 | Test Acc: 0.777 | Test Loss: 1.381\n",
      "Epoch 296: | Train Loss: 0.01441 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.304 | Test Acc: 0.777 | Test Loss: 1.381\n",
      "Epoch 297: | Train Loss: 0.01441 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.332 | Test Acc: 0.777 | Test Loss: 1.381\n",
      "Epoch 298: | Train Loss: 0.01441 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.302 | Test Acc: 0.777 | Test Loss: 1.380\n",
      "Epoch 299: | Train Loss: 0.01440 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.332 | Test Acc: 0.777 | Test Loss: 1.380\n",
      "Epoch 300: | Train Loss: 0.01440 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.312 | Test Acc: 0.777 | Test Loss: 1.380\n",
      "Epoch 301: | Train Loss: 0.01440 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.323 | Test Acc: 0.777 | Test Loss: 1.380\n",
      "Epoch 302: | Train Loss: 0.01439 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.327 | Test Acc: 0.777 | Test Loss: 1.379\n",
      "Epoch 303: | Train Loss: 0.01439 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.313 | Test Acc: 0.777 | Test Loss: 1.379\n",
      "Epoch 304: | Train Loss: 0.01439 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.338 | Test Acc: 0.777 | Test Loss: 1.379\n",
      "Epoch 305: | Train Loss: 0.01439 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.311 | Test Acc: 0.777 | Test Loss: 1.378\n",
      "Epoch 306: | Train Loss: 0.01438 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.340 | Test Acc: 0.777 | Test Loss: 1.378\n",
      "Epoch 307: | Train Loss: 0.01438 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.319 | Test Acc: 0.777 | Test Loss: 1.378\n",
      "Epoch 308: | Train Loss: 0.01438 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.334 | Test Acc: 0.777 | Test Loss: 1.378\n",
      "Epoch 309: | Train Loss: 0.01437 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.329 | Test Acc: 0.777 | Test Loss: 1.378\n",
      "Epoch 310: | Train Loss: 0.01437 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.322 | Test Acc: 0.777 | Test Loss: 1.377\n",
      "Epoch 311: | Train Loss: 0.01437 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.340 | Test Acc: 0.777 | Test Loss: 1.377\n",
      "Epoch 312: | Train Loss: 0.01437 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.320 | Test Acc: 0.777 | Test Loss: 1.377\n",
      "Epoch 313: | Train Loss: 0.01437 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.347 | Test Acc: 0.777 | Test Loss: 1.377\n",
      "Epoch 314: | Train Loss: 0.01436 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.320 | Test Acc: 0.777 | Test Loss: 1.377\n",
      "Epoch 315: | Train Loss: 0.01436 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.347 | Test Acc: 0.777 | Test Loss: 1.377\n",
      "Epoch 316: | Train Loss: 0.01436 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.322 | Test Acc: 0.777 | Test Loss: 1.376\n",
      "Epoch 317: | Train Loss: 0.01436 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.343 | Test Acc: 0.777 | Test Loss: 1.376\n",
      "Epoch 318: | Train Loss: 0.01436 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.336 | Test Acc: 0.777 | Test Loss: 1.376\n",
      "Epoch 319: | Train Loss: 0.01435 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.336 | Test Acc: 0.777 | Test Loss: 1.376\n",
      "Epoch 320: | Train Loss: 0.01435 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.345 | Test Acc: 0.777 | Test Loss: 1.376\n",
      "Epoch 321: | Train Loss: 0.01435 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.328 | Test Acc: 0.778 | Test Loss: 1.375\n",
      "Epoch 322: | Train Loss: 0.01435 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.351 | Test Acc: 0.778 | Test Loss: 1.375\n",
      "Epoch 323: | Train Loss: 0.01435 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.324 | Test Acc: 0.778 | Test Loss: 1.375\n",
      "Epoch 324: | Train Loss: 0.01435 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.356 | Test Acc: 0.778 | Test Loss: 1.375\n",
      "Epoch 325: | Train Loss: 0.01435 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.326 | Test Acc: 0.778 | Test Loss: 1.375\n",
      "Epoch 326: | Train Loss: 0.01434 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.352 | Test Acc: 0.778 | Test Loss: 1.375\n",
      "Epoch 327: | Train Loss: 0.01434 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.336 | Test Acc: 0.778 | Test Loss: 1.375\n",
      "Epoch 328: | Train Loss: 0.01434 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.345 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 329: | Train Loss: 0.01434 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.345 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 330: | Train Loss: 0.01434 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.336 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 331: | Train Loss: 0.01434 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 332: | Train Loss: 0.01434 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.333 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 333: | Train Loss: 0.01433 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 334: | Train Loss: 0.01433 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.332 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 335: | Train Loss: 0.01433 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 336: | Train Loss: 0.01433 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.336 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 337: | Train Loss: 0.01433 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.355 | Test Acc: 0.778 | Test Loss: 1.374\n",
      "Epoch 338: | Train Loss: 0.01433 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.347 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 339: | Train Loss: 0.01433 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.347 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 340: | Train Loss: 0.01433 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 341: | Train Loss: 0.01433 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.342 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 342: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 343: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.337 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 344: | Train Loss: 0.01432 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 345: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.338 | Test Acc: 0.778 | Test Loss: 1.373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 346: | Train Loss: 0.01432 | Train F1: 0.003 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 347: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.346 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 348: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 349: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.353 | Test Acc: 0.778 | Test Loss: 1.373\n",
      "Epoch 350: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.353 | Test Acc: 0.778 | Test Loss: 1.372\n",
      "Epoch 351: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.778 | Test Loss: 1.372\n",
      "Epoch 352: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.350 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 353: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 354: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.345 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 355: | Train Loss: 0.01432 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.364 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 356: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.344 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 357: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.363 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 358: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.351 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 359: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 360: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.353 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 361: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 362: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.357 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 363: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 364: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 365: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.353 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 366: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.363 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 367: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.351 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 368: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 369: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.348 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 370: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 371: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.345 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 372: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.779 | Test Loss: 1.372\n",
      "Epoch 373: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.348 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 374: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 375: | Train Loss: 0.01431 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 376: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.361 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 377: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 378: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.357 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 379: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 380: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.352 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 381: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 382: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.351 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 383: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 384: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 385: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.364 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 386: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.357 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 387: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 388: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.363 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 389: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 390: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 391: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 392: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 393: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.352 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 394: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 395: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.352 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 396: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 397: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.355 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 398: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 399: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.357 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 400: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.365 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 401: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 402: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.363 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 403: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.362 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 404: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 405: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.365 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 406: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.779 | Test Loss: 1.370\n",
      "Epoch 407: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 408: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.355 | Test Acc: 0.779 | Test Loss: 1.370\n",
      "Epoch 409: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 410: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.355 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 411: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 412: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.351 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 413: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.779 | Test Loss: 1.371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.350 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 415: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.779 | Test Loss: 1.371\n",
      "Epoch 416: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.348 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 417: | Train Loss: 0.01430 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.780 | Test Loss: 1.371\n",
      "Epoch 418: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.352 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 419: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 420: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.362 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 421: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 422: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 423: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 424: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 425: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.355 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 426: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 427: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 428: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 429: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 430: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 431: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 432: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.356 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 433: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 434: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 435: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 436: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.365 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 437: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.365 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 438: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 439: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 440: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 441: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.357 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 442: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 443: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 444: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 445: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 446: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 447: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.365 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 448: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 449: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 450: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.365 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 451: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 452: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.362 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 453: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 454: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 455: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 456: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 457: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 458: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 459: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 460: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.370\n",
      "Epoch 461: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 462: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.370\n",
      "Epoch 463: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 464: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 465: | Train Loss: 0.01429 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.780 | Test Loss: 1.370\n",
      "Epoch 466: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 467: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.781 | Test Loss: 1.370\n",
      "Epoch 468: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 469: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.370\n",
      "Epoch 470: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 471: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.370\n",
      "Epoch 472: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 473: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 474: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.357 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 475: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 476: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.357 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 477: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 478: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 479: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 480: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 481: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 482: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 483: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 484: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.363 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 485: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 486: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.364 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 487: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 488: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 489: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 490: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 491: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 492: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 493: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 494: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 495: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 496: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.364 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 497: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 498: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 499: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 500: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.356 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 501: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 502: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 503: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.380 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 504: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 505: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.380 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 506: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.354 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 507: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 508: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 509: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 510: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 511: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 512: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 513: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.360 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 514: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 515: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 516: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 517: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.364 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 518: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 519: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 520: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 521: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 522: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 523: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 524: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 525: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 526: | Train Loss: 0.01428 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 527: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 528: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.781 | Test Loss: 1.368\n",
      "Epoch 529: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.781 | Test Loss: 1.369\n",
      "Epoch 530: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 531: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 532: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 533: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 534: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 535: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 536: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 537: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 538: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 539: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 540: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 541: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 542: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 543: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 544: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 545: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 546: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 547: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 548: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 549: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 550: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 551: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 552: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 553: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 554: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 555: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 556: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 557: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.365 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 558: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 559: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.362 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 560: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 561: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.362 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 562: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 563: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.362 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 564: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 565: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.366 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 566: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 567: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 568: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 569: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 570: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 571: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 572: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 573: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 574: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 575: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 576: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 577: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 578: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 579: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 580: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 581: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 582: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 583: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 584: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 585: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 586: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.359 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 587: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.380 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 588: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.355 | Test Acc: 0.783 | Test Loss: 1.368\n",
      "Epoch 589: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.387 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 590: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.351 | Test Acc: 0.783 | Test Loss: 1.368\n",
      "Epoch 591: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.388 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 592: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.353 | Test Acc: 0.783 | Test Loss: 1.368\n",
      "Epoch 593: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.381 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 594: | Train Loss: 0.01427 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.782 | Test Loss: 1.367\n",
      "Epoch 595: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.782 | Test Loss: 1.367\n",
      "Epoch 596: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.379 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 597: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.356 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 598: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.382 | Test Acc: 0.782 | Test Loss: 1.368\n",
      "Epoch 599: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.361 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 600: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.782 | Test Loss: 1.367\n",
      "Epoch 601: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.782 | Test Loss: 1.367\n",
      "Epoch 602: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 603: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.379 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 604: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.362 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 605: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 606: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 607: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 608: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 609: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.367 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 610: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.378 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 611: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 612: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 613: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 614: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 615: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 616: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.369 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 617: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 618: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 619: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 620: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 621: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 622: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 623: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.371 | Test Acc: 0.783 | Test Loss: 1.367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 624: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 625: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 626: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 627: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 628: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 629: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.377 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 630: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 631: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 632: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 633: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 634: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 635: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 636: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 637: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 638: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 639: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 640: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 641: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 642: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 643: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 644: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 645: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 646: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 647: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 648: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 649: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 650: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 651: | Train Loss: 0.01426 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 652: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 653: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.367\n",
      "Epoch 654: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 655: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 656: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 657: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 658: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 659: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 660: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 661: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 662: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 663: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 664: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 665: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 666: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 667: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.783 | Test Loss: 1.366\n",
      "Epoch 668: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.375 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 669: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 670: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.378 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 671: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.373 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 672: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.378 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 673: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.372 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 674: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.379 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 675: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 676: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.382 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 677: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.362 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 678: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.387 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 679: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.356 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 680: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.393 | Test Acc: 0.784 | Test Loss: 1.367\n",
      "Epoch 681: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.351 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 682: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.391 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 683: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 684: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.380 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 685: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 686: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 687: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.383 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 688: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.358 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 689: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.383 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 690: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.370 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 691: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.376 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 692: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.380 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 693: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.784 | Test Loss: 1.366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 694: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.382 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 695: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.368 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 696: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.380 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 697: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 698: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.374 | Test Acc: 0.784 | Test Loss: 1.366\n",
      "Epoch 699: | Train Loss: 0.01425 | Train F1: 0.004 | Train Acc: 0.008| Test F1: 0.380 | Test Acc: 0.784 | Test Loss: 1.366\n"
     ]
    }
   ],
   "source": [
    "# Training separate models\n",
    "args.lr = 0.002\n",
    "classifiers = [NNClassifier(args) for _ in range(args.n_clusters)]\n",
    "# optimizers = [torch.optim.Adam(classifiers[i].classifier.parameters(), lr=args.lr) for i in range(args.n_clusters)]\n",
    "EPOCHS = 700\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "\n",
    "latents_X = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id_train = model.clustering.update_assign(latents_X.cpu().detach().numpy())\n",
    "\n",
    "X_latents_data_loader = list(zip(latents_X, cluster_id_train, y_train))\n",
    "train_loader_latents = torch.utils.data.DataLoader(X_latents_data_loader,\n",
    "    batch_size=1024, shuffle=False)\n",
    "\n",
    "latents_test = model.autoencoder(torch.FloatTensor(np.array(X_test)).to(args.device), latent=True)\n",
    "\n",
    "cluster_id_test = model.clustering.update_assign(latents_test.cpu().detach().numpy())\n",
    "\n",
    "# plot(latents_X, y_train, latents_test, y_test)\n",
    "\n",
    "for e in range(1, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    alpha = (1-e/EPOCHS)\n",
    "    acc = 0\n",
    "\n",
    "    new_y = []\n",
    "    y_pred = []\n",
    "    for k in range(args.n_clusters):\n",
    "#         print(\"Training on cluster: \", k)\n",
    "        idx = np.where(cluster_id_train == k)[0]\n",
    "        y_pred_idx, loss = classifiers[k].fit(latents_X[idx], torch.tensor(y_train[idx]).to(device))\n",
    "        y_pred.append(y_pred_idx)\n",
    "        new_y.append(y_train[idx])\n",
    "        epoch_loss += loss\n",
    "#         print(\"Cluster: \", k, \"AUC: \", roc_auc_score(y_train[idx], y_pred_idx[:,1])\\\n",
    "#               , \"loss: \", loss, \"N =\", len(idx))\n",
    "\n",
    "    y_pred = np.vstack(y_pred)\n",
    "    new_y = np.hstack(new_y)\n",
    "\n",
    "    f1 = f1_score(np.argmax(y_pred, axis=1), new_y)\n",
    "    acc = roc_auc_score(new_y, y_pred[:,1])\n",
    "    epoch_f1 += f1.item()\n",
    "    epoch_acc += acc.item()\n",
    "\n",
    "    test_preds = []\n",
    "    test_loss = 0.0\n",
    "    new_y_test = []\n",
    "    for k in range(args.n_clusters):\n",
    "        classifiers[k].classifier.eval()\n",
    "        idx = np.where(cluster_id_test == k)[0]\n",
    "        latents_idx = latents_test[idx]\n",
    "        y_pred_idx = classifiers[k](latents_idx)\n",
    "        test_loss += nn.CrossEntropyLoss(reduction='mean')(y_pred_idx, torch.tensor(y_test[idx]).to(device))\n",
    "        test_preds.append(y_pred_idx.detach().numpy())\n",
    "        new_y_test.append(y_test[idx])\n",
    "#         print(\"Cluster: \", k, \"F1: \", roc_auc_score(y_test[idx], y_pred_idx[:,1].detach().numpy()),\\\n",
    "#               \"loss: \", loss, \"N =\", len(idx))\n",
    "\n",
    "\n",
    "    test_preds = np.vstack(test_preds)\n",
    "    new_y_test = np.hstack(new_y_test).reshape(-1)\n",
    "    test_f1 = f1_score(np.argmax(test_preds, axis=1), new_y_test)\n",
    "    test_acc = roc_auc_score(new_y_test, test_preds[:,1])\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {epoch_loss/len(train_loader):.5f} | Train F1: {epoch_f1/len(train_loader):.3f} | Train Acc: {epoch_acc/len(train_loader):.3f}| Test F1: {test_f1:.3f} | Test Acc: {test_acc:.3f} | Test Loss: {test_loss:.3f}')\n",
    "\n",
    "out = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id = model.clustering.update_assign(out.cpu().detach().numpy())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 0.01571 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.506 | Test Loss: 0.492\n",
      "Epoch 002: | Train Loss: 0.01367 | Train F1: 0.000 | Train Acc: 0.013| Test F1: 0.000 | Test Acc: 0.472 | Test Loss: 0.444\n",
      "Epoch 003: | Train Loss: 0.01234 | Train F1: 0.000 | Train Acc: 0.012| Test F1: 0.000 | Test Acc: 0.510 | Test Loss: 0.414\n",
      "Epoch 004: | Train Loss: 0.01150 | Train F1: 0.000 | Train Acc: 0.013| Test F1: 0.000 | Test Acc: 0.540 | Test Loss: 0.415\n",
      "Epoch 005: | Train Loss: 0.01153 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.524 | Test Loss: 0.432\n",
      "Epoch 006: | Train Loss: 0.01197 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.493 | Test Loss: 0.437\n",
      "Epoch 007: | Train Loss: 0.01206 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.479 | Test Loss: 0.430\n",
      "Epoch 008: | Train Loss: 0.01185 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.478 | Test Loss: 0.421\n",
      "Epoch 009: | Train Loss: 0.01160 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.479 | Test Loss: 0.415\n",
      "Epoch 010: | Train Loss: 0.01143 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.478 | Test Loss: 0.413\n",
      "Epoch 011: | Train Loss: 0.01138 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.479 | Test Loss: 0.414\n",
      "Epoch 012: | Train Loss: 0.01140 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.480 | Test Loss: 0.415\n",
      "Epoch 013: | Train Loss: 0.01145 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.480 | Test Loss: 0.417\n",
      "Epoch 014: | Train Loss: 0.01150 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.479 | Test Loss: 0.417\n",
      "Epoch 015: | Train Loss: 0.01152 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.479 | Test Loss: 0.417\n",
      "Epoch 016: | Train Loss: 0.01152 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.480 | Test Loss: 0.416\n",
      "Epoch 017: | Train Loss: 0.01150 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.481 | Test Loss: 0.415\n",
      "Epoch 018: | Train Loss: 0.01146 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.486 | Test Loss: 0.413\n",
      "Epoch 019: | Train Loss: 0.01142 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.498 | Test Loss: 0.412\n",
      "Epoch 020: | Train Loss: 0.01138 | Train F1: 0.000 | Train Acc: 0.014| Test F1: 0.000 | Test Acc: 0.516 | Test Loss: 0.411\n",
      "Epoch 021: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.527 | Test Loss: 0.410\n",
      "Epoch 022: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.534 | Test Loss: 0.410\n",
      "Epoch 023: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.538 | Test Loss: 0.411\n",
      "Epoch 024: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.540 | Test Loss: 0.411\n",
      "Epoch 025: | Train Loss: 0.01136 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.542 | Test Loss: 0.412\n",
      "Epoch 026: | Train Loss: 0.01138 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.544 | Test Loss: 0.412\n",
      "Epoch 027: | Train Loss: 0.01139 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.545 | Test Loss: 0.412\n",
      "Epoch 028: | Train Loss: 0.01139 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.545 | Test Loss: 0.412\n",
      "Epoch 029: | Train Loss: 0.01138 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.546 | Test Loss: 0.411\n",
      "Epoch 030: | Train Loss: 0.01137 | Train F1: 0.000 | Train Acc: 0.015| Test F1: 0.000 | Test Acc: 0.547 | Test Loss: 0.411\n",
      "Epoch 031: | Train Loss: 0.01136 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.548 | Test Loss: 0.410\n",
      "Epoch 032: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.548 | Test Loss: 0.410\n",
      "Epoch 033: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.549 | Test Loss: 0.410\n",
      "Epoch 034: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.550 | Test Loss: 0.410\n",
      "Epoch 035: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.550 | Test Loss: 0.410\n",
      "Epoch 036: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.551 | Test Loss: 0.410\n",
      "Epoch 037: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.552 | Test Loss: 0.410\n",
      "Epoch 038: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.552 | Test Loss: 0.410\n",
      "Epoch 039: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.553 | Test Loss: 0.410\n",
      "Epoch 040: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.553 | Test Loss: 0.410\n",
      "Epoch 041: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.554 | Test Loss: 0.410\n",
      "Epoch 042: | Train Loss: 0.01135 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.555 | Test Loss: 0.410\n",
      "Epoch 043: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.555 | Test Loss: 0.410\n",
      "Epoch 044: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.556 | Test Loss: 0.410\n",
      "Epoch 045: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.556 | Test Loss: 0.410\n",
      "Epoch 046: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.557 | Test Loss: 0.410\n",
      "Epoch 047: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.557 | Test Loss: 0.410\n",
      "Epoch 048: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.558 | Test Loss: 0.410\n",
      "Epoch 049: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.558 | Test Loss: 0.410\n",
      "Epoch 050: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.559 | Test Loss: 0.410\n",
      "Epoch 051: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.559 | Test Loss: 0.410\n",
      "Epoch 052: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.559 | Test Loss: 0.410\n",
      "Epoch 053: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.560 | Test Loss: 0.410\n",
      "Epoch 054: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.560 | Test Loss: 0.410\n",
      "Epoch 055: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.561 | Test Loss: 0.410\n",
      "Epoch 056: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.561 | Test Loss: 0.410\n",
      "Epoch 057: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.562 | Test Loss: 0.410\n",
      "Epoch 058: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.562 | Test Loss: 0.410\n",
      "Epoch 059: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.562 | Test Loss: 0.410\n",
      "Epoch 060: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.563 | Test Loss: 0.410\n",
      "Epoch 061: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.563 | Test Loss: 0.410\n",
      "Epoch 062: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.564 | Test Loss: 0.410\n",
      "Epoch 063: | Train Loss: 0.01134 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.564 | Test Loss: 0.410\n",
      "Epoch 064: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.564 | Test Loss: 0.410\n",
      "Epoch 065: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.565 | Test Loss: 0.410\n",
      "Epoch 066: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.565 | Test Loss: 0.410\n",
      "Epoch 067: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.566 | Test Loss: 0.410\n",
      "Epoch 068: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.566 | Test Loss: 0.410\n",
      "Epoch 069: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.566 | Test Loss: 0.410\n",
      "Epoch 070: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.567 | Test Loss: 0.410\n",
      "Epoch 071: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.567 | Test Loss: 0.410\n",
      "Epoch 072: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.567 | Test Loss: 0.410\n",
      "Epoch 073: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.567 | Test Loss: 0.410\n",
      "Epoch 074: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.567 | Test Loss: 0.410\n",
      "Epoch 075: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.567 | Test Loss: 0.410\n",
      "Epoch 076: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.567 | Test Loss: 0.410\n",
      "Epoch 077: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.568 | Test Loss: 0.410\n",
      "Epoch 078: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.568 | Test Loss: 0.410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 079: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.568 | Test Loss: 0.410\n",
      "Epoch 080: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.568 | Test Loss: 0.410\n",
      "Epoch 081: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.569 | Test Loss: 0.410\n",
      "Epoch 082: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.570 | Test Loss: 0.410\n",
      "Epoch 083: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.570 | Test Loss: 0.410\n",
      "Epoch 084: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.571 | Test Loss: 0.410\n",
      "Epoch 085: | Train Loss: 0.01133 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.571 | Test Loss: 0.410\n",
      "Epoch 086: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.571 | Test Loss: 0.410\n",
      "Epoch 087: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.572 | Test Loss: 0.409\n",
      "Epoch 088: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.572 | Test Loss: 0.409\n",
      "Epoch 089: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.572 | Test Loss: 0.409\n",
      "Epoch 090: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.572 | Test Loss: 0.409\n",
      "Epoch 091: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.572 | Test Loss: 0.409\n",
      "Epoch 092: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.572 | Test Loss: 0.409\n",
      "Epoch 093: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.573 | Test Loss: 0.409\n",
      "Epoch 094: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.573 | Test Loss: 0.409\n",
      "Epoch 095: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.573 | Test Loss: 0.409\n",
      "Epoch 096: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.573 | Test Loss: 0.409\n",
      "Epoch 097: | Train Loss: 0.01132 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.573 | Test Loss: 0.409\n",
      "Epoch 098: | Train Loss: 0.01131 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.573 | Test Loss: 0.409\n",
      "Epoch 099: | Train Loss: 0.01131 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.574 | Test Loss: 0.409\n",
      "Epoch 100: | Train Loss: 0.01131 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.574 | Test Loss: 0.409\n",
      "Epoch 101: | Train Loss: 0.01131 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.574 | Test Loss: 0.409\n",
      "Epoch 102: | Train Loss: 0.01131 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.574 | Test Loss: 0.409\n",
      "Epoch 103: | Train Loss: 0.01131 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.574 | Test Loss: 0.409\n",
      "Epoch 104: | Train Loss: 0.01131 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.575 | Test Loss: 0.409\n",
      "Epoch 105: | Train Loss: 0.01130 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.575 | Test Loss: 0.409\n",
      "Epoch 106: | Train Loss: 0.01130 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.575 | Test Loss: 0.409\n",
      "Epoch 107: | Train Loss: 0.01130 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.575 | Test Loss: 0.409\n",
      "Epoch 108: | Train Loss: 0.01130 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.575 | Test Loss: 0.409\n",
      "Epoch 109: | Train Loss: 0.01130 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.575 | Test Loss: 0.409\n",
      "Epoch 110: | Train Loss: 0.01129 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.576 | Test Loss: 0.408\n",
      "Epoch 111: | Train Loss: 0.01129 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.576 | Test Loss: 0.408\n",
      "Epoch 112: | Train Loss: 0.01129 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.576 | Test Loss: 0.408\n",
      "Epoch 113: | Train Loss: 0.01129 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.576 | Test Loss: 0.408\n",
      "Epoch 114: | Train Loss: 0.01128 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.577 | Test Loss: 0.408\n",
      "Epoch 115: | Train Loss: 0.01128 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.577 | Test Loss: 0.408\n",
      "Epoch 116: | Train Loss: 0.01128 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.577 | Test Loss: 0.408\n",
      "Epoch 117: | Train Loss: 0.01127 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.577 | Test Loss: 0.408\n",
      "Epoch 118: | Train Loss: 0.01127 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.578 | Test Loss: 0.407\n",
      "Epoch 119: | Train Loss: 0.01127 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.578 | Test Loss: 0.407\n",
      "Epoch 120: | Train Loss: 0.01126 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.578 | Test Loss: 0.407\n",
      "Epoch 121: | Train Loss: 0.01126 | Train F1: 0.000 | Train Acc: 0.016| Test F1: 0.000 | Test Acc: 0.579 | Test Loss: 0.407\n",
      "Epoch 122: | Train Loss: 0.01125 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.579 | Test Loss: 0.407\n",
      "Epoch 123: | Train Loss: 0.01125 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.580 | Test Loss: 0.407\n",
      "Epoch 124: | Train Loss: 0.01124 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.580 | Test Loss: 0.406\n",
      "Epoch 125: | Train Loss: 0.01124 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.582 | Test Loss: 0.406\n",
      "Epoch 126: | Train Loss: 0.01123 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.584 | Test Loss: 0.406\n",
      "Epoch 127: | Train Loss: 0.01123 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.588 | Test Loss: 0.406\n",
      "Epoch 128: | Train Loss: 0.01122 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.593 | Test Loss: 0.405\n",
      "Epoch 129: | Train Loss: 0.01121 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.598 | Test Loss: 0.405\n",
      "Epoch 130: | Train Loss: 0.01121 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.603 | Test Loss: 0.405\n",
      "Epoch 131: | Train Loss: 0.01120 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.610 | Test Loss: 0.404\n",
      "Epoch 132: | Train Loss: 0.01119 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.617 | Test Loss: 0.404\n",
      "Epoch 133: | Train Loss: 0.01118 | Train F1: 0.000 | Train Acc: 0.017| Test F1: 0.000 | Test Acc: 0.622 | Test Loss: 0.404\n",
      "Epoch 134: | Train Loss: 0.01118 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.625 | Test Loss: 0.403\n",
      "Epoch 135: | Train Loss: 0.01117 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.626 | Test Loss: 0.403\n",
      "Epoch 136: | Train Loss: 0.01116 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.628 | Test Loss: 0.403\n",
      "Epoch 137: | Train Loss: 0.01115 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.631 | Test Loss: 0.402\n",
      "Epoch 138: | Train Loss: 0.01114 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.636 | Test Loss: 0.402\n",
      "Epoch 139: | Train Loss: 0.01113 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.637 | Test Loss: 0.401\n",
      "Epoch 140: | Train Loss: 0.01112 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.637 | Test Loss: 0.401\n",
      "Epoch 141: | Train Loss: 0.01111 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.640 | Test Loss: 0.401\n",
      "Epoch 142: | Train Loss: 0.01109 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.643 | Test Loss: 0.400\n",
      "Epoch 143: | Train Loss: 0.01108 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.644 | Test Loss: 0.400\n",
      "Epoch 144: | Train Loss: 0.01107 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.646 | Test Loss: 0.399\n",
      "Epoch 145: | Train Loss: 0.01106 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.647 | Test Loss: 0.399\n",
      "Epoch 146: | Train Loss: 0.01104 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.649 | Test Loss: 0.398\n",
      "Epoch 147: | Train Loss: 0.01103 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.651 | Test Loss: 0.398\n",
      "Epoch 148: | Train Loss: 0.01102 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.652 | Test Loss: 0.397\n",
      "Epoch 149: | Train Loss: 0.01100 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.654 | Test Loss: 0.396\n",
      "Epoch 150: | Train Loss: 0.01099 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.659 | Test Loss: 0.396\n",
      "Epoch 151: | Train Loss: 0.01097 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.653 | Test Loss: 0.395\n",
      "Epoch 152: | Train Loss: 0.01096 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.663 | Test Loss: 0.395\n",
      "Epoch 153: | Train Loss: 0.01094 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.657 | Test Loss: 0.394\n",
      "Epoch 154: | Train Loss: 0.01093 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.662 | Test Loss: 0.394\n",
      "Epoch 155: | Train Loss: 0.01091 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.667 | Test Loss: 0.393\n",
      "Epoch 156: | Train Loss: 0.01089 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.654 | Test Loss: 0.393\n",
      "Epoch 157: | Train Loss: 0.01088 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.672 | Test Loss: 0.392\n",
      "Epoch 158: | Train Loss: 0.01087 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.638 | Test Loss: 0.394\n",
      "Epoch 159: | Train Loss: 0.01089 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.660 | Test Loss: 0.392\n",
      "Epoch 160: | Train Loss: 0.01090 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.648 | Test Loss: 0.391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161: | Train Loss: 0.01083 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.645 | Test Loss: 0.391\n",
      "Epoch 162: | Train Loss: 0.01083 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.661 | Test Loss: 0.390\n",
      "Epoch 163: | Train Loss: 0.01085 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.662 | Test Loss: 0.389\n",
      "Epoch 164: | Train Loss: 0.01079 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.644 | Test Loss: 0.391\n",
      "Epoch 165: | Train Loss: 0.01081 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.667 | Test Loss: 0.389\n",
      "Epoch 166: | Train Loss: 0.01080 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.674 | Test Loss: 0.388\n",
      "Epoch 167: | Train Loss: 0.01076 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.646 | Test Loss: 0.390\n",
      "Epoch 168: | Train Loss: 0.01079 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.675 | Test Loss: 0.388\n",
      "Epoch 169: | Train Loss: 0.01076 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.673 | Test Loss: 0.387\n",
      "Epoch 170: | Train Loss: 0.01076 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.649 | Test Loss: 0.389\n",
      "Epoch 171: | Train Loss: 0.01077 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.673 | Test Loss: 0.387\n",
      "Epoch 172: | Train Loss: 0.01073 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.670 | Test Loss: 0.387\n",
      "Epoch 173: | Train Loss: 0.01075 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.658 | Test Loss: 0.388\n",
      "Epoch 174: | Train Loss: 0.01074 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.666 | Test Loss: 0.387\n",
      "Epoch 175: | Train Loss: 0.01073 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.672 | Test Loss: 0.387\n",
      "Epoch 176: | Train Loss: 0.01074 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.667 | Test Loss: 0.387\n",
      "Epoch 177: | Train Loss: 0.01072 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.662 | Test Loss: 0.387\n",
      "Epoch 178: | Train Loss: 0.01072 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.677 | Test Loss: 0.386\n",
      "Epoch 179: | Train Loss: 0.01072 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.674 | Test Loss: 0.386\n",
      "Epoch 180: | Train Loss: 0.01070 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.663 | Test Loss: 0.387\n",
      "Epoch 181: | Train Loss: 0.01071 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.678 | Test Loss: 0.386\n",
      "Epoch 182: | Train Loss: 0.01071 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.677 | Test Loss: 0.386\n",
      "Epoch 183: | Train Loss: 0.01070 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.666 | Test Loss: 0.386\n",
      "Epoch 184: | Train Loss: 0.01070 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.678 | Test Loss: 0.385\n",
      "Epoch 185: | Train Loss: 0.01069 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.678 | Test Loss: 0.385\n",
      "Epoch 186: | Train Loss: 0.01069 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.668 | Test Loss: 0.386\n",
      "Epoch 187: | Train Loss: 0.01069 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.677 | Test Loss: 0.385\n",
      "Epoch 188: | Train Loss: 0.01068 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.678 | Test Loss: 0.385\n",
      "Epoch 189: | Train Loss: 0.01068 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.669 | Test Loss: 0.385\n",
      "Epoch 190: | Train Loss: 0.01068 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.678 | Test Loss: 0.384\n",
      "Epoch 191: | Train Loss: 0.01067 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.678 | Test Loss: 0.384\n",
      "Epoch 192: | Train Loss: 0.01067 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.670 | Test Loss: 0.385\n",
      "Epoch 193: | Train Loss: 0.01067 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.678 | Test Loss: 0.384\n",
      "Epoch 194: | Train Loss: 0.01067 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.679 | Test Loss: 0.384\n",
      "Epoch 195: | Train Loss: 0.01066 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.673 | Test Loss: 0.384\n",
      "Epoch 196: | Train Loss: 0.01066 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.678 | Test Loss: 0.384\n",
      "Epoch 197: | Train Loss: 0.01066 | Train F1: 0.000 | Train Acc: 0.018| Test F1: 0.000 | Test Acc: 0.680 | Test Loss: 0.384\n",
      "Epoch 198: | Train Loss: 0.01065 | Train F1: 0.000 | Train Acc: 0.019| Test F1: 0.000 | Test Acc: 0.677 | Test Loss: 0.384\n",
      "Epoch 199: | Train Loss: 0.01065 | Train F1: 0.000 | Train Acc: 0.019| Test F1: 0.000 | Test Acc: 0.679 | Test Loss: 0.384\n",
      "Epoch 200: | Train Loss: 0.01065 | Train F1: 0.000 | Train Acc: 0.019| Test F1: 0.123 | Test Acc: 0.680 | Test Loss: 0.384\n",
      "Epoch 201: | Train Loss: 0.01065 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.678 | Test Loss: 0.384\n",
      "Epoch 202: | Train Loss: 0.01064 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.680 | Test Loss: 0.384\n",
      "Epoch 203: | Train Loss: 0.01064 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.679 | Test Loss: 0.384\n",
      "Epoch 204: | Train Loss: 0.01064 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.679 | Test Loss: 0.383\n",
      "Epoch 205: | Train Loss: 0.01063 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.681 | Test Loss: 0.383\n",
      "Epoch 206: | Train Loss: 0.01063 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.680 | Test Loss: 0.383\n",
      "Epoch 207: | Train Loss: 0.01063 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.679 | Test Loss: 0.383\n",
      "Epoch 208: | Train Loss: 0.01062 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.681 | Test Loss: 0.383\n",
      "Epoch 209: | Train Loss: 0.01062 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.680 | Test Loss: 0.383\n",
      "Epoch 210: | Train Loss: 0.01062 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.123 | Test Acc: 0.679 | Test Loss: 0.383\n",
      "Epoch 211: | Train Loss: 0.01062 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.683 | Test Loss: 0.383\n",
      "Epoch 212: | Train Loss: 0.01061 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.679 | Test Loss: 0.383\n",
      "Epoch 213: | Train Loss: 0.01061 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.682 | Test Loss: 0.383\n",
      "Epoch 214: | Train Loss: 0.01061 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.681 | Test Loss: 0.383\n",
      "Epoch 215: | Train Loss: 0.01061 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.681 | Test Loss: 0.383\n",
      "Epoch 216: | Train Loss: 0.01060 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.682 | Test Loss: 0.382\n",
      "Epoch 217: | Train Loss: 0.01060 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.123 | Test Acc: 0.683 | Test Loss: 0.382\n",
      "Epoch 218: | Train Loss: 0.01060 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.681 | Test Loss: 0.382\n",
      "Epoch 219: | Train Loss: 0.01059 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.685 | Test Loss: 0.382\n",
      "Epoch 220: | Train Loss: 0.01059 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.679 | Test Loss: 0.382\n",
      "Epoch 221: | Train Loss: 0.01059 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.687 | Test Loss: 0.382\n",
      "Epoch 222: | Train Loss: 0.01061 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.119 | Test Acc: 0.669 | Test Loss: 0.386\n",
      "Epoch 223: | Train Loss: 0.01066 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.687 | Test Loss: 0.382\n",
      "Epoch 224: | Train Loss: 0.01062 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.111 | Test Acc: 0.687 | Test Loss: 0.381\n",
      "Epoch 225: | Train Loss: 0.01058 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.122 | Test Acc: 0.672 | Test Loss: 0.384\n",
      "Epoch 226: | Train Loss: 0.01061 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.686 | Test Loss: 0.382\n",
      "Epoch 227: | Train Loss: 0.01058 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.115 | Test Acc: 0.688 | Test Loss: 0.381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228: | Train Loss: 0.01059 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.674 | Test Loss: 0.383\n",
      "Epoch 229: | Train Loss: 0.01059 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.683 | Test Loss: 0.382\n",
      "Epoch 230: | Train Loss: 0.01057 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.114 | Test Acc: 0.689 | Test Loss: 0.381\n",
      "Epoch 231: | Train Loss: 0.01059 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.677 | Test Loss: 0.382\n",
      "Epoch 232: | Train Loss: 0.01058 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.683 | Test Loss: 0.382\n",
      "Epoch 233: | Train Loss: 0.01057 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.689 | Test Loss: 0.381\n",
      "Epoch 234: | Train Loss: 0.01059 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.682 | Test Loss: 0.381\n",
      "Epoch 235: | Train Loss: 0.01056 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.682 | Test Loss: 0.382\n",
      "Epoch 236: | Train Loss: 0.01057 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.689 | Test Loss: 0.381\n",
      "Epoch 237: | Train Loss: 0.01057 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.688 | Test Loss: 0.381\n",
      "Epoch 238: | Train Loss: 0.01056 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.681 | Test Loss: 0.382\n",
      "Epoch 239: | Train Loss: 0.01057 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.688 | Test Loss: 0.381\n",
      "Epoch 240: | Train Loss: 0.01055 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.690 | Test Loss: 0.380\n",
      "Epoch 241: | Train Loss: 0.01055 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.679 | Test Loss: 0.382\n",
      "Epoch 242: | Train Loss: 0.01056 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.689 | Test Loss: 0.380\n",
      "Epoch 243: | Train Loss: 0.01055 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.690 | Test Loss: 0.380\n",
      "Epoch 244: | Train Loss: 0.01055 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.683 | Test Loss: 0.381\n",
      "Epoch 245: | Train Loss: 0.01055 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.689 | Test Loss: 0.380\n",
      "Epoch 246: | Train Loss: 0.01054 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.690 | Test Loss: 0.380\n",
      "Epoch 247: | Train Loss: 0.01054 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.685 | Test Loss: 0.381\n",
      "Epoch 248: | Train Loss: 0.01055 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.689 | Test Loss: 0.380\n",
      "Epoch 249: | Train Loss: 0.01054 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.692 | Test Loss: 0.380\n",
      "Epoch 250: | Train Loss: 0.01054 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.136 | Test Acc: 0.683 | Test Loss: 0.381\n",
      "Epoch 251: | Train Loss: 0.01054 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.692 | Test Loss: 0.380\n",
      "Epoch 252: | Train Loss: 0.01053 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.691 | Test Loss: 0.380\n",
      "Epoch 253: | Train Loss: 0.01053 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.686 | Test Loss: 0.381\n",
      "Epoch 254: | Train Loss: 0.01053 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.693 | Test Loss: 0.379\n",
      "Epoch 255: | Train Loss: 0.01053 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.689 | Test Loss: 0.380\n",
      "Epoch 256: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.691 | Test Loss: 0.380\n",
      "Epoch 257: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.136 | Test Acc: 0.693 | Test Loss: 0.379\n",
      "Epoch 258: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.690 | Test Loss: 0.380\n",
      "Epoch 259: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.136 | Test Acc: 0.692 | Test Loss: 0.379\n",
      "Epoch 260: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.694 | Test Loss: 0.379\n",
      "Epoch 261: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.144 | Test Acc: 0.687 | Test Loss: 0.380\n",
      "Epoch 262: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.697 | Test Loss: 0.379\n",
      "Epoch 263: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.144 | Test Acc: 0.689 | Test Loss: 0.380\n",
      "Epoch 264: | Train Loss: 0.01051 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.695 | Test Loss: 0.379\n",
      "Epoch 265: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.695 | Test Loss: 0.379\n",
      "Epoch 266: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.140 | Test Acc: 0.690 | Test Loss: 0.380\n",
      "Epoch 267: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.698 | Test Loss: 0.378\n",
      "Epoch 268: | Train Loss: 0.01051 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.144 | Test Acc: 0.688 | Test Loss: 0.380\n",
      "Epoch 269: | Train Loss: 0.01051 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.699 | Test Loss: 0.378\n",
      "Epoch 270: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.132 | Test Acc: 0.692 | Test Loss: 0.379\n",
      "Epoch 271: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.144 | Test Acc: 0.697 | Test Loss: 0.378\n",
      "Epoch 272: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.696 | Test Loss: 0.380\n",
      "Epoch 273: | Train Loss: 0.01052 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.147 | Test Acc: 0.697 | Test Loss: 0.380\n",
      "Epoch 274: | Train Loss: 0.01053 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.118 | Test Acc: 0.695 | Test Loss: 0.382\n",
      "Epoch 275: | Train Loss: 0.01056 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.147 | Test Acc: 0.699 | Test Loss: 0.379\n",
      "Epoch 276: | Train Loss: 0.01054 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.694 | Test Loss: 0.380\n",
      "Epoch 277: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.698 | Test Loss: 0.378\n",
      "Epoch 278: | Train Loss: 0.01048 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.144 | Test Acc: 0.700 | Test Loss: 0.378\n",
      "Epoch 279: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.694 | Test Loss: 0.381\n",
      "Epoch 280: | Train Loss: 0.01053 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.144 | Test Acc: 0.700 | Test Loss: 0.378\n",
      "Epoch 281: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.699 | Test Loss: 0.378\n",
      "Epoch 282: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.695 | Test Loss: 0.379\n",
      "Epoch 283: | Train Loss: 0.01048 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.139 | Test Acc: 0.702 | Test Loss: 0.378\n",
      "Epoch 284: | Train Loss: 0.01049 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.699 | Test Loss: 0.379\n",
      "Epoch 285: | Train Loss: 0.01049 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.147 | Test Acc: 0.698 | Test Loss: 0.378\n",
      "Epoch 286: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.703 | Test Loss: 0.377\n",
      "Epoch 287: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.136 | Test Acc: 0.695 | Test Loss: 0.379\n",
      "Epoch 288: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.143 | Test Acc: 0.702 | Test Loss: 0.377\n",
      "Epoch 289: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.704 | Test Loss: 0.378\n",
      "Epoch 290: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.147 | Test Acc: 0.697 | Test Loss: 0.378\n",
      "Epoch 291: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.704 | Test Loss: 0.377\n",
      "Epoch 292: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.133 | Test Acc: 0.702 | Test Loss: 0.377\n",
      "Epoch 293: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.146 | Test Acc: 0.700 | Test Loss: 0.378\n",
      "Epoch 294: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.705 | Test Loss: 0.377\n",
      "Epoch 295: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.143 | Test Acc: 0.703 | Test Loss: 0.377\n",
      "Epoch 296: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.144 | Test Acc: 0.701 | Test Loss: 0.377\n",
      "Epoch 297: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.706 | Test Loss: 0.376\n",
      "Epoch 298: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.147 | Test Acc: 0.700 | Test Loss: 0.377\n",
      "Epoch 299: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.132 | Test Acc: 0.707 | Test Loss: 0.376\n",
      "Epoch 300: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.128 | Test Acc: 0.703 | Test Loss: 0.378\n",
      "Epoch 301: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.147 | Test Acc: 0.709 | Test Loss: 0.377\n",
      "Epoch 302: | Train Loss: 0.01049 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.691 | Test Loss: 0.386\n",
      "Epoch 303: | Train Loss: 0.01062 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.148 | Test Acc: 0.708 | Test Loss: 0.381\n",
      "Epoch 304: | Train Loss: 0.01064 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.130 | Test Acc: 0.694 | Test Loss: 0.383\n",
      "Epoch 305: | Train Loss: 0.01059 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.160 | Test Acc: 0.698 | Test Loss: 0.377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 306: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.111 | Test Acc: 0.691 | Test Loss: 0.381\n",
      "Epoch 307: | Train Loss: 0.01070 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.189 | Test Acc: 0.655 | Test Loss: 0.391\n",
      "Epoch 308: | Train Loss: 0.01076 | Train F1: 0.005 | Train Acc: 0.018| Test F1: 0.100 | Test Acc: 0.708 | Test Loss: 0.380\n",
      "Epoch 309: | Train Loss: 0.01053 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.114 | Test Acc: 0.699 | Test Loss: 0.383\n",
      "Epoch 310: | Train Loss: 0.01069 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.132 | Test Acc: 0.691 | Test Loss: 0.380\n",
      "Epoch 311: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.128 | Test Acc: 0.672 | Test Loss: 0.386\n",
      "Epoch 312: | Train Loss: 0.01060 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.105 | Test Acc: 0.710 | Test Loss: 0.378\n",
      "Epoch 313: | Train Loss: 0.01046 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.109 | Test Acc: 0.707 | Test Loss: 0.380\n",
      "Epoch 314: | Train Loss: 0.01057 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.131 | Test Acc: 0.692 | Test Loss: 0.381\n",
      "Epoch 315: | Train Loss: 0.01048 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.681 | Test Loss: 0.384\n",
      "Epoch 316: | Train Loss: 0.01053 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.127 | Test Acc: 0.709 | Test Loss: 0.378\n",
      "Epoch 317: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.124 | Test Acc: 0.708 | Test Loss: 0.378\n",
      "Epoch 318: | Train Loss: 0.01050 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.128 | Test Acc: 0.699 | Test Loss: 0.379\n",
      "Epoch 319: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.143 | Test Acc: 0.688 | Test Loss: 0.381\n",
      "Epoch 320: | Train Loss: 0.01048 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.129 | Test Acc: 0.710 | Test Loss: 0.376\n",
      "Epoch 321: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.125 | Test Acc: 0.709 | Test Loss: 0.376\n",
      "Epoch 322: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.140 | Test Acc: 0.701 | Test Loss: 0.377\n",
      "Epoch 323: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.145 | Test Acc: 0.698 | Test Loss: 0.378\n",
      "Epoch 324: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.130 | Test Acc: 0.710 | Test Loss: 0.375\n",
      "Epoch 325: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.149 | Test Acc: 0.710 | Test Loss: 0.375\n",
      "Epoch 326: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.149 | Test Acc: 0.703 | Test Loss: 0.376\n",
      "Epoch 327: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.138 | Test Acc: 0.706 | Test Loss: 0.376\n",
      "Epoch 328: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.153 | Test Acc: 0.709 | Test Loss: 0.375\n",
      "Epoch 329: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.141 | Test Acc: 0.710 | Test Loss: 0.374\n",
      "Epoch 330: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.142 | Test Acc: 0.706 | Test Loss: 0.376\n",
      "Epoch 331: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.149 | Test Acc: 0.704 | Test Loss: 0.376\n",
      "Epoch 332: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.711 | Test Loss: 0.374\n",
      "Epoch 333: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.138 | Test Acc: 0.709 | Test Loss: 0.374\n",
      "Epoch 334: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.141 | Test Acc: 0.703 | Test Loss: 0.376\n",
      "Epoch 335: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.709 | Test Loss: 0.375\n",
      "Epoch 336: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.710 | Test Loss: 0.374\n",
      "Epoch 337: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.706 | Test Loss: 0.375\n",
      "Epoch 338: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.707 | Test Loss: 0.375\n",
      "Epoch 339: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.130 | Test Acc: 0.710 | Test Loss: 0.374\n",
      "Epoch 340: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.130 | Test Acc: 0.709 | Test Loss: 0.374\n",
      "Epoch 341: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.707 | Test Loss: 0.375\n",
      "Epoch 342: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.130 | Test Acc: 0.710 | Test Loss: 0.374\n",
      "Epoch 343: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.711 | Test Loss: 0.374\n",
      "Epoch 344: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.708 | Test Loss: 0.374\n",
      "Epoch 345: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.710 | Test Loss: 0.374\n",
      "Epoch 346: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.712 | Test Loss: 0.373\n",
      "Epoch 347: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.130 | Test Acc: 0.708 | Test Loss: 0.374\n",
      "Epoch 348: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.711 | Test Loss: 0.373\n",
      "Epoch 349: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.712 | Test Loss: 0.373\n",
      "Epoch 350: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.138 | Test Acc: 0.709 | Test Loss: 0.373\n",
      "Epoch 351: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.713 | Test Loss: 0.372\n",
      "Epoch 352: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.712 | Test Loss: 0.373\n",
      "Epoch 353: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.712 | Test Loss: 0.372\n",
      "Epoch 354: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.713 | Test Loss: 0.372\n",
      "Epoch 355: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.713 | Test Loss: 0.372\n",
      "Epoch 356: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.712 | Test Loss: 0.372\n",
      "Epoch 357: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.715 | Test Loss: 0.371\n",
      "Epoch 358: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.708 | Test Loss: 0.373\n",
      "Epoch 359: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.717 | Test Loss: 0.372\n",
      "Epoch 360: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.130 | Test Acc: 0.693 | Test Loss: 0.382\n",
      "Epoch 361: | Train Loss: 0.01055 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.145 | Test Acc: 0.717 | Test Loss: 0.379\n",
      "Epoch 362: | Train Loss: 0.01063 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.107 | Test Acc: 0.690 | Test Loss: 0.393\n",
      "Epoch 363: | Train Loss: 0.01085 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.163 | Test Acc: 0.708 | Test Loss: 0.374\n",
      "Epoch 364: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.095 | Test Acc: 0.698 | Test Loss: 0.381\n",
      "Epoch 365: | Train Loss: 0.01073 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.130 | Test Acc: 0.666 | Test Loss: 0.395\n",
      "Epoch 366: | Train Loss: 0.01087 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.160 | Test Acc: 0.679 | Test Loss: 0.381\n",
      "Epoch 367: | Train Loss: 0.01051 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.084 | Test Acc: 0.681 | Test Loss: 0.394\n",
      "Epoch 368: | Train Loss: 0.01112 | Train F1: 0.003 | Train Acc: 0.018| Test F1: 0.103 | Test Acc: 0.714 | Test Loss: 0.373\n",
      "Epoch 369: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.638 | Test Loss: 0.398\n",
      "Epoch 370: | Train Loss: 0.01091 | Train F1: 0.004 | Train Acc: 0.018| Test F1: 0.088 | Test Acc: 0.715 | Test Loss: 0.374\n",
      "Epoch 371: | Train Loss: 0.01039 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.079 | Test Acc: 0.696 | Test Loss: 0.384\n",
      "Epoch 372: | Train Loss: 0.01077 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.123 | Test Acc: 0.715 | Test Loss: 0.375\n",
      "Epoch 373: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.662 | Test Loss: 0.387\n",
      "Epoch 374: | Train Loss: 0.01063 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.084 | Test Acc: 0.693 | Test Loss: 0.382\n",
      "Epoch 375: | Train Loss: 0.01053 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.083 | Test Acc: 0.714 | Test Loss: 0.375\n",
      "Epoch 376: | Train Loss: 0.01046 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.107 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 377: | Train Loss: 0.01057 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.111 | Test Acc: 0.702 | Test Loss: 0.376\n",
      "Epoch 378: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.108 | Test Acc: 0.689 | Test Loss: 0.383\n",
      "Epoch 379: | Train Loss: 0.01054 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.111 | Test Acc: 0.705 | Test Loss: 0.376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.122 | Test Acc: 0.715 | Test Loss: 0.377\n",
      "Epoch 381: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.107 | Test Acc: 0.714 | Test Loss: 0.375\n",
      "Epoch 382: | Train Loss: 0.01042 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.112 | Test Acc: 0.706 | Test Loss: 0.377\n",
      "Epoch 383: | Train Loss: 0.01040 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.690 | Test Loss: 0.380\n",
      "Epoch 384: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.126 | Test Acc: 0.707 | Test Loss: 0.375\n",
      "Epoch 385: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.111 | Test Acc: 0.712 | Test Loss: 0.375\n",
      "Epoch 386: | Train Loss: 0.01043 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.123 | Test Acc: 0.714 | Test Loss: 0.374\n",
      "Epoch 387: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.142 | Test Acc: 0.702 | Test Loss: 0.376\n",
      "Epoch 388: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.701 | Test Loss: 0.377\n",
      "Epoch 389: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.123 | Test Acc: 0.712 | Test Loss: 0.374\n",
      "Epoch 390: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.714 | Test Loss: 0.375\n",
      "Epoch 391: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.713 | Test Loss: 0.374\n",
      "Epoch 392: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.709 | Test Loss: 0.375\n",
      "Epoch 393: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.705 | Test Loss: 0.376\n",
      "Epoch 394: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.141 | Test Acc: 0.711 | Test Loss: 0.374\n",
      "Epoch 395: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.714 | Test Loss: 0.373\n",
      "Epoch 396: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.713 | Test Loss: 0.373\n",
      "Epoch 397: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.705 | Test Loss: 0.375\n",
      "Epoch 398: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.134 | Test Acc: 0.709 | Test Loss: 0.374\n",
      "Epoch 399: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.133 | Test Acc: 0.714 | Test Loss: 0.372\n",
      "Epoch 400: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.714 | Test Loss: 0.373\n",
      "Epoch 401: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.711 | Test Loss: 0.373\n",
      "Epoch 402: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.709 | Test Loss: 0.374\n",
      "Epoch 403: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.711 | Test Loss: 0.373\n",
      "Epoch 404: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.715 | Test Loss: 0.372\n",
      "Epoch 405: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.714 | Test Loss: 0.372\n",
      "Epoch 406: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.709 | Test Loss: 0.373\n",
      "Epoch 407: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.711 | Test Loss: 0.373\n",
      "Epoch 408: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.715 | Test Loss: 0.372\n",
      "Epoch 409: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.715 | Test Loss: 0.372\n",
      "Epoch 410: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.712 | Test Loss: 0.373\n",
      "Epoch 411: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.712 | Test Loss: 0.373\n",
      "Epoch 412: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.715 | Test Loss: 0.372\n",
      "Epoch 413: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.716 | Test Loss: 0.372\n",
      "Epoch 414: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.714 | Test Loss: 0.372\n",
      "Epoch 415: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.712 | Test Loss: 0.372\n",
      "Epoch 416: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.716 | Test Loss: 0.371\n",
      "Epoch 417: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.717 | Test Loss: 0.371\n",
      "Epoch 418: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.714 | Test Loss: 0.372\n",
      "Epoch 419: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.714 | Test Loss: 0.371\n",
      "Epoch 420: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.717 | Test Loss: 0.371\n",
      "Epoch 421: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.717 | Test Loss: 0.371\n",
      "Epoch 422: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.715 | Test Loss: 0.371\n",
      "Epoch 423: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.716 | Test Loss: 0.371\n",
      "Epoch 424: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.718 | Test Loss: 0.370\n",
      "Epoch 425: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.717 | Test Loss: 0.370\n",
      "Epoch 426: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.716 | Test Loss: 0.370\n",
      "Epoch 427: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.718 | Test Loss: 0.370\n",
      "Epoch 428: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.718 | Test Loss: 0.370\n",
      "Epoch 429: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.717 | Test Loss: 0.370\n",
      "Epoch 430: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.718 | Test Loss: 0.370\n",
      "Epoch 431: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.719 | Test Loss: 0.370\n",
      "Epoch 432: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.718 | Test Loss: 0.370\n",
      "Epoch 433: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.719 | Test Loss: 0.370\n",
      "Epoch 434: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.720 | Test Loss: 0.369\n",
      "Epoch 435: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.718 | Test Loss: 0.370\n",
      "Epoch 436: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.719 | Test Loss: 0.369\n",
      "Epoch 437: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.721 | Test Loss: 0.369\n",
      "Epoch 438: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.719 | Test Loss: 0.369\n",
      "Epoch 439: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.720 | Test Loss: 0.369\n",
      "Epoch 440: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.721 | Test Loss: 0.369\n",
      "Epoch 441: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.720 | Test Loss: 0.369\n",
      "Epoch 442: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.722 | Test Loss: 0.369\n",
      "Epoch 443: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.721 | Test Loss: 0.369\n",
      "Epoch 444: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.721 | Test Loss: 0.369\n",
      "Epoch 445: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.722 | Test Loss: 0.368\n",
      "Epoch 446: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.721 | Test Loss: 0.369\n",
      "Epoch 447: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.723 | Test Loss: 0.368\n",
      "Epoch 448: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.722 | Test Loss: 0.368\n",
      "Epoch 449: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.723 | Test Loss: 0.368\n",
      "Epoch 450: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.723 | Test Loss: 0.368\n",
      "Epoch 451: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.723 | Test Loss: 0.368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 452: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.723 | Test Loss: 0.368\n",
      "Epoch 453: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.724 | Test Loss: 0.368\n",
      "Epoch 454: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.723 | Test Loss: 0.368\n",
      "Epoch 455: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.724 | Test Loss: 0.368\n",
      "Epoch 456: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.723 | Test Loss: 0.369\n",
      "Epoch 457: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.724 | Test Loss: 0.369\n",
      "Epoch 458: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.724 | Test Loss: 0.370\n",
      "Epoch 459: | Train Loss: 0.01029 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.725 | Test Loss: 0.370\n",
      "Epoch 460: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.722 | Test Loss: 0.373\n",
      "Epoch 461: | Train Loss: 0.01036 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.726 | Test Loss: 0.371\n",
      "Epoch 462: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.718 | Test Loss: 0.375\n",
      "Epoch 463: | Train Loss: 0.01041 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.727 | Test Loss: 0.370\n",
      "Epoch 464: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.718 | Test Loss: 0.371\n",
      "Epoch 465: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 466: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.725 | Test Loss: 0.368\n",
      "Epoch 467: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.722 | Test Loss: 0.370\n",
      "Epoch 468: | Train Loss: 0.01028 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.728 | Test Loss: 0.369\n",
      "Epoch 469: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.719 | Test Loss: 0.371\n",
      "Epoch 470: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.728 | Test Loss: 0.368\n",
      "Epoch 471: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.726 | Test Loss: 0.368\n",
      "Epoch 472: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.126 | Test Acc: 0.723 | Test Loss: 0.368\n",
      "Epoch 473: | Train Loss: 0.01024 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.728 | Test Loss: 0.368\n",
      "Epoch 474: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.722 | Test Loss: 0.370\n",
      "Epoch 475: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.727 | Test Loss: 0.368\n",
      "Epoch 476: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.726 | Test Loss: 0.368\n",
      "Epoch 477: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.724 | Test Loss: 0.368\n",
      "Epoch 478: | Train Loss: 0.01024 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 479: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.723 | Test Loss: 0.368\n",
      "Epoch 480: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 481: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.725 | Test Loss: 0.368\n",
      "Epoch 482: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.726 | Test Loss: 0.368\n",
      "Epoch 483: | Train Loss: 0.01024 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.727 | Test Loss: 0.368\n",
      "Epoch 484: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.724 | Test Loss: 0.368\n",
      "Epoch 485: | Train Loss: 0.01024 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.728 | Test Loss: 0.368\n",
      "Epoch 486: | Train Loss: 0.01025 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.724 | Test Loss: 0.368\n",
      "Epoch 487: | Train Loss: 0.01024 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.728 | Test Loss: 0.367\n",
      "Epoch 488: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.726 | Test Loss: 0.368\n",
      "Epoch 489: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.727 | Test Loss: 0.368\n",
      "Epoch 490: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.729 | Test Loss: 0.367\n",
      "Epoch 491: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.724 | Test Loss: 0.368\n",
      "Epoch 492: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.730 | Test Loss: 0.367\n",
      "Epoch 493: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.721 | Test Loss: 0.369\n",
      "Epoch 494: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.730 | Test Loss: 0.367\n",
      "Epoch 495: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.719 | Test Loss: 0.371\n",
      "Epoch 496: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.731 | Test Loss: 0.368\n",
      "Epoch 497: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.720 | Test Loss: 0.374\n",
      "Epoch 498: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.731 | Test Loss: 0.371\n",
      "Epoch 499: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.728 | Test Loss: 0.375\n",
      "Epoch 500: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.170 | Test Acc: 0.725 | Test Loss: 0.373\n",
      "Epoch 501: | Train Loss: 0.01037 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.729 | Test Loss: 0.371\n",
      "Epoch 502: | Train Loss: 0.01034 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 503: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.726 | Test Loss: 0.368\n",
      "Epoch 504: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.730 | Test Loss: 0.370\n",
      "Epoch 505: | Train Loss: 0.01031 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.729 | Test Loss: 0.370\n",
      "Epoch 506: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.722 | Test Loss: 0.370\n",
      "Epoch 507: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.730 | Test Loss: 0.367\n",
      "Epoch 508: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.720 | Test Loss: 0.369\n",
      "Epoch 509: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.730 | Test Loss: 0.367\n",
      "Epoch 510: | Train Loss: 0.01021 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.731 | Test Loss: 0.367\n",
      "Epoch 511: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.715 | Test Loss: 0.371\n",
      "Epoch 512: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.731 | Test Loss: 0.367\n",
      "Epoch 513: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.726 | Test Loss: 0.368\n",
      "Epoch 514: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 515: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.731 | Test Loss: 0.366\n",
      "Epoch 516: | Train Loss: 0.01022 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.723 | Test Loss: 0.368\n",
      "Epoch 517: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.732 | Test Loss: 0.367\n",
      "Epoch 518: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.730 | Test Loss: 0.368\n",
      "Epoch 519: | Train Loss: 0.01022 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.730 | Test Loss: 0.367\n",
      "Epoch 520: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.730 | Test Loss: 0.367\n",
      "Epoch 521: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.732 | Test Loss: 0.366\n",
      "Epoch 522: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.726 | Test Loss: 0.367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 523: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.732 | Test Loss: 0.366\n",
      "Epoch 524: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.729 | Test Loss: 0.366\n",
      "Epoch 525: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.730 | Test Loss: 0.366\n",
      "Epoch 526: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.732 | Test Loss: 0.365\n",
      "Epoch 527: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.725 | Test Loss: 0.367\n",
      "Epoch 528: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.733 | Test Loss: 0.365\n",
      "Epoch 529: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.724 | Test Loss: 0.368\n",
      "Epoch 530: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.733 | Test Loss: 0.366\n",
      "Epoch 531: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.721 | Test Loss: 0.372\n",
      "Epoch 532: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.734 | Test Loss: 0.370\n",
      "Epoch 533: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.717 | Test Loss: 0.380\n",
      "Epoch 534: | Train Loss: 0.01050 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.734 | Test Loss: 0.371\n",
      "Epoch 535: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.728 | Test Loss: 0.372\n",
      "Epoch 536: | Train Loss: 0.01033 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.732 | Test Loss: 0.367\n",
      "Epoch 537: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.734 | Test Loss: 0.365\n",
      "Epoch 538: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.726 | Test Loss: 0.369\n",
      "Epoch 539: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.734 | Test Loss: 0.368\n",
      "Epoch 540: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.729 | Test Loss: 0.370\n",
      "Epoch 541: | Train Loss: 0.01027 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.733 | Test Loss: 0.367\n",
      "Epoch 542: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.734 | Test Loss: 0.365\n",
      "Epoch 543: | Train Loss: 0.01017 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.729 | Test Loss: 0.367\n",
      "Epoch 544: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.735 | Test Loss: 0.367\n",
      "Epoch 545: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.731 | Test Loss: 0.368\n",
      "Epoch 546: | Train Loss: 0.01022 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.733 | Test Loss: 0.366\n",
      "Epoch 547: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.734 | Test Loss: 0.365\n",
      "Epoch 548: | Train Loss: 0.01016 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.731 | Test Loss: 0.366\n",
      "Epoch 549: | Train Loss: 0.01017 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.734 | Test Loss: 0.366\n",
      "Epoch 550: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.731 | Test Loss: 0.367\n",
      "Epoch 551: | Train Loss: 0.01021 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.734 | Test Loss: 0.366\n",
      "Epoch 552: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.733 | Test Loss: 0.365\n",
      "Epoch 553: | Train Loss: 0.01016 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.733 | Test Loss: 0.365\n",
      "Epoch 554: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.734 | Test Loss: 0.365\n",
      "Epoch 555: | Train Loss: 0.01016 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.733 | Test Loss: 0.366\n",
      "Epoch 556: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.735 | Test Loss: 0.366\n",
      "Epoch 557: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.732 | Test Loss: 0.366\n",
      "Epoch 558: | Train Loss: 0.01017 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.137 | Test Acc: 0.736 | Test Loss: 0.365\n",
      "Epoch 559: | Train Loss: 0.01016 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.732 | Test Loss: 0.366\n",
      "Epoch 560: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.736 | Test Loss: 0.364\n",
      "Epoch 561: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.733 | Test Loss: 0.365\n",
      "Epoch 562: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.735 | Test Loss: 0.365\n",
      "Epoch 563: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.735 | Test Loss: 0.365\n",
      "Epoch 564: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.734 | Test Loss: 0.365\n",
      "Epoch 565: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.736 | Test Loss: 0.364\n",
      "Epoch 566: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.730 | Test Loss: 0.366\n",
      "Epoch 567: | Train Loss: 0.01017 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.737 | Test Loss: 0.365\n",
      "Epoch 568: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.722 | Test Loss: 0.371\n",
      "Epoch 569: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.736 | Test Loss: 0.367\n",
      "Epoch 570: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.710 | Test Loss: 0.377\n",
      "Epoch 571: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.146 | Test Acc: 0.736 | Test Loss: 0.368\n",
      "Epoch 572: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.723 | Test Loss: 0.372\n",
      "Epoch 573: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.737 | Test Loss: 0.365\n",
      "Epoch 574: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.737 | Test Loss: 0.364\n",
      "Epoch 575: | Train Loss: 0.01016 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.721 | Test Loss: 0.368\n",
      "Epoch 576: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.096 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 577: | Train Loss: 0.01030 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.170 | Test Acc: 0.704 | Test Loss: 0.374\n",
      "Epoch 578: | Train Loss: 0.01033 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.730 | Test Loss: 0.366\n",
      "Epoch 579: | Train Loss: 0.01027 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.730 | Test Loss: 0.366\n",
      "Epoch 580: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.731 | Test Loss: 0.365\n",
      "Epoch 581: | Train Loss: 0.01014 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.735 | Test Loss: 0.365\n",
      "Epoch 582: | Train Loss: 0.01021 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.714 | Test Loss: 0.371\n",
      "Epoch 583: | Train Loss: 0.01024 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.737 | Test Loss: 0.364\n",
      "Epoch 584: | Train Loss: 0.01017 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.736 | Test Loss: 0.364\n",
      "Epoch 585: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 586: | Train Loss: 0.01016 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.735 | Test Loss: 0.365\n",
      "Epoch 587: | Train Loss: 0.01020 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.722 | Test Loss: 0.368\n",
      "Epoch 588: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.739 | Test Loss: 0.364\n",
      "Epoch 589: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.738 | Test Loss: 0.364\n",
      "Epoch 590: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.729 | Test Loss: 0.366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.111 | Test Acc: 0.738 | Test Loss: 0.364\n",
      "Epoch 592: | Train Loss: 0.01016 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.145 | Test Acc: 0.729 | Test Loss: 0.366\n",
      "Epoch 593: | Train Loss: 0.01014 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.739 | Test Loss: 0.364\n",
      "Epoch 594: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.738 | Test Loss: 0.364\n",
      "Epoch 595: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.733 | Test Loss: 0.365\n",
      "Epoch 596: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.738 | Test Loss: 0.364\n",
      "Epoch 597: | Train Loss: 0.01016 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.729 | Test Loss: 0.366\n",
      "Epoch 598: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.740 | Test Loss: 0.363\n",
      "Epoch 599: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.737 | Test Loss: 0.364\n",
      "Epoch 600: | Train Loss: 0.01011 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.737 | Test Loss: 0.364\n",
      "Epoch 601: | Train Loss: 0.01011 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.740 | Test Loss: 0.363\n",
      "Epoch 602: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.732 | Test Loss: 0.365\n",
      "Epoch 603: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.740 | Test Loss: 0.363\n",
      "Epoch 604: | Train Loss: 0.01014 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.734 | Test Loss: 0.364\n",
      "Epoch 605: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.740 | Test Loss: 0.363\n",
      "Epoch 606: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.738 | Test Loss: 0.363\n",
      "Epoch 607: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.739 | Test Loss: 0.363\n",
      "Epoch 608: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.740 | Test Loss: 0.364\n",
      "Epoch 609: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.740 | Test Loss: 0.365\n",
      "Epoch 610: | Train Loss: 0.01015 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.736 | Test Loss: 0.369\n",
      "Epoch 611: | Train Loss: 0.01025 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.741 | Test Loss: 0.373\n",
      "Epoch 612: | Train Loss: 0.01041 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.704 | Test Loss: 0.403\n",
      "Epoch 613: | Train Loss: 0.01111 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.742 | Test Loss: 0.374\n",
      "Epoch 614: | Train Loss: 0.01042 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.740 | Test Loss: 0.367\n",
      "Epoch 615: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.731 | Test Loss: 0.365\n",
      "Epoch 616: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.742 | Test Loss: 0.365\n",
      "Epoch 617: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.729 | Test Loss: 0.377\n",
      "Epoch 618: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.741 | Test Loss: 0.368\n",
      "Epoch 619: | Train Loss: 0.01025 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.742 | Test Loss: 0.363\n",
      "Epoch 620: | Train Loss: 0.01013 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.735 | Test Loss: 0.365\n",
      "Epoch 621: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.742 | Test Loss: 0.367\n",
      "Epoch 622: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.739 | Test Loss: 0.370\n",
      "Epoch 623: | Train Loss: 0.01027 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.145 | Test Acc: 0.741 | Test Loss: 0.364\n",
      "Epoch 624: | Train Loss: 0.01011 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.740 | Test Loss: 0.364\n",
      "Epoch 625: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.739 | Test Loss: 0.368\n",
      "Epoch 626: | Train Loss: 0.01023 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.742 | Test Loss: 0.365\n",
      "Epoch 627: | Train Loss: 0.01016 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.736 | Test Loss: 0.364\n",
      "Epoch 628: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.741 | Test Loss: 0.364\n",
      "Epoch 629: | Train Loss: 0.01012 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.741 | Test Loss: 0.365\n",
      "Epoch 630: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.734 | Test Loss: 0.366\n",
      "Epoch 631: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.122 | Test Acc: 0.742 | Test Loss: 0.363\n",
      "Epoch 632: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.738 | Test Loss: 0.364\n",
      "Epoch 633: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.739 | Test Loss: 0.365\n",
      "Epoch 634: | Train Loss: 0.01013 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.144 | Test Acc: 0.742 | Test Loss: 0.364\n",
      "Epoch 635: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.734 | Test Loss: 0.365\n",
      "Epoch 636: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.742 | Test Loss: 0.362\n",
      "Epoch 637: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.740 | Test Loss: 0.363\n",
      "Epoch 638: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.738 | Test Loss: 0.364\n",
      "Epoch 639: | Train Loss: 0.01011 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.145 | Test Acc: 0.743 | Test Loss: 0.363\n",
      "Epoch 640: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.736 | Test Loss: 0.364\n",
      "Epoch 641: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.742 | Test Loss: 0.362\n",
      "Epoch 642: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.740 | Test Loss: 0.363\n",
      "Epoch 643: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.740 | Test Loss: 0.363\n",
      "Epoch 644: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 645: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.737 | Test Loss: 0.364\n",
      "Epoch 646: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 647: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.737 | Test Loss: 0.364\n",
      "Epoch 648: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 649: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.739 | Test Loss: 0.363\n",
      "Epoch 650: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 651: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.742 | Test Loss: 0.362\n",
      "Epoch 652: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.742 | Test Loss: 0.362\n",
      "Epoch 653: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.744 | Test Loss: 0.362\n",
      "Epoch 654: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.740 | Test Loss: 0.363\n",
      "Epoch 655: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.744 | Test Loss: 0.362\n",
      "Epoch 656: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.733 | Test Loss: 0.367\n",
      "Epoch 657: | Train Loss: 0.01016 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.743 | Test Loss: 0.366\n",
      "Epoch 658: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.710 | Test Loss: 0.382\n",
      "Epoch 659: | Train Loss: 0.01053 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.743 | Test Loss: 0.369\n",
      "Epoch 660: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.726 | Test Loss: 0.374\n",
      "Epoch 661: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.746 | Test Loss: 0.364\n",
      "Epoch 662: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.745 | Test Loss: 0.362\n",
      "Epoch 663: | Train Loss: 0.01008 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.728 | Test Loss: 0.366\n",
      "Epoch 664: | Train Loss: 0.01012 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.738 | Test Loss: 0.364\n",
      "Epoch 665: | Train Loss: 0.01020 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.710 | Test Loss: 0.373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 666: | Train Loss: 0.01028 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.742 | Test Loss: 0.363\n",
      "Epoch 667: | Train Loss: 0.01015 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.738 | Test Loss: 0.363\n",
      "Epoch 668: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 669: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.746 | Test Loss: 0.362\n",
      "Epoch 670: | Train Loss: 0.01011 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.726 | Test Loss: 0.368\n",
      "Epoch 671: | Train Loss: 0.01017 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.745 | Test Loss: 0.362\n",
      "Epoch 672: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.739 | Test Loss: 0.363\n",
      "Epoch 673: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.744 | Test Loss: 0.362\n",
      "Epoch 674: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.148 | Test Acc: 0.747 | Test Loss: 0.363\n",
      "Epoch 675: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.736 | Test Loss: 0.366\n",
      "Epoch 676: | Train Loss: 0.01014 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.148 | Test Acc: 0.746 | Test Loss: 0.362\n",
      "Epoch 677: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 678: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 679: | Train Loss: 0.01004 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.746 | Test Loss: 0.361\n",
      "Epoch 680: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.735 | Test Loss: 0.364\n",
      "Epoch 681: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.745 | Test Loss: 0.361\n",
      "Epoch 682: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.734 | Test Loss: 0.364\n",
      "Epoch 683: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.746 | Test Loss: 0.361\n",
      "Epoch 684: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.737 | Test Loss: 0.363\n",
      "Epoch 685: | Train Loss: 0.01006 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.746 | Test Loss: 0.362\n",
      "Epoch 686: | Train Loss: 0.01009 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.739 | Test Loss: 0.365\n",
      "Epoch 687: | Train Loss: 0.01012 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.079 | Test Acc: 0.744 | Test Loss: 0.367\n",
      "Epoch 688: | Train Loss: 0.01023 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.186 | Test Acc: 0.741 | Test Loss: 0.369\n",
      "Epoch 689: | Train Loss: 0.01022 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.079 | Test Acc: 0.745 | Test Loss: 0.371\n",
      "Epoch 690: | Train Loss: 0.01033 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.165 | Test Acc: 0.747 | Test Loss: 0.366\n",
      "Epoch 691: | Train Loss: 0.01019 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.737 | Test Loss: 0.367\n",
      "Epoch 692: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.747 | Test Loss: 0.361\n",
      "Epoch 693: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.729 | Test Loss: 0.366\n",
      "Epoch 694: | Train Loss: 0.01010 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.096 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 695: | Train Loss: 0.01012 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.737 | Test Loss: 0.364\n",
      "Epoch 696: | Train Loss: 0.01007 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.748 | Test Loss: 0.361\n",
      "Epoch 697: | Train Loss: 0.01003 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.748 | Test Loss: 0.360\n",
      "Epoch 698: | Train Loss: 0.01002 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.734 | Test Loss: 0.364\n",
      "Epoch 699: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.744 | Test Loss: 0.362\n",
      "Epoch 700: | Train Loss: 0.01011 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.725 | Test Loss: 0.367\n",
      "Epoch 701: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.747 | Test Loss: 0.362\n",
      "Epoch 702: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.736 | Test Loss: 0.365\n",
      "Epoch 703: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.749 | Test Loss: 0.363\n",
      "Epoch 704: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.745 | Test Loss: 0.365\n",
      "Epoch 705: | Train Loss: 0.01012 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.748 | Test Loss: 0.363\n",
      "Epoch 706: | Train Loss: 0.01008 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.745 | Test Loss: 0.363\n",
      "Epoch 707: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.148 | Test Acc: 0.749 | Test Loss: 0.361\n",
      "Epoch 708: | Train Loss: 0.01004 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.739 | Test Loss: 0.363\n",
      "Epoch 709: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.748 | Test Loss: 0.360\n",
      "Epoch 710: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.734 | Test Loss: 0.364\n",
      "Epoch 711: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.747 | Test Loss: 0.361\n",
      "Epoch 712: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.733 | Test Loss: 0.365\n",
      "Epoch 713: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.748 | Test Loss: 0.361\n",
      "Epoch 714: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.146 | Test Acc: 0.733 | Test Loss: 0.367\n",
      "Epoch 715: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.749 | Test Loss: 0.363\n",
      "Epoch 716: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.735 | Test Loss: 0.370\n",
      "Epoch 717: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.750 | Test Loss: 0.364\n",
      "Epoch 718: | Train Loss: 0.01016 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.744 | Test Loss: 0.367\n",
      "Epoch 719: | Train Loss: 0.01017 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.165 | Test Acc: 0.749 | Test Loss: 0.363\n",
      "Epoch 720: | Train Loss: 0.01009 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.112 | Test Acc: 0.749 | Test Loss: 0.361\n",
      "Epoch 721: | Train Loss: 0.01006 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.745 | Test Loss: 0.361\n",
      "Epoch 722: | Train Loss: 0.01002 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.750 | Test Loss: 0.359\n",
      "Epoch 723: | Train Loss: 0.01001 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.742 | Test Loss: 0.361\n",
      "Epoch 724: | Train Loss: 0.01001 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.750 | Test Loss: 0.360\n",
      "Epoch 725: | Train Loss: 0.01002 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.743 | Test Loss: 0.363\n",
      "Epoch 726: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.751 | Test Loss: 0.361\n",
      "Epoch 727: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.746 | Test Loss: 0.363\n",
      "Epoch 728: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.162 | Test Acc: 0.750 | Test Loss: 0.362\n",
      "Epoch 729: | Train Loss: 0.01007 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.116 | Test Acc: 0.748 | Test Loss: 0.363\n",
      "Epoch 730: | Train Loss: 0.01008 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.165 | Test Acc: 0.750 | Test Loss: 0.362\n",
      "Epoch 731: | Train Loss: 0.01006 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.120 | Test Acc: 0.749 | Test Loss: 0.362\n",
      "Epoch 732: | Train Loss: 0.01007 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.162 | Test Acc: 0.750 | Test Loss: 0.361\n",
      "Epoch 733: | Train Loss: 0.01004 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.123 | Test Acc: 0.747 | Test Loss: 0.362\n",
      "Epoch 734: | Train Loss: 0.01004 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.155 | Test Acc: 0.751 | Test Loss: 0.360\n",
      "Epoch 735: | Train Loss: 0.01003 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.131 | Test Acc: 0.743 | Test Loss: 0.363\n",
      "Epoch 736: | Train Loss: 0.01004 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.149 | Test Acc: 0.751 | Test Loss: 0.360\n",
      "Epoch 737: | Train Loss: 0.01004 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.736 | Test Loss: 0.365\n",
      "Epoch 738: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.750 | Test Loss: 0.361\n",
      "Epoch 739: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.729 | Test Loss: 0.367\n",
      "Epoch 740: | Train Loss: 0.01014 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.748 | Test Loss: 0.361\n",
      "Epoch 741: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 742: | Train Loss: 0.01013 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.747 | Test Loss: 0.360\n",
      "Epoch 743: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.732 | Test Loss: 0.365\n",
      "Epoch 744: | Train Loss: 0.01007 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.748 | Test Loss: 0.360\n",
      "Epoch 745: | Train Loss: 0.01006 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.165 | Test Acc: 0.737 | Test Loss: 0.363\n",
      "Epoch 746: | Train Loss: 0.01004 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.096 | Test Acc: 0.749 | Test Loss: 0.361\n",
      "Epoch 747: | Train Loss: 0.01008 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.743 | Test Loss: 0.363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 748: | Train Loss: 0.01006 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.104 | Test Acc: 0.751 | Test Loss: 0.362\n",
      "Epoch 749: | Train Loss: 0.01008 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.165 | Test Acc: 0.751 | Test Loss: 0.362\n",
      "Epoch 750: | Train Loss: 0.01006 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.124 | Test Acc: 0.745 | Test Loss: 0.364\n",
      "Epoch 751: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.156 | Test Acc: 0.752 | Test Loss: 0.362\n",
      "Epoch 752: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.727 | Test Loss: 0.369\n",
      "Epoch 753: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.748 | Test Loss: 0.362\n",
      "Epoch 754: | Train Loss: 0.01011 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.727 | Test Loss: 0.367\n",
      "Epoch 755: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.749 | Test Loss: 0.360\n",
      "Epoch 756: | Train Loss: 0.01004 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.741 | Test Loss: 0.362\n",
      "Epoch 757: | Train Loss: 0.01000 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.753 | Test Loss: 0.358\n",
      "Epoch 758: | Train Loss: 0.00997 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.155 | Test Acc: 0.750 | Test Loss: 0.359\n",
      "Epoch 759: | Train Loss: 0.00996 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.131 | Test Acc: 0.749 | Test Loss: 0.360\n",
      "Epoch 760: | Train Loss: 0.00997 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.149 | Test Acc: 0.753 | Test Loss: 0.359\n",
      "Epoch 761: | Train Loss: 0.01000 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.150 | Test Acc: 0.739 | Test Loss: 0.363\n",
      "Epoch 762: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.752 | Test Loss: 0.360\n",
      "Epoch 763: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.733 | Test Loss: 0.366\n",
      "Epoch 764: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.751 | Test Loss: 0.360\n",
      "Epoch 765: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.737 | Test Loss: 0.364\n",
      "Epoch 766: | Train Loss: 0.01004 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.751 | Test Loss: 0.358\n",
      "Epoch 767: | Train Loss: 0.01001 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.165 | Test Acc: 0.742 | Test Loss: 0.361\n",
      "Epoch 768: | Train Loss: 0.00999 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.108 | Test Acc: 0.752 | Test Loss: 0.359\n",
      "Epoch 769: | Train Loss: 0.01001 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.743 | Test Loss: 0.362\n",
      "Epoch 770: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.084 | Test Acc: 0.750 | Test Loss: 0.363\n",
      "Epoch 771: | Train Loss: 0.01014 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.192 | Test Acc: 0.745 | Test Loss: 0.366\n",
      "Epoch 772: | Train Loss: 0.01013 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.079 | Test Acc: 0.752 | Test Loss: 0.367\n",
      "Epoch 773: | Train Loss: 0.01023 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.172 | Test Acc: 0.753 | Test Loss: 0.365\n",
      "Epoch 774: | Train Loss: 0.01013 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.124 | Test Acc: 0.741 | Test Loss: 0.368\n",
      "Epoch 775: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.150 | Test Acc: 0.752 | Test Loss: 0.362\n",
      "Epoch 776: | Train Loss: 0.01011 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.720 | Test Loss: 0.371\n",
      "Epoch 777: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.746 | Test Loss: 0.361\n",
      "Epoch 778: | Train Loss: 0.01010 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.735 | Test Loss: 0.363\n",
      "Epoch 779: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.753 | Test Loss: 0.358\n",
      "Epoch 780: | Train Loss: 0.00998 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.152 | Test Acc: 0.752 | Test Loss: 0.358\n",
      "Epoch 781: | Train Loss: 0.00995 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.146 | Test Acc: 0.746 | Test Loss: 0.360\n",
      "Epoch 782: | Train Loss: 0.00997 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.123 | Test Acc: 0.752 | Test Loss: 0.359\n",
      "Epoch 783: | Train Loss: 0.01002 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.730 | Test Loss: 0.366\n",
      "Epoch 784: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.750 | Test Loss: 0.359\n",
      "Epoch 785: | Train Loss: 0.01005 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.738 | Test Loss: 0.363\n",
      "Epoch 786: | Train Loss: 0.01002 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.754 | Test Loss: 0.358\n",
      "Epoch 787: | Train Loss: 0.00997 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.134 | Test Acc: 0.751 | Test Loss: 0.358\n",
      "Epoch 788: | Train Loss: 0.00994 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.155 | Test Acc: 0.751 | Test Loss: 0.358\n",
      "Epoch 789: | Train Loss: 0.00994 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.123 | Test Acc: 0.754 | Test Loss: 0.357\n",
      "Epoch 790: | Train Loss: 0.00996 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.165 | Test Acc: 0.742 | Test Loss: 0.361\n",
      "Epoch 791: | Train Loss: 0.00998 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.123 | Test Acc: 0.752 | Test Loss: 0.358\n",
      "Epoch 792: | Train Loss: 0.01000 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.165 | Test Acc: 0.736 | Test Loss: 0.363\n",
      "Epoch 793: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.751 | Test Loss: 0.359\n",
      "Epoch 794: | Train Loss: 0.01003 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.162 | Test Acc: 0.732 | Test Loss: 0.365\n",
      "Epoch 795: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.139 | Test Acc: 0.752 | Test Loss: 0.360\n",
      "Epoch 796: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.735 | Test Loss: 0.366\n",
      "Epoch 797: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.146 | Test Acc: 0.754 | Test Loss: 0.360\n",
      "Epoch 798: | Train Loss: 0.01004 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.134 | Test Acc: 0.743 | Test Loss: 0.364\n",
      "Epoch 799: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.162 | Test Acc: 0.756 | Test Loss: 0.359\n",
      "Epoch 800: | Train Loss: 0.01001 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.124 | Test Acc: 0.752 | Test Loss: 0.361\n",
      "Epoch 801: | Train Loss: 0.01002 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.169 | Test Acc: 0.753 | Test Loss: 0.361\n",
      "Epoch 802: | Train Loss: 0.01002 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.096 | Test Acc: 0.754 | Test Loss: 0.362\n",
      "Epoch 803: | Train Loss: 0.01007 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.193 | Test Acc: 0.747 | Test Loss: 0.363\n",
      "Epoch 804: | Train Loss: 0.01006 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.079 | Test Acc: 0.750 | Test Loss: 0.363\n",
      "Epoch 805: | Train Loss: 0.01015 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.179 | Test Acc: 0.745 | Test Loss: 0.362\n",
      "Epoch 806: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.108 | Test Acc: 0.755 | Test Loss: 0.358\n",
      "Epoch 807: | Train Loss: 0.00998 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.162 | Test Acc: 0.752 | Test Loss: 0.358\n",
      "Epoch 808: | Train Loss: 0.00992 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.153 | Test Acc: 0.753 | Test Loss: 0.358\n",
      "Epoch 809: | Train Loss: 0.00991 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.123 | Test Acc: 0.755 | Test Loss: 0.357\n",
      "Epoch 810: | Train Loss: 0.00994 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.169 | Test Acc: 0.746 | Test Loss: 0.360\n",
      "Epoch 811: | Train Loss: 0.00997 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.100 | Test Acc: 0.753 | Test Loss: 0.359\n",
      "Epoch 812: | Train Loss: 0.01000 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.166 | Test Acc: 0.748 | Test Loss: 0.360\n",
      "Epoch 813: | Train Loss: 0.00995 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.127 | Test Acc: 0.755 | Test Loss: 0.357\n",
      "Epoch 814: | Train Loss: 0.00992 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.150 | Test Acc: 0.755 | Test Loss: 0.357\n",
      "Epoch 815: | Train Loss: 0.00991 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.159 | Test Acc: 0.750 | Test Loss: 0.358\n",
      "Epoch 816: | Train Loss: 0.00992 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.128 | Test Acc: 0.755 | Test Loss: 0.357\n",
      "Epoch 817: | Train Loss: 0.00995 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.166 | Test Acc: 0.737 | Test Loss: 0.362\n",
      "Epoch 818: | Train Loss: 0.01000 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.748 | Test Loss: 0.360\n",
      "Epoch 819: | Train Loss: 0.01008 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.707 | Test Loss: 0.377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 820: | Train Loss: 0.01036 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.741 | Test Loss: 0.370\n",
      "Epoch 821: | Train Loss: 0.01038 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.150 | Test Acc: 0.680 | Test Loss: 0.404\n",
      "Epoch 822: | Train Loss: 0.01107 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.159 | Test Acc: 0.757 | Test Loss: 0.358\n",
      "Epoch 823: | Train Loss: 0.00997 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.132 | Test Acc: 0.738 | Test Loss: 0.367\n",
      "Epoch 824: | Train Loss: 0.01031 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.649 | Test Loss: 0.432\n",
      "Epoch 825: | Train Loss: 0.01177 | Train F1: 0.005 | Train Acc: 0.018| Test F1: 0.116 | Test Acc: 0.756 | Test Loss: 0.360\n",
      "Epoch 826: | Train Loss: 0.01000 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.334 | Test Acc: 0.684 | Test Loss: 0.474\n",
      "Epoch 827: | Train Loss: 0.01339 | Train F1: 0.008 | Train Acc: 0.018| Test F1: 0.128 | Test Acc: 0.633 | Test Loss: 0.514\n",
      "Epoch 828: | Train Loss: 0.01394 | Train F1: 0.003 | Train Acc: 0.018| Test F1: 0.156 | Test Acc: 0.628 | Test Loss: 0.567\n",
      "Epoch 829: | Train Loss: 0.01530 | Train F1: 0.004 | Train Acc: 0.018| Test F1: 0.127 | Test Acc: 0.667 | Test Loss: 0.406\n",
      "Epoch 830: | Train Loss: 0.01112 | Train F1: 0.004 | Train Acc: 0.019| Test F1: 0.131 | Test Acc: 0.659 | Test Loss: 0.386\n",
      "Epoch 831: | Train Loss: 0.01071 | Train F1: 0.004 | Train Acc: 0.018| Test F1: 0.127 | Test Acc: 0.635 | Test Loss: 0.420\n",
      "Epoch 832: | Train Loss: 0.01175 | Train F1: 0.004 | Train Acc: 0.017| Test F1: 0.104 | Test Acc: 0.658 | Test Loss: 0.385\n",
      "Epoch 833: | Train Loss: 0.01065 | Train F1: 0.003 | Train Acc: 0.018| Test F1: 0.124 | Test Acc: 0.624 | Test Loss: 0.390\n",
      "Epoch 834: | Train Loss: 0.01067 | Train F1: 0.003 | Train Acc: 0.018| Test F1: 0.127 | Test Acc: 0.598 | Test Loss: 0.409\n",
      "Epoch 835: | Train Loss: 0.01111 | Train F1: 0.004 | Train Acc: 0.017| Test F1: 0.096 | Test Acc: 0.616 | Test Loss: 0.404\n",
      "Epoch 836: | Train Loss: 0.01101 | Train F1: 0.003 | Train Acc: 0.018| Test F1: 0.108 | Test Acc: 0.640 | Test Loss: 0.388\n",
      "Epoch 837: | Train Loss: 0.01060 | Train F1: 0.003 | Train Acc: 0.018| Test F1: 0.107 | Test Acc: 0.673 | Test Loss: 0.383\n",
      "Epoch 838: | Train Loss: 0.01054 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.091 | Test Acc: 0.660 | Test Loss: 0.391\n",
      "Epoch 839: | Train Loss: 0.01083 | Train F1: 0.003 | Train Acc: 0.018| Test F1: 0.083 | Test Acc: 0.660 | Test Loss: 0.391\n",
      "Epoch 840: | Train Loss: 0.01084 | Train F1: 0.003 | Train Acc: 0.018| Test F1: 0.095 | Test Acc: 0.709 | Test Loss: 0.383\n",
      "Epoch 841: | Train Loss: 0.01057 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.107 | Test Acc: 0.704 | Test Loss: 0.382\n",
      "Epoch 842: | Train Loss: 0.01049 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.088 | Test Acc: 0.686 | Test Loss: 0.386\n",
      "Epoch 843: | Train Loss: 0.01058 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.096 | Test Acc: 0.671 | Test Loss: 0.390\n",
      "Epoch 844: | Train Loss: 0.01066 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.108 | Test Acc: 0.668 | Test Loss: 0.389\n",
      "Epoch 845: | Train Loss: 0.01063 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.100 | Test Acc: 0.695 | Test Loss: 0.384\n",
      "Epoch 846: | Train Loss: 0.01053 | Train F1: 0.003 | Train Acc: 0.019| Test F1: 0.087 | Test Acc: 0.712 | Test Loss: 0.381\n",
      "Epoch 847: | Train Loss: 0.01049 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.095 | Test Acc: 0.717 | Test Loss: 0.382\n",
      "Epoch 848: | Train Loss: 0.01054 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.111 | Test Acc: 0.719 | Test Loss: 0.384\n",
      "Epoch 849: | Train Loss: 0.01059 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.719 | Test Loss: 0.383\n",
      "Epoch 850: | Train Loss: 0.01055 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.095 | Test Acc: 0.714 | Test Loss: 0.381\n",
      "Epoch 851: | Train Loss: 0.01049 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.706 | Test Loss: 0.382\n",
      "Epoch 852: | Train Loss: 0.01049 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.697 | Test Loss: 0.384\n",
      "Epoch 853: | Train Loss: 0.01052 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.107 | Test Acc: 0.699 | Test Loss: 0.384\n",
      "Epoch 854: | Train Loss: 0.01054 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.702 | Test Loss: 0.383\n",
      "Epoch 855: | Train Loss: 0.01051 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.706 | Test Loss: 0.382\n",
      "Epoch 856: | Train Loss: 0.01048 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.714 | Test Loss: 0.381\n",
      "Epoch 857: | Train Loss: 0.01047 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.717 | Test Loss: 0.381\n",
      "Epoch 858: | Train Loss: 0.01050 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.716 | Test Loss: 0.381\n",
      "Epoch 859: | Train Loss: 0.01050 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.715 | Test Loss: 0.381\n",
      "Epoch 860: | Train Loss: 0.01048 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 861: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.708 | Test Loss: 0.381\n",
      "Epoch 862: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.706 | Test Loss: 0.382\n",
      "Epoch 863: | Train Loss: 0.01048 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.707 | Test Loss: 0.382\n",
      "Epoch 864: | Train Loss: 0.01048 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.708 | Test Loss: 0.381\n",
      "Epoch 865: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.709 | Test Loss: 0.381\n",
      "Epoch 866: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.130 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 867: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.716 | Test Loss: 0.380\n",
      "Epoch 868: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.137 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 869: | Train Loss: 0.01047 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 870: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 871: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.708 | Test Loss: 0.381\n",
      "Epoch 872: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.708 | Test Loss: 0.381\n",
      "Epoch 873: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.145 | Test Acc: 0.709 | Test Loss: 0.381\n",
      "Epoch 874: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.710 | Test Loss: 0.381\n",
      "Epoch 875: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 876: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.716 | Test Loss: 0.380\n",
      "Epoch 877: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.716 | Test Loss: 0.380\n",
      "Epoch 878: | Train Loss: 0.01046 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 879: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 880: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.711 | Test Loss: 0.380\n",
      "Epoch 881: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.710 | Test Loss: 0.380\n",
      "Epoch 882: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.711 | Test Loss: 0.380\n",
      "Epoch 883: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.711 | Test Loss: 0.380\n",
      "Epoch 884: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 885: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 886: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 887: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 888: | Train Loss: 0.01045 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 889: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 890: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 891: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.152 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 892: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 893: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 894: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 895: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 896: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 897: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 898: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 899: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 900: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 901: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 902: | Train Loss: 0.01044 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 903: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 904: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.715 | Test Loss: 0.380\n",
      "Epoch 905: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 906: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 907: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 908: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 909: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 910: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 911: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 912: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 913: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 914: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 915: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 916: | Train Loss: 0.01043 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 917: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.713 | Test Loss: 0.379\n",
      "Epoch 918: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 919: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.713 | Test Loss: 0.379\n",
      "Epoch 920: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 921: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 922: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.153 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 923: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.713 | Test Loss: 0.379\n",
      "Epoch 924: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 925: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 926: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 927: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.713 | Test Loss: 0.379\n",
      "Epoch 928: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.146 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 929: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 930: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 931: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 932: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.717 | Test Loss: 0.379\n",
      "Epoch 933: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.169 | Test Acc: 0.711 | Test Loss: 0.380\n",
      "Epoch 934: | Train Loss: 0.01043 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.721 | Test Loss: 0.379\n",
      "Epoch 935: | Train Loss: 0.01045 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.183 | Test Acc: 0.706 | Test Loss: 0.383\n",
      "Epoch 936: | Train Loss: 0.01047 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.095 | Test Acc: 0.718 | Test Loss: 0.382\n",
      "Epoch 937: | Train Loss: 0.01056 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.205 | Test Acc: 0.690 | Test Loss: 0.387\n",
      "Epoch 938: | Train Loss: 0.01058 | Train F1: 0.006 | Train Acc: 0.019| Test F1: 0.075 | Test Acc: 0.700 | Test Loss: 0.388\n",
      "Epoch 939: | Train Loss: 0.01076 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.205 | Test Acc: 0.690 | Test Loss: 0.387\n",
      "Epoch 940: | Train Loss: 0.01058 | Train F1: 0.006 | Train Acc: 0.019| Test F1: 0.095 | Test Acc: 0.719 | Test Loss: 0.381\n",
      "Epoch 941: | Train Loss: 0.01053 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.165 | Test Acc: 0.711 | Test Loss: 0.380\n",
      "Epoch 942: | Train Loss: 0.01043 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 943: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.115 | Test Acc: 0.721 | Test Loss: 0.380\n",
      "Epoch 944: | Train Loss: 0.01046 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.180 | Test Acc: 0.698 | Test Loss: 0.384\n",
      "Epoch 945: | Train Loss: 0.01050 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.087 | Test Acc: 0.718 | Test Loss: 0.382\n",
      "Epoch 946: | Train Loss: 0.01055 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.173 | Test Acc: 0.707 | Test Loss: 0.382\n",
      "Epoch 947: | Train Loss: 0.01046 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 948: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 949: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.165 | Test Acc: 0.710 | Test Loss: 0.381\n",
      "Epoch 950: | Train Loss: 0.01044 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.720 | Test Loss: 0.380\n",
      "Epoch 951: | Train Loss: 0.01048 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.173 | Test Acc: 0.707 | Test Loss: 0.382\n",
      "Epoch 952: | Train Loss: 0.01045 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.719 | Test Loss: 0.379\n",
      "Epoch 953: | Train Loss: 0.01042 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 954: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.155 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 955: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.721 | Test Loss: 0.379\n",
      "Epoch 956: | Train Loss: 0.01043 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.169 | Test Acc: 0.710 | Test Loss: 0.381\n",
      "Epoch 957: | Train Loss: 0.01044 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.721 | Test Loss: 0.379\n",
      "Epoch 958: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 959: | Train Loss: 0.01041 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 960: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 961: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.163 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 962: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.719 | Test Loss: 0.379\n",
      "Epoch 963: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.169 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 964: | Train Loss: 0.01042 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.721 | Test Loss: 0.379\n",
      "Epoch 965: | Train Loss: 0.01043 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.169 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 966: | Train Loss: 0.01042 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.719 | Test Loss: 0.378\n",
      "Epoch 967: | Train Loss: 0.01041 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.713 | Test Loss: 0.379\n",
      "Epoch 968: | Train Loss: 0.01040 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 969: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.716 | Test Loss: 0.379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 970: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 971: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.145 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 972: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 973: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 974: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 975: | Train Loss: 0.01039 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.718 | Test Loss: 0.378\n",
      "Epoch 976: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.173 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 977: | Train Loss: 0.01041 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.722 | Test Loss: 0.379\n",
      "Epoch 978: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.190 | Test Acc: 0.703 | Test Loss: 0.383\n",
      "Epoch 979: | Train Loss: 0.01047 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.709 | Test Loss: 0.384\n",
      "Epoch 980: | Train Loss: 0.01062 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.226 | Test Acc: 0.682 | Test Loss: 0.389\n",
      "Epoch 981: | Train Loss: 0.01063 | Train F1: 0.006 | Train Acc: 0.019| Test F1: 0.076 | Test Acc: 0.693 | Test Loss: 0.393\n",
      "Epoch 982: | Train Loss: 0.01092 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.205 | Test Acc: 0.695 | Test Loss: 0.385\n",
      "Epoch 983: | Train Loss: 0.01054 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.721 | Test Loss: 0.378\n",
      "Epoch 984: | Train Loss: 0.01040 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.717 | Test Loss: 0.378\n",
      "Epoch 985: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.180 | Test Acc: 0.702 | Test Loss: 0.383\n",
      "Epoch 986: | Train Loss: 0.01046 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.079 | Test Acc: 0.710 | Test Loss: 0.384\n",
      "Epoch 987: | Train Loss: 0.01062 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.193 | Test Acc: 0.696 | Test Loss: 0.384\n",
      "Epoch 988: | Train Loss: 0.01051 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.721 | Test Loss: 0.379\n",
      "Epoch 989: | Train Loss: 0.01045 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 990: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 991: | Train Loss: 0.01040 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.720 | Test Loss: 0.380\n",
      "Epoch 992: | Train Loss: 0.01047 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.180 | Test Acc: 0.703 | Test Loss: 0.382\n",
      "Epoch 993: | Train Loss: 0.01046 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.722 | Test Loss: 0.379\n",
      "Epoch 994: | Train Loss: 0.01043 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 995: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 996: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.723 | Test Loss: 0.378\n",
      "Epoch 997: | Train Loss: 0.01042 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.176 | Test Acc: 0.710 | Test Loss: 0.381\n",
      "Epoch 998: | Train Loss: 0.01043 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.723 | Test Loss: 0.378\n",
      "Epoch 999: | Train Loss: 0.01042 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 1000: | Train Loss: 0.01039 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.717 | Test Loss: 0.378\n",
      "Epoch 1001: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.719 | Test Loss: 0.378\n",
      "Epoch 1002: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.169 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 1003: | Train Loss: 0.01040 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.723 | Test Loss: 0.378\n",
      "Epoch 1004: | Train Loss: 0.01041 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 1005: | Train Loss: 0.01039 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.718 | Test Loss: 0.378\n",
      "Epoch 1006: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.145 | Test Acc: 0.718 | Test Loss: 0.378\n",
      "Epoch 1007: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 1008: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.720 | Test Loss: 0.378\n",
      "Epoch 1009: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.173 | Test Acc: 0.714 | Test Loss: 0.379\n",
      "Epoch 1010: | Train Loss: 0.01039 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.722 | Test Loss: 0.378\n",
      "Epoch 1011: | Train Loss: 0.01039 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.169 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 1012: | Train Loss: 0.01039 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.720 | Test Loss: 0.377\n",
      "Epoch 1013: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.159 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 1014: | Train Loss: 0.01037 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.141 | Test Acc: 0.718 | Test Loss: 0.377\n",
      "Epoch 1015: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.718 | Test Loss: 0.378\n",
      "Epoch 1016: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.718 | Test Loss: 0.377\n",
      "Epoch 1017: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.719 | Test Loss: 0.377\n",
      "Epoch 1018: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.718 | Test Loss: 0.378\n",
      "Epoch 1019: | Train Loss: 0.01036 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.719 | Test Loss: 0.377\n",
      "Epoch 1020: | Train Loss: 0.01036 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.169 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 1021: | Train Loss: 0.01037 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.723 | Test Loss: 0.377\n",
      "Epoch 1022: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.183 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 1023: | Train Loss: 0.01041 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.720 | Test Loss: 0.380\n",
      "Epoch 1024: | Train Loss: 0.01049 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.217 | Test Acc: 0.691 | Test Loss: 0.387\n",
      "Epoch 1025: | Train Loss: 0.01056 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.076 | Test Acc: 0.694 | Test Loss: 0.392\n",
      "Epoch 1026: | Train Loss: 0.01090 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.228 | Test Acc: 0.685 | Test Loss: 0.389\n",
      "Epoch 1027: | Train Loss: 0.01061 | Train F1: 0.006 | Train Acc: 0.019| Test F1: 0.084 | Test Acc: 0.709 | Test Loss: 0.383\n",
      "Epoch 1028: | Train Loss: 0.01061 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.184 | Test Acc: 0.712 | Test Loss: 0.380\n",
      "Epoch 1029: | Train Loss: 0.01041 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.719 | Test Loss: 0.377\n",
      "Epoch 1030: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.136 | Test Acc: 0.722 | Test Loss: 0.377\n",
      "Epoch 1031: | Train Loss: 0.01038 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.704 | Test Loss: 0.382\n",
      "Epoch 1032: | Train Loss: 0.01044 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.713 | Test Loss: 0.382\n",
      "Epoch 1033: | Train Loss: 0.01057 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.192 | Test Acc: 0.701 | Test Loss: 0.383\n",
      "Epoch 1034: | Train Loss: 0.01047 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.722 | Test Loss: 0.378\n",
      "Epoch 1035: | Train Loss: 0.01042 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.717 | Test Loss: 0.378\n",
      "Epoch 1036: | Train Loss: 0.01036 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.717 | Test Loss: 0.378\n",
      "Epoch 1037: | Train Loss: 0.01035 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.724 | Test Loss: 0.378\n",
      "Epoch 1038: | Train Loss: 0.01040 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.184 | Test Acc: 0.707 | Test Loss: 0.381\n",
      "Epoch 1039: | Train Loss: 0.01042 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.722 | Test Loss: 0.379\n",
      "Epoch 1040: | Train Loss: 0.01045 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.173 | Test Acc: 0.715 | Test Loss: 0.379\n",
      "Epoch 1041: | Train Loss: 0.01038 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.720 | Test Loss: 0.377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1042: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.720 | Test Loss: 0.377\n",
      "Epoch 1043: | Train Loss: 0.01035 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.170 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 1044: | Train Loss: 0.01037 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.724 | Test Loss: 0.378\n",
      "Epoch 1045: | Train Loss: 0.01040 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.176 | Test Acc: 0.713 | Test Loss: 0.380\n",
      "Epoch 1046: | Train Loss: 0.01039 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.725 | Test Loss: 0.377\n",
      "Epoch 1047: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.717 | Test Loss: 0.378\n",
      "Epoch 1048: | Train Loss: 0.01035 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.149 | Test Acc: 0.720 | Test Loss: 0.377\n",
      "Epoch 1049: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1050: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.170 | Test Acc: 0.717 | Test Loss: 0.378\n",
      "Epoch 1051: | Train Loss: 0.01036 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.725 | Test Loss: 0.377\n",
      "Epoch 1052: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 1053: | Train Loss: 0.01038 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.725 | Test Loss: 0.377\n",
      "Epoch 1054: | Train Loss: 0.01039 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 1055: | Train Loss: 0.01037 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.136 | Test Acc: 0.725 | Test Loss: 0.377\n",
      "Epoch 1056: | Train Loss: 0.01037 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.170 | Test Acc: 0.718 | Test Loss: 0.378\n",
      "Epoch 1057: | Train Loss: 0.01035 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.134 | Test Acc: 0.722 | Test Loss: 0.376\n",
      "Epoch 1058: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.719 | Test Loss: 0.377\n",
      "Epoch 1059: | Train Loss: 0.01034 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.146 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1060: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.720 | Test Loss: 0.376\n",
      "Epoch 1061: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1062: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1063: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.720 | Test Loss: 0.376\n",
      "Epoch 1064: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1065: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.720 | Test Loss: 0.376\n",
      "Epoch 1066: | Train Loss: 0.01032 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1067: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.167 | Test Acc: 0.719 | Test Loss: 0.377\n",
      "Epoch 1068: | Train Loss: 0.01032 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.724 | Test Loss: 0.376\n",
      "Epoch 1069: | Train Loss: 0.01034 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.188 | Test Acc: 0.716 | Test Loss: 0.379\n",
      "Epoch 1070: | Train Loss: 0.01036 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.721 | Test Loss: 0.379\n",
      "Epoch 1071: | Train Loss: 0.01048 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.244 | Test Acc: 0.685 | Test Loss: 0.389\n",
      "Epoch 1072: | Train Loss: 0.01062 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.037 | Test Acc: 0.680 | Test Loss: 0.407\n",
      "Epoch 1073: | Train Loss: 0.01137 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.221 | Test Acc: 0.693 | Test Loss: 0.386\n",
      "Epoch 1074: | Train Loss: 0.01054 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.725 | Test Loss: 0.376\n",
      "Epoch 1075: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.724 | Test Loss: 0.376\n",
      "Epoch 1076: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.201 | Test Acc: 0.698 | Test Loss: 0.383\n",
      "Epoch 1077: | Train Loss: 0.01047 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.071 | Test Acc: 0.697 | Test Loss: 0.391\n",
      "Epoch 1078: | Train Loss: 0.01087 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.215 | Test Acc: 0.693 | Test Loss: 0.386\n",
      "Epoch 1079: | Train Loss: 0.01053 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.723 | Test Loss: 0.378\n",
      "Epoch 1080: | Train Loss: 0.01042 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.720 | Test Loss: 0.376\n",
      "Epoch 1081: | Train Loss: 0.01032 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 1082: | Train Loss: 0.01035 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.088 | Test Acc: 0.721 | Test Loss: 0.380\n",
      "Epoch 1083: | Train Loss: 0.01048 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.196 | Test Acc: 0.701 | Test Loss: 0.382\n",
      "Epoch 1084: | Train Loss: 0.01045 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.722 | Test Loss: 0.378\n",
      "Epoch 1085: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.167 | Test Acc: 0.720 | Test Loss: 0.377\n",
      "Epoch 1086: | Train Loss: 0.01033 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.720 | Test Loss: 0.376\n",
      "Epoch 1087: | Train Loss: 0.01032 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.726 | Test Loss: 0.377\n",
      "Epoch 1088: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.191 | Test Acc: 0.708 | Test Loss: 0.380\n",
      "Epoch 1089: | Train Loss: 0.01040 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.725 | Test Loss: 0.377\n",
      "Epoch 1090: | Train Loss: 0.01040 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.167 | Test Acc: 0.720 | Test Loss: 0.377\n",
      "Epoch 1091: | Train Loss: 0.01033 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.156 | Test Acc: 0.722 | Test Loss: 0.376\n",
      "Epoch 1092: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.727 | Test Loss: 0.376\n",
      "Epoch 1093: | Train Loss: 0.01034 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.181 | Test Acc: 0.713 | Test Loss: 0.379\n",
      "Epoch 1094: | Train Loss: 0.01036 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.726 | Test Loss: 0.377\n",
      "Epoch 1095: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.719 | Test Loss: 0.377\n",
      "Epoch 1096: | Train Loss: 0.01034 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.724 | Test Loss: 0.375\n",
      "Epoch 1097: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.142 | Test Acc: 0.724 | Test Loss: 0.375\n",
      "Epoch 1098: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.167 | Test Acc: 0.719 | Test Loss: 0.377\n",
      "Epoch 1099: | Train Loss: 0.01033 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.728 | Test Loss: 0.376\n",
      "Epoch 1100: | Train Loss: 0.01035 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.718 | Test Loss: 0.378\n",
      "Epoch 1101: | Train Loss: 0.01034 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.132 | Test Acc: 0.728 | Test Loss: 0.375\n",
      "Epoch 1102: | Train Loss: 0.01033 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.168 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1103: | Train Loss: 0.01031 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.724 | Test Loss: 0.375\n",
      "Epoch 1104: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.724 | Test Loss: 0.375\n",
      "Epoch 1105: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.168 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1106: | Train Loss: 0.01031 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.136 | Test Acc: 0.728 | Test Loss: 0.375\n",
      "Epoch 1107: | Train Loss: 0.01032 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.720 | Test Loss: 0.377\n",
      "Epoch 1108: | Train Loss: 0.01033 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.728 | Test Loss: 0.376\n",
      "Epoch 1109: | Train Loss: 0.01035 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.191 | Test Acc: 0.716 | Test Loss: 0.378\n",
      "Epoch 1110: | Train Loss: 0.01035 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.726 | Test Loss: 0.376\n",
      "Epoch 1111: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.713 | Test Loss: 0.379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1112: | Train Loss: 0.01037 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.724 | Test Loss: 0.377\n",
      "Epoch 1113: | Train Loss: 0.01042 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.193 | Test Acc: 0.708 | Test Loss: 0.380\n",
      "Epoch 1114: | Train Loss: 0.01039 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.723 | Test Loss: 0.378\n",
      "Epoch 1115: | Train Loss: 0.01045 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.199 | Test Acc: 0.707 | Test Loss: 0.381\n",
      "Epoch 1116: | Train Loss: 0.01041 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.722 | Test Loss: 0.378\n",
      "Epoch 1117: | Train Loss: 0.01045 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.193 | Test Acc: 0.709 | Test Loss: 0.380\n",
      "Epoch 1118: | Train Loss: 0.01039 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.725 | Test Loss: 0.377\n",
      "Epoch 1119: | Train Loss: 0.01040 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.190 | Test Acc: 0.714 | Test Loss: 0.378\n",
      "Epoch 1120: | Train Loss: 0.01035 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.728 | Test Loss: 0.375\n",
      "Epoch 1121: | Train Loss: 0.01034 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.720 | Test Loss: 0.377\n",
      "Epoch 1122: | Train Loss: 0.01031 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.136 | Test Acc: 0.729 | Test Loss: 0.374\n",
      "Epoch 1123: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.178 | Test Acc: 0.722 | Test Loss: 0.376\n",
      "Epoch 1124: | Train Loss: 0.01030 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.138 | Test Acc: 0.728 | Test Loss: 0.374\n",
      "Epoch 1125: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.723 | Test Loss: 0.375\n",
      "Epoch 1126: | Train Loss: 0.01029 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.139 | Test Acc: 0.729 | Test Loss: 0.374\n",
      "Epoch 1127: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.722 | Test Loss: 0.376\n",
      "Epoch 1128: | Train Loss: 0.01030 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.132 | Test Acc: 0.730 | Test Loss: 0.374\n",
      "Epoch 1129: | Train Loss: 0.01031 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.191 | Test Acc: 0.718 | Test Loss: 0.377\n",
      "Epoch 1130: | Train Loss: 0.01033 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.726 | Test Loss: 0.377\n",
      "Epoch 1131: | Train Loss: 0.01040 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.205 | Test Acc: 0.704 | Test Loss: 0.382\n",
      "Epoch 1132: | Train Loss: 0.01043 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.075 | Test Acc: 0.709 | Test Loss: 0.384\n",
      "Epoch 1133: | Train Loss: 0.01064 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.230 | Test Acc: 0.694 | Test Loss: 0.385\n",
      "Epoch 1134: | Train Loss: 0.01052 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.075 | Test Acc: 0.709 | Test Loss: 0.384\n",
      "Epoch 1135: | Train Loss: 0.01064 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.199 | Test Acc: 0.707 | Test Loss: 0.381\n",
      "Epoch 1136: | Train Loss: 0.01040 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.729 | Test Loss: 0.375\n",
      "Epoch 1137: | Train Loss: 0.01033 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.724 | Test Loss: 0.375\n",
      "Epoch 1138: | Train Loss: 0.01027 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.168 | Test Acc: 0.724 | Test Loss: 0.374\n",
      "Epoch 1139: | Train Loss: 0.01027 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.132 | Test Acc: 0.730 | Test Loss: 0.374\n",
      "Epoch 1140: | Train Loss: 0.01030 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.714 | Test Loss: 0.378\n",
      "Epoch 1141: | Train Loss: 0.01034 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.724 | Test Loss: 0.378\n",
      "Epoch 1142: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.199 | Test Acc: 0.705 | Test Loss: 0.381\n",
      "Epoch 1143: | Train Loss: 0.01041 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.723 | Test Loss: 0.378\n",
      "Epoch 1144: | Train Loss: 0.01045 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.714 | Test Loss: 0.378\n",
      "Epoch 1145: | Train Loss: 0.01035 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.128 | Test Acc: 0.731 | Test Loss: 0.374\n",
      "Epoch 1146: | Train Loss: 0.01031 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.725 | Test Loss: 0.374\n",
      "Epoch 1147: | Train Loss: 0.01027 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.164 | Test Acc: 0.726 | Test Loss: 0.374\n",
      "Epoch 1148: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.139 | Test Acc: 0.730 | Test Loss: 0.373\n",
      "Epoch 1149: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.722 | Test Loss: 0.376\n",
      "Epoch 1150: | Train Loss: 0.01029 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.729 | Test Loss: 0.375\n",
      "Epoch 1151: | Train Loss: 0.01033 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.715 | Test Loss: 0.378\n",
      "Epoch 1152: | Train Loss: 0.01034 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.727 | Test Loss: 0.376\n",
      "Epoch 1153: | Train Loss: 0.01039 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.712 | Test Loss: 0.379\n",
      "Epoch 1154: | Train Loss: 0.01035 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.727 | Test Loss: 0.376\n",
      "Epoch 1155: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.715 | Test Loss: 0.378\n",
      "Epoch 1156: | Train Loss: 0.01033 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.729 | Test Loss: 0.375\n",
      "Epoch 1157: | Train Loss: 0.01033 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.184 | Test Acc: 0.720 | Test Loss: 0.376\n",
      "Epoch 1158: | Train Loss: 0.01030 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.128 | Test Acc: 0.731 | Test Loss: 0.374\n",
      "Epoch 1159: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.178 | Test Acc: 0.724 | Test Loss: 0.375\n",
      "Epoch 1160: | Train Loss: 0.01027 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.731 | Test Loss: 0.373\n",
      "Epoch 1161: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.725 | Test Loss: 0.374\n",
      "Epoch 1162: | Train Loss: 0.01026 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.143 | Test Acc: 0.731 | Test Loss: 0.373\n",
      "Epoch 1163: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.726 | Test Loss: 0.374\n",
      "Epoch 1164: | Train Loss: 0.01026 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.143 | Test Acc: 0.731 | Test Loss: 0.373\n",
      "Epoch 1165: | Train Loss: 0.01026 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.178 | Test Acc: 0.725 | Test Loss: 0.374\n",
      "Epoch 1166: | Train Loss: 0.01026 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.132 | Test Acc: 0.732 | Test Loss: 0.373\n",
      "Epoch 1167: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.192 | Test Acc: 0.720 | Test Loss: 0.376\n",
      "Epoch 1168: | Train Loss: 0.01029 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.727 | Test Loss: 0.376\n",
      "Epoch 1169: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.224 | Test Acc: 0.701 | Test Loss: 0.382\n",
      "Epoch 1170: | Train Loss: 0.01044 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.067 | Test Acc: 0.703 | Test Loss: 0.389\n",
      "Epoch 1171: | Train Loss: 0.01081 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.238 | Test Acc: 0.692 | Test Loss: 0.386\n",
      "Epoch 1172: | Train Loss: 0.01054 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.071 | Test Acc: 0.711 | Test Loss: 0.383\n",
      "Epoch 1173: | Train Loss: 0.01063 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.200 | Test Acc: 0.711 | Test Loss: 0.379\n",
      "Epoch 1174: | Train Loss: 0.01036 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.132 | Test Acc: 0.732 | Test Loss: 0.373\n",
      "Epoch 1175: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.728 | Test Loss: 0.373\n",
      "Epoch 1176: | Train Loss: 0.01024 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.727 | Test Loss: 0.373\n",
      "Epoch 1177: | Train Loss: 0.01024 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.128 | Test Acc: 0.732 | Test Loss: 0.373\n",
      "Epoch 1178: | Train Loss: 0.01029 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.713 | Test Loss: 0.378\n",
      "Epoch 1179: | Train Loss: 0.01034 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.722 | Test Loss: 0.378\n",
      "Epoch 1180: | Train Loss: 0.01048 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.212 | Test Acc: 0.704 | Test Loss: 0.381\n",
      "Epoch 1181: | Train Loss: 0.01041 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.722 | Test Loss: 0.378\n",
      "Epoch 1182: | Train Loss: 0.01048 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.714 | Test Loss: 0.378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1183: | Train Loss: 0.01033 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.128 | Test Acc: 0.733 | Test Loss: 0.373\n",
      "Epoch 1184: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.728 | Test Loss: 0.373\n",
      "Epoch 1185: | Train Loss: 0.01023 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.729 | Test Loss: 0.373\n",
      "Epoch 1186: | Train Loss: 0.01023 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.733 | Test Loss: 0.372\n",
      "Epoch 1187: | Train Loss: 0.01025 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.184 | Test Acc: 0.722 | Test Loss: 0.375\n",
      "Epoch 1188: | Train Loss: 0.01027 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.731 | Test Loss: 0.374\n",
      "Epoch 1189: | Train Loss: 0.01032 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.714 | Test Loss: 0.378\n",
      "Epoch 1190: | Train Loss: 0.01032 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.728 | Test Loss: 0.376\n",
      "Epoch 1191: | Train Loss: 0.01039 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.713 | Test Loss: 0.378\n",
      "Epoch 1192: | Train Loss: 0.01033 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.729 | Test Loss: 0.375\n",
      "Epoch 1193: | Train Loss: 0.01035 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.195 | Test Acc: 0.719 | Test Loss: 0.376\n",
      "Epoch 1194: | Train Loss: 0.01029 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.733 | Test Loss: 0.373\n",
      "Epoch 1195: | Train Loss: 0.01028 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.725 | Test Loss: 0.374\n",
      "Epoch 1196: | Train Loss: 0.01025 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.734 | Test Loss: 0.372\n",
      "Epoch 1197: | Train Loss: 0.01024 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.729 | Test Loss: 0.373\n",
      "Epoch 1198: | Train Loss: 0.01022 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.733 | Test Loss: 0.372\n",
      "Epoch 1199: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.730 | Test Loss: 0.372\n",
      "Epoch 1200: | Train Loss: 0.01022 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.732 | Test Loss: 0.372\n",
      "Epoch 1201: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.730 | Test Loss: 0.372\n",
      "Epoch 1202: | Train Loss: 0.01021 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.732 | Test Loss: 0.372\n",
      "Epoch 1203: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.730 | Test Loss: 0.372\n",
      "Epoch 1204: | Train Loss: 0.01021 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.733 | Test Loss: 0.371\n",
      "Epoch 1205: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.175 | Test Acc: 0.730 | Test Loss: 0.372\n",
      "Epoch 1206: | Train Loss: 0.01021 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.135 | Test Acc: 0.735 | Test Loss: 0.371\n",
      "Epoch 1207: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.191 | Test Acc: 0.723 | Test Loss: 0.375\n",
      "Epoch 1208: | Train Loss: 0.01026 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.728 | Test Loss: 0.375\n",
      "Epoch 1209: | Train Loss: 0.01038 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.241 | Test Acc: 0.696 | Test Loss: 0.385\n",
      "Epoch 1210: | Train Loss: 0.01051 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.045 | Test Acc: 0.687 | Test Loss: 0.402\n",
      "Epoch 1211: | Train Loss: 0.01123 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.231 | Test Acc: 0.700 | Test Loss: 0.383\n",
      "Epoch 1212: | Train Loss: 0.01045 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.734 | Test Loss: 0.372\n",
      "Epoch 1213: | Train Loss: 0.01027 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.731 | Test Loss: 0.372\n",
      "Epoch 1214: | Train Loss: 0.01020 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.174 | Test Acc: 0.729 | Test Loss: 0.372\n",
      "Epoch 1215: | Train Loss: 0.01021 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.733 | Test Loss: 0.373\n",
      "Epoch 1216: | Train Loss: 0.01030 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.221 | Test Acc: 0.706 | Test Loss: 0.381\n",
      "Epoch 1217: | Train Loss: 0.01041 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.063 | Test Acc: 0.703 | Test Loss: 0.389\n",
      "Epoch 1218: | Train Loss: 0.01083 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.231 | Test Acc: 0.698 | Test Loss: 0.383\n",
      "Epoch 1219: | Train Loss: 0.01045 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.727 | Test Loss: 0.376\n",
      "Epoch 1220: | Train Loss: 0.01041 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.188 | Test Acc: 0.725 | Test Loss: 0.374\n",
      "Epoch 1221: | Train Loss: 0.01025 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.735 | Test Loss: 0.371\n",
      "Epoch 1222: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.157 | Test Acc: 0.735 | Test Loss: 0.371\n",
      "Epoch 1223: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.184 | Test Acc: 0.725 | Test Loss: 0.374\n",
      "Epoch 1224: | Train Loss: 0.01024 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.731 | Test Loss: 0.374\n",
      "Epoch 1225: | Train Loss: 0.01033 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.202 | Test Acc: 0.711 | Test Loss: 0.378\n",
      "Epoch 1226: | Train Loss: 0.01034 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.725 | Test Loss: 0.377\n",
      "Epoch 1227: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.196 | Test Acc: 0.715 | Test Loss: 0.377\n",
      "Epoch 1228: | Train Loss: 0.01031 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.735 | Test Loss: 0.372\n",
      "Epoch 1229: | Train Loss: 0.01027 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.174 | Test Acc: 0.729 | Test Loss: 0.372\n",
      "Epoch 1230: | Train Loss: 0.01021 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.160 | Test Acc: 0.734 | Test Loss: 0.371\n",
      "Epoch 1231: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.158 | Test Acc: 0.736 | Test Loss: 0.370\n",
      "Epoch 1232: | Train Loss: 0.01019 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.177 | Test Acc: 0.728 | Test Loss: 0.373\n",
      "Epoch 1233: | Train Loss: 0.01021 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.736 | Test Loss: 0.372\n",
      "Epoch 1234: | Train Loss: 0.01025 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.191 | Test Acc: 0.721 | Test Loss: 0.375\n",
      "Epoch 1235: | Train Loss: 0.01026 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.732 | Test Loss: 0.374\n",
      "Epoch 1236: | Train Loss: 0.01033 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.715 | Test Loss: 0.377\n",
      "Epoch 1237: | Train Loss: 0.01030 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.108 | Test Acc: 0.731 | Test Loss: 0.374\n",
      "Epoch 1238: | Train Loss: 0.01034 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.719 | Test Loss: 0.376\n",
      "Epoch 1239: | Train Loss: 0.01028 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.734 | Test Loss: 0.372\n",
      "Epoch 1240: | Train Loss: 0.01029 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.191 | Test Acc: 0.723 | Test Loss: 0.374\n",
      "Epoch 1241: | Train Loss: 0.01024 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.737 | Test Loss: 0.371\n",
      "Epoch 1242: | Train Loss: 0.01023 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.185 | Test Acc: 0.728 | Test Loss: 0.373\n",
      "Epoch 1243: | Train Loss: 0.01021 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.738 | Test Loss: 0.370\n",
      "Epoch 1244: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.181 | Test Acc: 0.729 | Test Loss: 0.372\n",
      "Epoch 1245: | Train Loss: 0.01020 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.738 | Test Loss: 0.370\n",
      "Epoch 1246: | Train Loss: 0.01021 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.188 | Test Acc: 0.727 | Test Loss: 0.373\n",
      "Epoch 1247: | Train Loss: 0.01021 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.737 | Test Loss: 0.371\n",
      "Epoch 1248: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.721 | Test Loss: 0.375\n",
      "Epoch 1249: | Train Loss: 0.01026 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.096 | Test Acc: 0.729 | Test Loss: 0.375\n",
      "Epoch 1250: | Train Loss: 0.01037 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.224 | Test Acc: 0.707 | Test Loss: 0.380\n",
      "Epoch 1251: | Train Loss: 0.01038 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.071 | Test Acc: 0.712 | Test Loss: 0.382\n",
      "Epoch 1252: | Train Loss: 0.01064 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.231 | Test Acc: 0.705 | Test Loss: 0.381\n",
      "Epoch 1253: | Train Loss: 0.01040 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.725 | Test Loss: 0.376\n",
      "Epoch 1254: | Train Loss: 0.01044 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.200 | Test Acc: 0.718 | Test Loss: 0.376\n",
      "Epoch 1255: | Train Loss: 0.01027 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.737 | Test Loss: 0.371\n",
      "Epoch 1256: | Train Loss: 0.01023 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.181 | Test Acc: 0.731 | Test Loss: 0.371\n",
      "Epoch 1257: | Train Loss: 0.01018 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.154 | Test Acc: 0.739 | Test Loss: 0.369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1258: | Train Loss: 0.01017 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.735 | Test Loss: 0.370\n",
      "Epoch 1259: | Train Loss: 0.01016 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.737 | Test Loss: 0.369\n",
      "Epoch 1260: | Train Loss: 0.01016 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.168 | Test Acc: 0.738 | Test Loss: 0.369\n",
      "Epoch 1261: | Train Loss: 0.01016 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.735 | Test Loss: 0.370\n",
      "Epoch 1262: | Train Loss: 0.01016 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.154 | Test Acc: 0.739 | Test Loss: 0.369\n",
      "Epoch 1263: | Train Loss: 0.01016 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.181 | Test Acc: 0.732 | Test Loss: 0.371\n",
      "Epoch 1264: | Train Loss: 0.01018 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.738 | Test Loss: 0.370\n",
      "Epoch 1265: | Train Loss: 0.01022 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.202 | Test Acc: 0.719 | Test Loss: 0.375\n",
      "Epoch 1266: | Train Loss: 0.01027 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.075 | Test Acc: 0.722 | Test Loss: 0.377\n",
      "Epoch 1267: | Train Loss: 0.01047 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.244 | Test Acc: 0.699 | Test Loss: 0.384\n",
      "Epoch 1268: | Train Loss: 0.01047 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.054 | Test Acc: 0.701 | Test Loss: 0.391\n",
      "Epoch 1269: | Train Loss: 0.01091 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.221 | Test Acc: 0.708 | Test Loss: 0.379\n",
      "Epoch 1270: | Train Loss: 0.01035 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.739 | Test Loss: 0.370\n",
      "Epoch 1271: | Train Loss: 0.01020 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.737 | Test Loss: 0.369\n",
      "Epoch 1272: | Train Loss: 0.01014 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.178 | Test Acc: 0.732 | Test Loss: 0.371\n",
      "Epoch 1273: | Train Loss: 0.01016 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.736 | Test Loss: 0.371\n",
      "Epoch 1274: | Train Loss: 0.01026 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.221 | Test Acc: 0.712 | Test Loss: 0.378\n",
      "Epoch 1275: | Train Loss: 0.01034 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.067 | Test Acc: 0.712 | Test Loss: 0.383\n",
      "Epoch 1276: | Train Loss: 0.01067 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.231 | Test Acc: 0.706 | Test Loss: 0.380\n",
      "Epoch 1277: | Train Loss: 0.01037 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.731 | Test Loss: 0.373\n",
      "Epoch 1278: | Train Loss: 0.01033 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.188 | Test Acc: 0.727 | Test Loss: 0.372\n",
      "Epoch 1279: | Train Loss: 0.01019 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.158 | Test Acc: 0.740 | Test Loss: 0.369\n",
      "Epoch 1280: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.168 | Test Acc: 0.740 | Test Loss: 0.369\n",
      "Epoch 1281: | Train Loss: 0.01014 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.178 | Test Acc: 0.734 | Test Loss: 0.370\n",
      "Epoch 1282: | Train Loss: 0.01016 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.740 | Test Loss: 0.370\n",
      "Epoch 1283: | Train Loss: 0.01021 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.200 | Test Acc: 0.723 | Test Loss: 0.374\n",
      "Epoch 1284: | Train Loss: 0.01024 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.731 | Test Loss: 0.374\n",
      "Epoch 1285: | Train Loss: 0.01035 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.219 | Test Acc: 0.712 | Test Loss: 0.377\n",
      "Epoch 1286: | Train Loss: 0.01030 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.728 | Test Loss: 0.375\n",
      "Epoch 1287: | Train Loss: 0.01039 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.206 | Test Acc: 0.718 | Test Loss: 0.375\n",
      "Epoch 1288: | Train Loss: 0.01026 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.738 | Test Loss: 0.371\n",
      "Epoch 1289: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.185 | Test Acc: 0.731 | Test Loss: 0.371\n",
      "Epoch 1290: | Train Loss: 0.01017 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.154 | Test Acc: 0.741 | Test Loss: 0.368\n",
      "Epoch 1291: | Train Loss: 0.01015 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.171 | Test Acc: 0.738 | Test Loss: 0.369\n",
      "Epoch 1292: | Train Loss: 0.01013 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.171 | Test Acc: 0.739 | Test Loss: 0.368\n",
      "Epoch 1293: | Train Loss: 0.01013 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.161 | Test Acc: 0.741 | Test Loss: 0.368\n",
      "Epoch 1294: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.178 | Test Acc: 0.734 | Test Loss: 0.370\n",
      "Epoch 1295: | Train Loss: 0.01015 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.741 | Test Loss: 0.369\n",
      "Epoch 1296: | Train Loss: 0.01018 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.724 | Test Loss: 0.373\n",
      "Epoch 1297: | Train Loss: 0.01021 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.096 | Test Acc: 0.732 | Test Loss: 0.373\n",
      "Epoch 1298: | Train Loss: 0.01034 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.231 | Test Acc: 0.710 | Test Loss: 0.379\n",
      "Epoch 1299: | Train Loss: 0.01034 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.067 | Test Acc: 0.713 | Test Loss: 0.381\n",
      "Epoch 1300: | Train Loss: 0.01062 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.231 | Test Acc: 0.706 | Test Loss: 0.379\n",
      "Epoch 1301: | Train Loss: 0.01035 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.729 | Test Loss: 0.374\n",
      "Epoch 1302: | Train Loss: 0.01036 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.196 | Test Acc: 0.723 | Test Loss: 0.373\n",
      "Epoch 1303: | Train Loss: 0.01022 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.741 | Test Loss: 0.369\n",
      "Epoch 1304: | Train Loss: 0.01019 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.185 | Test Acc: 0.735 | Test Loss: 0.370\n",
      "Epoch 1305: | Train Loss: 0.01014 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.154 | Test Acc: 0.743 | Test Loss: 0.368\n",
      "Epoch 1306: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.174 | Test Acc: 0.737 | Test Loss: 0.369\n",
      "Epoch 1307: | Train Loss: 0.01012 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.160 | Test Acc: 0.743 | Test Loss: 0.367\n",
      "Epoch 1308: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.739 | Test Loss: 0.368\n",
      "Epoch 1309: | Train Loss: 0.01011 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.160 | Test Acc: 0.743 | Test Loss: 0.367\n",
      "Epoch 1310: | Train Loss: 0.01011 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.739 | Test Loss: 0.368\n",
      "Epoch 1311: | Train Loss: 0.01011 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.154 | Test Acc: 0.743 | Test Loss: 0.367\n",
      "Epoch 1312: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.185 | Test Acc: 0.735 | Test Loss: 0.370\n",
      "Epoch 1313: | Train Loss: 0.01013 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.741 | Test Loss: 0.369\n",
      "Epoch 1314: | Train Loss: 0.01018 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.219 | Test Acc: 0.718 | Test Loss: 0.375\n",
      "Epoch 1315: | Train Loss: 0.01025 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.076 | Test Acc: 0.717 | Test Loss: 0.379\n",
      "Epoch 1316: | Train Loss: 0.01055 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.254 | Test Acc: 0.694 | Test Loss: 0.386\n",
      "Epoch 1317: | Train Loss: 0.01051 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.050 | Test Acc: 0.695 | Test Loss: 0.394\n",
      "Epoch 1318: | Train Loss: 0.01102 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.221 | Test Acc: 0.710 | Test Loss: 0.377\n",
      "Epoch 1319: | Train Loss: 0.01029 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.164 | Test Acc: 0.744 | Test Loss: 0.367\n",
      "Epoch 1320: | Train Loss: 0.01011 | Train F1: 0.005 | Train Acc: 0.020| Test F1: 0.139 | Test Acc: 0.743 | Test Loss: 0.367\n",
      "Epoch 1321: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.231 | Test Acc: 0.710 | Test Loss: 0.377\n",
      "Epoch 1322: | Train Loss: 0.01030 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.059 | Test Acc: 0.703 | Test Loss: 0.387\n",
      "Epoch 1323: | Train Loss: 0.01081 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.236 | Test Acc: 0.699 | Test Loss: 0.382\n",
      "Epoch 1324: | Train Loss: 0.01040 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.734 | Test Loss: 0.371\n",
      "Epoch 1325: | Train Loss: 0.01028 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.188 | Test Acc: 0.737 | Test Loss: 0.369\n",
      "Epoch 1326: | Train Loss: 0.01013 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.171 | Test Acc: 0.740 | Test Loss: 0.368\n",
      "Epoch 1327: | Train Loss: 0.01010 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.139 | Test Acc: 0.744 | Test Loss: 0.367\n",
      "Epoch 1328: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.196 | Test Acc: 0.726 | Test Loss: 0.372\n",
      "Epoch 1329: | Train Loss: 0.01019 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.733 | Test Loss: 0.374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1330: | Train Loss: 0.01036 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.224 | Test Acc: 0.720 | Test Loss: 0.376\n",
      "Epoch 1331: | Train Loss: 0.01028 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.733 | Test Loss: 0.373\n",
      "Epoch 1332: | Train Loss: 0.01034 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.198 | Test Acc: 0.724 | Test Loss: 0.372\n",
      "Epoch 1333: | Train Loss: 0.01018 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.131 | Test Acc: 0.744 | Test Loss: 0.367\n",
      "Epoch 1334: | Train Loss: 0.01013 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.174 | Test Acc: 0.740 | Test Loss: 0.368\n",
      "Epoch 1335: | Train Loss: 0.01009 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.742 | Test Loss: 0.367\n",
      "Epoch 1336: | Train Loss: 0.01009 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.147 | Test Acc: 0.745 | Test Loss: 0.367\n",
      "Epoch 1337: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.192 | Test Acc: 0.729 | Test Loss: 0.371\n",
      "Epoch 1338: | Train Loss: 0.01014 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.741 | Test Loss: 0.369\n",
      "Epoch 1339: | Train Loss: 0.01020 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.199 | Test Acc: 0.725 | Test Loss: 0.373\n",
      "Epoch 1340: | Train Loss: 0.01020 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.738 | Test Loss: 0.371\n",
      "Epoch 1341: | Train Loss: 0.01028 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.209 | Test Acc: 0.725 | Test Loss: 0.373\n",
      "Epoch 1342: | Train Loss: 0.01021 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.739 | Test Loss: 0.370\n",
      "Epoch 1343: | Train Loss: 0.01023 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.198 | Test Acc: 0.726 | Test Loss: 0.371\n",
      "Epoch 1344: | Train Loss: 0.01016 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.119 | Test Acc: 0.743 | Test Loss: 0.367\n",
      "Epoch 1345: | Train Loss: 0.01016 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.195 | Test Acc: 0.731 | Test Loss: 0.370\n",
      "Epoch 1346: | Train Loss: 0.01013 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.127 | Test Acc: 0.745 | Test Loss: 0.367\n",
      "Epoch 1347: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.188 | Test Acc: 0.736 | Test Loss: 0.369\n",
      "Epoch 1348: | Train Loss: 0.01011 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.127 | Test Acc: 0.746 | Test Loss: 0.367\n",
      "Epoch 1349: | Train Loss: 0.01012 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.194 | Test Acc: 0.736 | Test Loss: 0.369\n",
      "Epoch 1350: | Train Loss: 0.01012 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.119 | Test Acc: 0.744 | Test Loss: 0.368\n",
      "Epoch 1351: | Train Loss: 0.01016 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.199 | Test Acc: 0.726 | Test Loss: 0.372\n",
      "Epoch 1352: | Train Loss: 0.01018 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.096 | Test Acc: 0.732 | Test Loss: 0.372\n",
      "Epoch 1353: | Train Loss: 0.01032 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.233 | Test Acc: 0.708 | Test Loss: 0.378\n",
      "Epoch 1354: | Train Loss: 0.01032 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.067 | Test Acc: 0.713 | Test Loss: 0.381\n",
      "Epoch 1355: | Train Loss: 0.01062 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.236 | Test Acc: 0.704 | Test Loss: 0.379\n",
      "Epoch 1356: | Train Loss: 0.01034 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.729 | Test Loss: 0.372\n",
      "Epoch 1357: | Train Loss: 0.01035 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.202 | Test Acc: 0.723 | Test Loss: 0.372\n",
      "Epoch 1358: | Train Loss: 0.01018 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.123 | Test Acc: 0.744 | Test Loss: 0.367\n",
      "Epoch 1359: | Train Loss: 0.01015 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.195 | Test Acc: 0.738 | Test Loss: 0.368\n",
      "Epoch 1360: | Train Loss: 0.01010 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.135 | Test Acc: 0.747 | Test Loss: 0.366\n",
      "Epoch 1361: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.188 | Test Acc: 0.742 | Test Loss: 0.367\n",
      "Epoch 1362: | Train Loss: 0.01009 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.151 | Test Acc: 0.747 | Test Loss: 0.366\n",
      "Epoch 1363: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.188 | Test Acc: 0.739 | Test Loss: 0.368\n",
      "Epoch 1364: | Train Loss: 0.01008 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.139 | Test Acc: 0.746 | Test Loss: 0.366\n",
      "Epoch 1365: | Train Loss: 0.01010 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.195 | Test Acc: 0.731 | Test Loss: 0.369\n",
      "Epoch 1366: | Train Loss: 0.01011 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.112 | Test Acc: 0.740 | Test Loss: 0.368\n",
      "Epoch 1367: | Train Loss: 0.01018 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.225 | Test Acc: 0.716 | Test Loss: 0.375\n",
      "Epoch 1368: | Train Loss: 0.01023 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.076 | Test Acc: 0.719 | Test Loss: 0.377\n",
      "Epoch 1369: | Train Loss: 0.01051 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.247 | Test Acc: 0.704 | Test Loss: 0.381\n",
      "Epoch 1370: | Train Loss: 0.01038 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.067 | Test Acc: 0.714 | Test Loss: 0.380\n",
      "Epoch 1371: | Train Loss: 0.01061 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.225 | Test Acc: 0.713 | Test Loss: 0.375\n",
      "Epoch 1372: | Train Loss: 0.01025 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.115 | Test Acc: 0.742 | Test Loss: 0.367\n",
      "Epoch 1373: | Train Loss: 0.01016 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.188 | Test Acc: 0.736 | Test Loss: 0.368\n",
      "Epoch 1374: | Train Loss: 0.01008 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.165 | Test Acc: 0.748 | Test Loss: 0.365\n",
      "Epoch 1375: | Train Loss: 0.01005 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.178 | Test Acc: 0.747 | Test Loss: 0.365\n",
      "Epoch 1376: | Train Loss: 0.01004 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.178 | Test Acc: 0.743 | Test Loss: 0.366\n",
      "Epoch 1377: | Train Loss: 0.01005 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.161 | Test Acc: 0.748 | Test Loss: 0.365\n",
      "Epoch 1378: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.188 | Test Acc: 0.734 | Test Loss: 0.368\n",
      "Epoch 1379: | Train Loss: 0.01009 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.112 | Test Acc: 0.743 | Test Loss: 0.367\n",
      "Epoch 1380: | Train Loss: 0.01015 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.225 | Test Acc: 0.720 | Test Loss: 0.374\n",
      "Epoch 1381: | Train Loss: 0.01021 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.076 | Test Acc: 0.723 | Test Loss: 0.377\n",
      "Epoch 1382: | Train Loss: 0.01048 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.247 | Test Acc: 0.709 | Test Loss: 0.379\n",
      "Epoch 1383: | Train Loss: 0.01035 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.067 | Test Acc: 0.719 | Test Loss: 0.378\n",
      "Epoch 1384: | Train Loss: 0.01055 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.225 | Test Acc: 0.717 | Test Loss: 0.374\n",
      "Epoch 1385: | Train Loss: 0.01022 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.742 | Test Loss: 0.367\n",
      "Epoch 1386: | Train Loss: 0.01015 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.188 | Test Acc: 0.734 | Test Loss: 0.368\n",
      "Epoch 1387: | Train Loss: 0.01008 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.169 | Test Acc: 0.749 | Test Loss: 0.364\n",
      "Epoch 1388: | Train Loss: 0.01005 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.178 | Test Acc: 0.745 | Test Loss: 0.365\n",
      "Epoch 1389: | Train Loss: 0.01004 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.178 | Test Acc: 0.748 | Test Loss: 0.364\n",
      "Epoch 1390: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.171 | Test Acc: 0.749 | Test Loss: 0.364\n",
      "Epoch 1391: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.178 | Test Acc: 0.745 | Test Loss: 0.365\n",
      "Epoch 1392: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.169 | Test Acc: 0.749 | Test Loss: 0.364\n",
      "Epoch 1393: | Train Loss: 0.01004 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.188 | Test Acc: 0.737 | Test Loss: 0.367\n",
      "Epoch 1394: | Train Loss: 0.01006 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.127 | Test Acc: 0.745 | Test Loss: 0.365\n",
      "Epoch 1395: | Train Loss: 0.01011 | Train F1: 0.004 | Train Acc: 0.020| Test F1: 0.218 | Test Acc: 0.720 | Test Loss: 0.373\n",
      "Epoch 1396: | Train Loss: 0.01018 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.075 | Test Acc: 0.721 | Test Loss: 0.376\n",
      "Epoch 1397: | Train Loss: 0.01046 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.253 | Test Acc: 0.697 | Test Loss: 0.383\n",
      "Epoch 1398: | Train Loss: 0.01045 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.054 | Test Acc: 0.701 | Test Loss: 0.388\n",
      "Epoch 1399: | Train Loss: 0.01088 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.231 | Test Acc: 0.708 | Test Loss: 0.377\n",
      "Epoch 1400: | Train Loss: 0.01028 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.166 | Test Acc: 0.747 | Test Loss: 0.365\n",
      "Epoch 1401: | Train Loss: 0.01009 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.164 | Test Acc: 0.746 | Test Loss: 0.366\n",
      "Epoch 1402: | Train Loss: 0.01006 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.200 | Test Acc: 0.742 | Test Loss: 0.369\n",
      "Epoch 1403: | Train Loss: 0.01011 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.084 | Test Acc: 0.734 | Test Loss: 0.373\n",
      "Epoch 1404: | Train Loss: 0.01037 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.251 | Test Acc: 0.711 | Test Loss: 0.379\n",
      "Epoch 1405: | Train Loss: 0.01036 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.059 | Test Acc: 0.712 | Test Loss: 0.383\n",
      "Epoch 1406: | Train Loss: 0.01069 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.215 | Test Acc: 0.719 | Test Loss: 0.373\n",
      "Epoch 1407: | Train Loss: 0.01018 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.172 | Test Acc: 0.750 | Test Loss: 0.364\n",
      "Epoch 1408: | Train Loss: 0.01005 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.139 | Test Acc: 0.750 | Test Loss: 0.366\n",
      "Epoch 1409: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.225 | Test Acc: 0.735 | Test Loss: 0.372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1410: | Train Loss: 0.01019 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.067 | Test Acc: 0.726 | Test Loss: 0.378\n",
      "Epoch 1411: | Train Loss: 0.01053 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.233 | Test Acc: 0.721 | Test Loss: 0.374\n",
      "Epoch 1412: | Train Loss: 0.01023 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.742 | Test Loss: 0.367\n",
      "Epoch 1413: | Train Loss: 0.01016 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.181 | Test Acc: 0.736 | Test Loss: 0.367\n",
      "Epoch 1414: | Train Loss: 0.01006 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.178 | Test Acc: 0.751 | Test Loss: 0.364\n",
      "Epoch 1415: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.139 | Test Acc: 0.751 | Test Loss: 0.365\n",
      "Epoch 1416: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.200 | Test Acc: 0.739 | Test Loss: 0.369\n",
      "Epoch 1417: | Train Loss: 0.01011 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.092 | Test Acc: 0.741 | Test Loss: 0.369\n",
      "Epoch 1418: | Train Loss: 0.01024 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.222 | Test Acc: 0.724 | Test Loss: 0.372\n",
      "Epoch 1419: | Train Loss: 0.01018 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.738 | Test Loss: 0.368\n",
      "Epoch 1420: | Train Loss: 0.01023 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.726 | Test Loss: 0.370\n",
      "Epoch 1421: | Train Loss: 0.01012 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.143 | Test Acc: 0.749 | Test Loss: 0.364\n",
      "Epoch 1422: | Train Loss: 0.01007 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.181 | Test Acc: 0.742 | Test Loss: 0.366\n",
      "Epoch 1423: | Train Loss: 0.01003 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.751 | Test Loss: 0.363\n",
      "Epoch 1424: | Train Loss: 0.01001 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.165 | Test Acc: 0.751 | Test Loss: 0.363\n",
      "Epoch 1425: | Train Loss: 0.01001 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.184 | Test Acc: 0.743 | Test Loss: 0.366\n",
      "Epoch 1426: | Train Loss: 0.01003 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.123 | Test Acc: 0.749 | Test Loss: 0.364\n",
      "Epoch 1427: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.200 | Test Acc: 0.728 | Test Loss: 0.370\n",
      "Epoch 1428: | Train Loss: 0.01011 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.100 | Test Acc: 0.738 | Test Loss: 0.368\n",
      "Epoch 1429: | Train Loss: 0.01022 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.225 | Test Acc: 0.717 | Test Loss: 0.373\n",
      "Epoch 1430: | Train Loss: 0.01020 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.731 | Test Loss: 0.371\n",
      "Epoch 1431: | Train Loss: 0.01032 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.225 | Test Acc: 0.718 | Test Loss: 0.373\n",
      "Epoch 1432: | Train Loss: 0.01019 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.104 | Test Acc: 0.738 | Test Loss: 0.368\n",
      "Epoch 1433: | Train Loss: 0.01022 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.203 | Test Acc: 0.728 | Test Loss: 0.370\n",
      "Epoch 1434: | Train Loss: 0.01011 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.747 | Test Loss: 0.365\n",
      "Epoch 1435: | Train Loss: 0.01010 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.197 | Test Acc: 0.737 | Test Loss: 0.367\n",
      "Epoch 1436: | Train Loss: 0.01005 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.123 | Test Acc: 0.750 | Test Loss: 0.364\n",
      "Epoch 1437: | Train Loss: 0.01006 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.197 | Test Acc: 0.741 | Test Loss: 0.366\n",
      "Epoch 1438: | Train Loss: 0.01004 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.115 | Test Acc: 0.751 | Test Loss: 0.365\n",
      "Epoch 1439: | Train Loss: 0.01008 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.206 | Test Acc: 0.741 | Test Loss: 0.368\n",
      "Epoch 1440: | Train Loss: 0.01009 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.100 | Test Acc: 0.748 | Test Loss: 0.369\n",
      "Epoch 1441: | Train Loss: 0.01020 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.246 | Test Acc: 0.738 | Test Loss: 0.372\n",
      "Epoch 1442: | Train Loss: 0.01019 | Train F1: 0.007 | Train Acc: 0.021| Test F1: 0.080 | Test Acc: 0.743 | Test Loss: 0.375\n",
      "Epoch 1443: | Train Loss: 0.01040 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.249 | Test Acc: 0.740 | Test Loss: 0.373\n",
      "Epoch 1444: | Train Loss: 0.01024 | Train F1: 0.007 | Train Acc: 0.021| Test F1: 0.092 | Test Acc: 0.749 | Test Loss: 0.374\n",
      "Epoch 1445: | Train Loss: 0.01034 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.225 | Test Acc: 0.747 | Test Loss: 0.370\n",
      "Epoch 1446: | Train Loss: 0.01016 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.112 | Test Acc: 0.753 | Test Loss: 0.368\n",
      "Epoch 1447: | Train Loss: 0.01015 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.202 | Test Acc: 0.750 | Test Loss: 0.366\n",
      "Epoch 1448: | Train Loss: 0.01006 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.154 | Test Acc: 0.753 | Test Loss: 0.364\n",
      "Epoch 1449: | Train Loss: 0.01003 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.181 | Test Acc: 0.750 | Test Loss: 0.364\n",
      "Epoch 1450: | Train Loss: 0.01000 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.169 | Test Acc: 0.753 | Test Loss: 0.362\n",
      "Epoch 1451: | Train Loss: 0.00999 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.181 | Test Acc: 0.745 | Test Loss: 0.364\n",
      "Epoch 1452: | Train Loss: 0.00999 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.166 | Test Acc: 0.752 | Test Loss: 0.363\n",
      "Epoch 1453: | Train Loss: 0.01002 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.198 | Test Acc: 0.728 | Test Loss: 0.369\n",
      "Epoch 1454: | Train Loss: 0.01009 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.120 | Test Acc: 0.737 | Test Loss: 0.367\n",
      "Epoch 1455: | Train Loss: 0.01021 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.240 | Test Acc: 0.693 | Test Loss: 0.384\n",
      "Epoch 1456: | Train Loss: 0.01044 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.063 | Test Acc: 0.705 | Test Loss: 0.384\n",
      "Epoch 1457: | Train Loss: 0.01075 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.249 | Test Acc: 0.690 | Test Loss: 0.385\n",
      "Epoch 1458: | Train Loss: 0.01048 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.084 | Test Acc: 0.725 | Test Loss: 0.373\n",
      "Epoch 1459: | Train Loss: 0.01039 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.215 | Test Acc: 0.735 | Test Loss: 0.368\n",
      "Epoch 1460: | Train Loss: 0.01008 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.143 | Test Acc: 0.754 | Test Loss: 0.364\n",
      "Epoch 1461: | Train Loss: 0.01003 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.185 | Test Acc: 0.754 | Test Loss: 0.364\n",
      "Epoch 1462: | Train Loss: 0.01003 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.738 | Test Loss: 0.368\n",
      "Epoch 1463: | Train Loss: 0.01008 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.173 | Test Acc: 0.752 | Test Loss: 0.363\n",
      "Epoch 1464: | Train Loss: 0.01003 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.198 | Test Acc: 0.737 | Test Loss: 0.366\n",
      "Epoch 1465: | Train Loss: 0.01003 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.112 | Test Acc: 0.749 | Test Loss: 0.365\n",
      "Epoch 1466: | Train Loss: 0.01011 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.235 | Test Acc: 0.740 | Test Loss: 0.370\n",
      "Epoch 1467: | Train Loss: 0.01015 | Train F1: 0.007 | Train Acc: 0.021| Test F1: 0.088 | Test Acc: 0.746 | Test Loss: 0.374\n",
      "Epoch 1468: | Train Loss: 0.01035 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.241 | Test Acc: 0.742 | Test Loss: 0.371\n",
      "Epoch 1469: | Train Loss: 0.01017 | Train F1: 0.007 | Train Acc: 0.021| Test F1: 0.112 | Test Acc: 0.751 | Test Loss: 0.367\n",
      "Epoch 1470: | Train Loss: 0.01015 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.201 | Test Acc: 0.743 | Test Loss: 0.366\n",
      "Epoch 1471: | Train Loss: 0.01002 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.162 | Test Acc: 0.753 | Test Loss: 0.362\n",
      "Epoch 1472: | Train Loss: 0.01000 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.181 | Test Acc: 0.742 | Test Loss: 0.365\n",
      "Epoch 1473: | Train Loss: 0.01001 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.175 | Test Acc: 0.754 | Test Loss: 0.362\n",
      "Epoch 1474: | Train Loss: 0.01000 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.747 | Test Loss: 0.364\n",
      "Epoch 1475: | Train Loss: 0.00999 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.755 | Test Loss: 0.362\n",
      "Epoch 1476: | Train Loss: 0.00996 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.754 | Test Loss: 0.362\n",
      "Epoch 1477: | Train Loss: 0.00995 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.178 | Test Acc: 0.752 | Test Loss: 0.362\n",
      "Epoch 1478: | Train Loss: 0.00995 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.174 | Test Acc: 0.755 | Test Loss: 0.361\n",
      "Epoch 1479: | Train Loss: 0.00997 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.181 | Test Acc: 0.745 | Test Loss: 0.364\n",
      "Epoch 1480: | Train Loss: 0.00999 | Train F1: 0.005 | Train Acc: 0.021| Test F1: 0.177 | Test Acc: 0.753 | Test Loss: 0.362\n",
      "Epoch 1481: | Train Loss: 0.00999 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.198 | Test Acc: 0.735 | Test Loss: 0.366\n",
      "Epoch 1482: | Train Loss: 0.01003 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.112 | Test Acc: 0.744 | Test Loss: 0.365\n",
      "Epoch 1483: | Train Loss: 0.01012 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.244 | Test Acc: 0.717 | Test Loss: 0.375\n",
      "Epoch 1484: | Train Loss: 0.01023 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.054 | Test Acc: 0.709 | Test Loss: 0.385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1485: | Train Loss: 0.01077 | Train F1: 0.002 | Train Acc: 0.019| Test F1: 0.253 | Test Acc: 0.711 | Test Loss: 0.377\n",
      "Epoch 1486: | Train Loss: 0.01029 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.092 | Test Acc: 0.732 | Test Loss: 0.369\n",
      "Epoch 1487: | Train Loss: 0.01028 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.218 | Test Acc: 0.723 | Test Loss: 0.371\n",
      "Epoch 1488: | Train Loss: 0.01012 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.124 | Test Acc: 0.744 | Test Loss: 0.364\n",
      "Epoch 1489: | Train Loss: 0.01011 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.209 | Test Acc: 0.728 | Test Loss: 0.369\n",
      "Epoch 1490: | Train Loss: 0.01008 | Train F1: 0.006 | Train Acc: 0.020| Test F1: 0.116 | Test Acc: 0.743 | Test Loss: 0.365\n",
      "Epoch 1491: | Train Loss: 0.01014 | Train F1: 0.003 | Train Acc: 0.020| Test F1: 0.243 | Test Acc: 0.726 | Test Loss: 0.371\n",
      "Epoch 1492: | Train Loss: 0.01014 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.080 | Test Acc: 0.731 | Test Loss: 0.372\n",
      "Epoch 1493: | Train Loss: 0.01037 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.250 | Test Acc: 0.726 | Test Loss: 0.373\n",
      "Epoch 1494: | Train Loss: 0.01020 | Train F1: 0.007 | Train Acc: 0.020| Test F1: 0.080 | Test Acc: 0.736 | Test Loss: 0.372\n",
      "Epoch 1495: | Train Loss: 0.01034 | Train F1: 0.002 | Train Acc: 0.020| Test F1: 0.237 | Test Acc: 0.737 | Test Loss: 0.369\n",
      "Epoch 1496: | Train Loss: 0.01010 | Train F1: 0.007 | Train Acc: 0.021| Test F1: 0.112 | Test Acc: 0.752 | Test Loss: 0.364\n",
      "Epoch 1497: | Train Loss: 0.01007 | Train F1: 0.003 | Train Acc: 0.021| Test F1: 0.201 | Test Acc: 0.746 | Test Loss: 0.364\n",
      "Epoch 1498: | Train Loss: 0.00999 | Train F1: 0.006 | Train Acc: 0.021| Test F1: 0.159 | Test Acc: 0.756 | Test Loss: 0.361\n",
      "Epoch 1499: | Train Loss: 0.00997 | Train F1: 0.004 | Train Acc: 0.021| Test F1: 0.191 | Test Acc: 0.749 | Test Loss: 0.363\n"
     ]
    }
   ],
   "source": [
    "# Training separate models\n",
    "args.lr = 0.05\n",
    "classifiers = [NNClassifier(args) for _ in range(1)]\n",
    "\n",
    "# optimizers = [torch.optim.Adam(classifiers[i].classifier.parameters(), lr=args.lr) for i in range(args.n_clusters)]\n",
    "EPOCHS = 1500\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "\n",
    "latents_X = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id_train = model.clustering.update_assign(latents_X.detach().numpy())\n",
    "X_latents_data_loader = list(zip(latents_X, cluster_id_train, y_train))\n",
    "train_loader_latents = torch.utils.data.DataLoader(X_latents_data_loader,\n",
    "    batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "latents_test = model.autoencoder(torch.FloatTensor(np.array(X_test)).to(args.device), latent=True)\n",
    "\n",
    "# plot(latents_X, y_train, latents_test, y_test)\n",
    "\n",
    "for e in range(1, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    alpha = (1-e/EPOCHS)\n",
    "    acc = 0\n",
    "\n",
    "    new_y = []\n",
    "    y_pred = []\n",
    "    y_pred_idx, loss = classifiers[0].fit(latents_X, torch.tensor(y_train).to(device))\n",
    "    y_pred.append(y_pred_idx)\n",
    "    new_y.append(y_train)\n",
    "    epoch_loss += loss\n",
    "    y_pred = np.vstack(y_pred)\n",
    "    new_y = np.hstack(new_y)\n",
    "\n",
    "    f1 = f1_score(np.argmax(y_pred, axis=1), new_y)\n",
    "    acc = roc_auc_score(new_y, y_pred[:,1])\n",
    "    epoch_acc += acc.item()\n",
    "    epoch_f1 += f1.item()\n",
    "\n",
    "    test_preds = []\n",
    "    test_loss = 0.0\n",
    "    new_y_test = []\n",
    "    classifiers[0].classifier.eval()\n",
    "    latents_idx = latents_test\n",
    "    y_pred_idx = classifiers[0](latents_idx)\n",
    "    test_loss += nn.CrossEntropyLoss(reduction='mean')(y_pred_idx, torch.tensor(y_test).to(device))\n",
    "    test_preds.append(y_pred_idx.detach().numpy())\n",
    "    new_y_test.append(y_test)\n",
    "\n",
    "    test_preds = np.vstack(test_preds)\n",
    "    new_y_test = np.hstack(new_y_test).reshape(-1)\n",
    "    test_f1 = f1_score(np.argmax(test_preds, axis=1), new_y_test)\n",
    "    test_acc = roc_auc_score(new_y_test, test_preds[:,1])\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {epoch_loss/len(train_loader):.5f} | Train F1: {epoch_f1/len(train_loader):.3f} | Train Acc: {epoch_acc/len(train_loader):.3f}| Test F1: {test_f1:.3f} | Test Acc: {test_acc:.3f} | Test Loss: {test_loss:.3f}')\n",
    "\n",
    "out = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id = model.clustering.update_assign(out.cpu().detach().numpy())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Approch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNClassifier(\n",
      "  (criterion): CrossEntropyLoss()\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=117, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 001: | Train Loss: 0.52406 | Train F1: 0.000 | Train Acc: 0.623| Test F1: 0.000 | Test Acc: 0.727 | Test Loss: 0.381\n",
      "Epoch 002: | Train Loss: 0.34394 | Train F1: 0.000 | Train Acc: 0.784| Test F1: 0.000 | Test Acc: 0.816 | Test Loss: 0.326\n",
      "Epoch 003: | Train Loss: 0.31364 | Train F1: 0.000 | Train Acc: 0.842| Test F1: 0.000 | Test Acc: 0.844 | Test Loss: 0.310\n",
      "Epoch 004: | Train Loss: 0.29436 | Train F1: 0.000 | Train Acc: 0.864| Test F1: 0.000 | Test Acc: 0.854 | Test Loss: 0.304\n",
      "Epoch 005: | Train Loss: 0.28525 | Train F1: 0.000 | Train Acc: 0.875| Test F1: 0.000 | Test Acc: 0.856 | Test Loss: 0.302\n",
      "Epoch 006: | Train Loss: 0.28205 | Train F1: 0.000 | Train Acc: 0.884| Test F1: 0.000 | Test Acc: 0.856 | Test Loss: 0.302\n",
      "Epoch 007: | Train Loss: 0.27552 | Train F1: 0.000 | Train Acc: 0.890| Test F1: 0.000 | Test Acc: 0.859 | Test Loss: 0.299\n",
      "Epoch 008: | Train Loss: 0.26937 | Train F1: 0.000 | Train Acc: 0.895| Test F1: 0.000 | Test Acc: 0.858 | Test Loss: 0.301\n",
      "Epoch 009: | Train Loss: 0.26682 | Train F1: 0.000 | Train Acc: 0.899| Test F1: 0.000 | Test Acc: 0.859 | Test Loss: 0.303\n",
      "Epoch 010: | Train Loss: 0.26142 | Train F1: 0.000 | Train Acc: 0.904| Test F1: 0.000 | Test Acc: 0.861 | Test Loss: 0.298\n",
      "Epoch 011: | Train Loss: 0.25698 | Train F1: 0.021 | Train Acc: 0.906| Test F1: 0.411 | Test Acc: 0.860 | Test Loss: 0.300\n",
      "Epoch 012: | Train Loss: 0.25307 | Train F1: 0.480 | Train Acc: 0.911| Test F1: 0.435 | Test Acc: 0.861 | Test Loss: 0.299\n",
      "Epoch 013: | Train Loss: 0.24930 | Train F1: 0.522 | Train Acc: 0.915| Test F1: 0.450 | Test Acc: 0.859 | Test Loss: 0.301\n",
      "Epoch 014: | Train Loss: 0.24337 | Train F1: 0.532 | Train Acc: 0.917| Test F1: 0.446 | Test Acc: 0.858 | Test Loss: 0.305\n",
      "Epoch 015: | Train Loss: 0.24497 | Train F1: 0.545 | Train Acc: 0.918| Test F1: 0.465 | Test Acc: 0.859 | Test Loss: 0.304\n",
      "Epoch 016: | Train Loss: 0.24749 | Train F1: 0.567 | Train Acc: 0.919| Test F1: 0.466 | Test Acc: 0.857 | Test Loss: 0.306\n",
      "Epoch 017: | Train Loss: 0.23387 | Train F1: 0.585 | Train Acc: 0.925| Test F1: 0.468 | Test Acc: 0.857 | Test Loss: 0.305\n",
      "Epoch 018: | Train Loss: 0.22998 | Train F1: 0.614 | Train Acc: 0.927| Test F1: 0.474 | Test Acc: 0.857 | Test Loss: 0.305\n",
      "Epoch 019: | Train Loss: 0.22552 | Train F1: 0.617 | Train Acc: 0.932| Test F1: 0.482 | Test Acc: 0.857 | Test Loss: 0.306\n",
      "Epoch 020: | Train Loss: 0.22143 | Train F1: 0.622 | Train Acc: 0.935| Test F1: 0.476 | Test Acc: 0.853 | Test Loss: 0.313\n",
      "Epoch 021: | Train Loss: 0.21929 | Train F1: 0.632 | Train Acc: 0.935| Test F1: 0.478 | Test Acc: 0.853 | Test Loss: 0.315\n",
      "Epoch 022: | Train Loss: 0.21145 | Train F1: 0.646 | Train Acc: 0.935| Test F1: 0.494 | Test Acc: 0.849 | Test Loss: 0.325\n",
      "Epoch 023: | Train Loss: 0.20645 | Train F1: 0.647 | Train Acc: 0.940| Test F1: 0.501 | Test Acc: 0.852 | Test Loss: 0.320\n",
      "Epoch 024: | Train Loss: 0.20481 | Train F1: 0.660 | Train Acc: 0.940| Test F1: 0.498 | Test Acc: 0.849 | Test Loss: 0.330\n",
      "Epoch 025: | Train Loss: 0.20093 | Train F1: 0.666 | Train Acc: 0.943| Test F1: 0.513 | Test Acc: 0.850 | Test Loss: 0.327\n",
      "Epoch 026: | Train Loss: 0.20352 | Train F1: 0.656 | Train Acc: 0.941| Test F1: 0.491 | Test Acc: 0.848 | Test Loss: 0.330\n",
      "Epoch 027: | Train Loss: 0.19485 | Train F1: 0.671 | Train Acc: 0.946| Test F1: 0.497 | Test Acc: 0.848 | Test Loss: 0.337\n",
      "Epoch 028: | Train Loss: 0.19018 | Train F1: 0.678 | Train Acc: 0.948| Test F1: 0.495 | Test Acc: 0.848 | Test Loss: 0.336\n",
      "Epoch 029: | Train Loss: 0.18838 | Train F1: 0.673 | Train Acc: 0.951| Test F1: 0.475 | Test Acc: 0.840 | Test Loss: 0.350\n",
      "Epoch 030: | Train Loss: 0.18747 | Train F1: 0.685 | Train Acc: 0.952| Test F1: 0.497 | Test Acc: 0.844 | Test Loss: 0.346\n",
      "Epoch 031: | Train Loss: 0.18349 | Train F1: 0.690 | Train Acc: 0.953| Test F1: 0.493 | Test Acc: 0.846 | Test Loss: 0.350\n",
      "Epoch 032: | Train Loss: 0.18466 | Train F1: 0.692 | Train Acc: 0.953| Test F1: 0.488 | Test Acc: 0.844 | Test Loss: 0.356\n",
      "Epoch 033: | Train Loss: 0.17853 | Train F1: 0.710 | Train Acc: 0.956| Test F1: 0.486 | Test Acc: 0.837 | Test Loss: 0.363\n",
      "Epoch 034: | Train Loss: 0.17865 | Train F1: 0.681 | Train Acc: 0.956| Test F1: 0.496 | Test Acc: 0.842 | Test Loss: 0.356\n",
      "Epoch 035: | Train Loss: 0.17588 | Train F1: 0.710 | Train Acc: 0.957| Test F1: 0.481 | Test Acc: 0.838 | Test Loss: 0.368\n",
      "Epoch 036: | Train Loss: 0.17301 | Train F1: 0.705 | Train Acc: 0.958| Test F1: 0.483 | Test Acc: 0.845 | Test Loss: 0.365\n",
      "Epoch 037: | Train Loss: 0.16845 | Train F1: 0.724 | Train Acc: 0.961| Test F1: 0.496 | Test Acc: 0.839 | Test Loss: 0.374\n",
      "Epoch 038: | Train Loss: 0.16695 | Train F1: 0.718 | Train Acc: 0.962| Test F1: 0.498 | Test Acc: 0.837 | Test Loss: 0.380\n",
      "Epoch 039: | Train Loss: 0.17077 | Train F1: 0.718 | Train Acc: 0.958| Test F1: 0.494 | Test Acc: 0.841 | Test Loss: 0.380\n",
      "Epoch 040: | Train Loss: 0.16906 | Train F1: 0.715 | Train Acc: 0.961| Test F1: 0.521 | Test Acc: 0.838 | Test Loss: 0.380\n",
      "Epoch 041: | Train Loss: 0.16689 | Train F1: 0.736 | Train Acc: 0.962| Test F1: 0.491 | Test Acc: 0.832 | Test Loss: 0.390\n",
      "Epoch 042: | Train Loss: 0.16057 | Train F1: 0.736 | Train Acc: 0.965| Test F1: 0.490 | Test Acc: 0.828 | Test Loss: 0.398\n",
      "Epoch 043: | Train Loss: 0.15767 | Train F1: 0.732 | Train Acc: 0.966| Test F1: 0.498 | Test Acc: 0.837 | Test Loss: 0.397\n",
      "Epoch 044: | Train Loss: 0.15864 | Train F1: 0.740 | Train Acc: 0.967| Test F1: 0.495 | Test Acc: 0.829 | Test Loss: 0.406\n",
      "Epoch 045: | Train Loss: 0.15708 | Train F1: 0.748 | Train Acc: 0.967| Test F1: 0.499 | Test Acc: 0.829 | Test Loss: 0.407\n",
      "Epoch 046: | Train Loss: 0.15866 | Train F1: 0.742 | Train Acc: 0.966| Test F1: 0.491 | Test Acc: 0.831 | Test Loss: 0.417\n",
      "Epoch 047: | Train Loss: 0.15441 | Train F1: 0.750 | Train Acc: 0.967| Test F1: 0.487 | Test Acc: 0.822 | Test Loss: 0.426\n",
      "Epoch 048: | Train Loss: 0.15126 | Train F1: 0.751 | Train Acc: 0.970| Test F1: 0.498 | Test Acc: 0.826 | Test Loss: 0.423\n",
      "Epoch 049: | Train Loss: 0.15222 | Train F1: 0.748 | Train Acc: 0.969| Test F1: 0.511 | Test Acc: 0.826 | Test Loss: 0.431\n",
      "Epoch 050: | Train Loss: 0.14945 | Train F1: 0.766 | Train Acc: 0.970| Test F1: 0.487 | Test Acc: 0.828 | Test Loss: 0.437\n",
      "Epoch 051: | Train Loss: 0.14962 | Train F1: 0.760 | Train Acc: 0.970| Test F1: 0.498 | Test Acc: 0.829 | Test Loss: 0.434\n",
      "Epoch 052: | Train Loss: 0.14331 | Train F1: 0.772 | Train Acc: 0.972| Test F1: 0.500 | Test Acc: 0.825 | Test Loss: 0.438\n",
      "Epoch 053: | Train Loss: 0.14490 | Train F1: 0.773 | Train Acc: 0.972| Test F1: 0.490 | Test Acc: 0.825 | Test Loss: 0.457\n",
      "Epoch 054: | Train Loss: 0.14328 | Train F1: 0.775 | Train Acc: 0.973| Test F1: 0.501 | Test Acc: 0.829 | Test Loss: 0.447\n",
      "Epoch 055: | Train Loss: 0.14422 | Train F1: 0.776 | Train Acc: 0.971| Test F1: 0.486 | Test Acc: 0.824 | Test Loss: 0.459\n",
      "Epoch 056: | Train Loss: 0.14750 | Train F1: 0.775 | Train Acc: 0.971| Test F1: 0.493 | Test Acc: 0.825 | Test Loss: 0.459\n",
      "Epoch 057: | Train Loss: 0.13860 | Train F1: 0.792 | Train Acc: 0.973| Test F1: 0.488 | Test Acc: 0.822 | Test Loss: 0.477\n",
      "Epoch 058: | Train Loss: 0.13897 | Train F1: 0.783 | Train Acc: 0.970| Test F1: 0.498 | Test Acc: 0.826 | Test Loss: 0.475\n",
      "Epoch 059: | Train Loss: 0.13919 | Train F1: 0.780 | Train Acc: 0.973| Test F1: 0.482 | Test Acc: 0.819 | Test Loss: 0.491\n",
      "Epoch 060: | Train Loss: 0.13215 | Train F1: 0.795 | Train Acc: 0.976| Test F1: 0.490 | Test Acc: 0.822 | Test Loss: 0.484\n",
      "Epoch 061: | Train Loss: 0.13414 | Train F1: 0.804 | Train Acc: 0.976| Test F1: 0.481 | Test Acc: 0.819 | Test Loss: 0.493\n",
      "Epoch 062: | Train Loss: 0.13857 | Train F1: 0.792 | Train Acc: 0.973| Test F1: 0.476 | Test Acc: 0.817 | Test Loss: 0.491\n",
      "Epoch 063: | Train Loss: 0.13511 | Train F1: 0.794 | Train Acc: 0.975| Test F1: 0.486 | Test Acc: 0.816 | Test Loss: 0.503\n",
      "Epoch 064: | Train Loss: 0.13285 | Train F1: 0.798 | Train Acc: 0.975| Test F1: 0.502 | Test Acc: 0.825 | Test Loss: 0.495\n",
      "Epoch 065: | Train Loss: 0.13373 | Train F1: 0.806 | Train Acc: 0.976| Test F1: 0.492 | Test Acc: 0.822 | Test Loss: 0.502\n",
      "Epoch 066: | Train Loss: 0.12901 | Train F1: 0.806 | Train Acc: 0.977| Test F1: 0.485 | Test Acc: 0.818 | Test Loss: 0.522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 067: | Train Loss: 0.12918 | Train F1: 0.811 | Train Acc: 0.977| Test F1: 0.485 | Test Acc: 0.821 | Test Loss: 0.521\n",
      "Epoch 068: | Train Loss: 0.12418 | Train F1: 0.815 | Train Acc: 0.979| Test F1: 0.497 | Test Acc: 0.811 | Test Loss: 0.538\n",
      "Epoch 069: | Train Loss: 0.12706 | Train F1: 0.812 | Train Acc: 0.979| Test F1: 0.492 | Test Acc: 0.818 | Test Loss: 0.531\n",
      "Epoch 070: | Train Loss: 0.12307 | Train F1: 0.813 | Train Acc: 0.979| Test F1: 0.485 | Test Acc: 0.812 | Test Loss: 0.549\n",
      "Epoch 071: | Train Loss: 0.12835 | Train F1: 0.812 | Train Acc: 0.979| Test F1: 0.492 | Test Acc: 0.815 | Test Loss: 0.540\n",
      "Epoch 072: | Train Loss: 0.12411 | Train F1: 0.821 | Train Acc: 0.979| Test F1: 0.473 | Test Acc: 0.808 | Test Loss: 0.577\n",
      "Epoch 073: | Train Loss: 0.12397 | Train F1: 0.819 | Train Acc: 0.980| Test F1: 0.480 | Test Acc: 0.816 | Test Loss: 0.566\n",
      "Epoch 074: | Train Loss: 0.11894 | Train F1: 0.821 | Train Acc: 0.981| Test F1: 0.491 | Test Acc: 0.813 | Test Loss: 0.570\n",
      "Epoch 075: | Train Loss: 0.11764 | Train F1: 0.828 | Train Acc: 0.981| Test F1: 0.484 | Test Acc: 0.813 | Test Loss: 0.582\n",
      "Epoch 076: | Train Loss: 0.11721 | Train F1: 0.827 | Train Acc: 0.981| Test F1: 0.487 | Test Acc: 0.812 | Test Loss: 0.588\n",
      "Epoch 077: | Train Loss: 0.11295 | Train F1: 0.827 | Train Acc: 0.983| Test F1: 0.485 | Test Acc: 0.812 | Test Loss: 0.582\n",
      "Epoch 078: | Train Loss: 0.11276 | Train F1: 0.837 | Train Acc: 0.983| Test F1: 0.484 | Test Acc: 0.815 | Test Loss: 0.598\n",
      "Epoch 079: | Train Loss: 0.11359 | Train F1: 0.840 | Train Acc: 0.983| Test F1: 0.488 | Test Acc: 0.813 | Test Loss: 0.606\n",
      "Epoch 080: | Train Loss: 0.11342 | Train F1: 0.829 | Train Acc: 0.982| Test F1: 0.477 | Test Acc: 0.809 | Test Loss: 0.619\n",
      "Epoch 081: | Train Loss: 0.11472 | Train F1: 0.836 | Train Acc: 0.983| Test F1: 0.490 | Test Acc: 0.804 | Test Loss: 0.638\n",
      "Epoch 082: | Train Loss: 0.15499 | Train F1: 0.797 | Train Acc: 0.973| Test F1: 0.484 | Test Acc: 0.815 | Test Loss: 0.586\n",
      "Epoch 083: | Train Loss: 0.11927 | Train F1: 0.826 | Train Acc: 0.981| Test F1: 0.494 | Test Acc: 0.815 | Test Loss: 0.603\n",
      "Epoch 084: | Train Loss: 0.11329 | Train F1: 0.827 | Train Acc: 0.983| Test F1: 0.486 | Test Acc: 0.810 | Test Loss: 0.616\n",
      "Epoch 085: | Train Loss: 0.10768 | Train F1: 0.838 | Train Acc: 0.985| Test F1: 0.475 | Test Acc: 0.807 | Test Loss: 0.631\n",
      "Epoch 086: | Train Loss: 0.10345 | Train F1: 0.849 | Train Acc: 0.986| Test F1: 0.484 | Test Acc: 0.805 | Test Loss: 0.646\n",
      "Epoch 087: | Train Loss: 0.10201 | Train F1: 0.851 | Train Acc: 0.986| Test F1: 0.482 | Test Acc: 0.807 | Test Loss: 0.658\n",
      "Epoch 088: | Train Loss: 0.10092 | Train F1: 0.852 | Train Acc: 0.986| Test F1: 0.479 | Test Acc: 0.805 | Test Loss: 0.661\n",
      "Epoch 089: | Train Loss: 0.09951 | Train F1: 0.854 | Train Acc: 0.986| Test F1: 0.478 | Test Acc: 0.809 | Test Loss: 0.663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivin/miniconda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-7714abb4e718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mepoch_f1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mepoch_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         return _average_binary_score(partial(_binary_roc_auc_score,\n\u001b[0m\u001b[1;32m    543\u001b[0m                                              max_fpr=max_fpr),\n\u001b[1;32m    544\u001b[0m                                      \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;34m\"\"\"Binary roc auc score.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         raise ValueError(\"Only one class present in y_true. ROC AUC score \"\n\u001b[0m\u001b[1;32m    328\u001b[0m                          \"is not defined in that case.\")\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "args.latent_dim = args.input_dim\n",
    "# print(X_train.shape)\n",
    "m = NNClassifier(args)\n",
    "args.latent_dim = 20\n",
    "print(m)\n",
    "EPOCHS = 100\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "\n",
    "for e in range(1, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    alpha = (1-e/EPOCHS)\n",
    "    acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#         latents = model.autoencoder(torch.FloatTensor(np.array(X_batch)).to(args.device), latent=True)\n",
    "#         cluster_id = model.clustering.update_assign(latents.cpu().detach().numpy())\n",
    "        y_pred, train_loss = m.fit(X_batch, y_batch)\n",
    "        epoch_loss += train_loss\n",
    "\n",
    "        f1 = f1_score(np.argmax(y_pred, axis=1), y_batch.detach().numpy())\n",
    "        acc = roc_auc_score(y_batch.detach().numpy(), y_pred[:,1])\n",
    "        epoch_f1 += f1.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    test_loss = 0.0\n",
    "\n",
    "    m.classifier.eval()\n",
    "    test_pred = m(torch.FloatTensor(np.array(X_test)).to(args.device))\n",
    "    test_loss += nn.CrossEntropyLoss(reduction='mean')(test_pred, torch.tensor(y_test).to(device))\n",
    "\n",
    "    test_f1 = f1_score(np.argmax(test_pred.detach().numpy(), axis=1), y_test)\n",
    "    test_acc = roc_auc_score(y_test, test_pred[:,1].detach().numpy())\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {epoch_loss/len(train_loader):.5f} | Train F1: {epoch_f1/len(train_loader):.3f} | Train Acc: {epoch_acc/len(train_loader):.3f}| Test F1: {test_f1:.3f} | Test Acc: {test_acc:.3f} | Test Loss: {test_loss:.3f}')\n",
    "\n",
    "out = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id = model.clustering.update_assign(out.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No embedding Cluster-then-predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 1.07413 | Train F1: 0.225 | Train Acc: 0.507| Test F1: 0.317 | Test Acc: 0.603 | Test Loss: 0.919\n",
      "Epoch 002: | Train Loss: 0.75942 | Train F1: 0.392 | Train Acc: 0.502| Test F1: 0.394 | Test Acc: 0.609 | Test Loss: 0.917\n",
      "Epoch 003: | Train Loss: 0.69078 | Train F1: 0.463 | Train Acc: 0.500| Test F1: 0.418 | Test Acc: 0.631 | Test Loss: 0.945\n",
      "Epoch 004: | Train Loss: 0.63199 | Train F1: 0.524 | Train Acc: 0.496| Test F1: 0.422 | Test Acc: 0.684 | Test Loss: 1.034\n",
      "Epoch 005: | Train Loss: 0.58356 | Train F1: 0.600 | Train Acc: 0.495| Test F1: 0.438 | Test Acc: 0.715 | Test Loss: 1.084\n",
      "Epoch 006: | Train Loss: 0.54814 | Train F1: 0.646 | Train Acc: 0.496| Test F1: 0.414 | Test Acc: 0.729 | Test Loss: 1.146\n",
      "Epoch 007: | Train Loss: 0.52095 | Train F1: 0.673 | Train Acc: 0.500| Test F1: 0.412 | Test Acc: 0.727 | Test Loss: 1.427\n",
      "Epoch 008: | Train Loss: 0.51500 | Train F1: 0.666 | Train Acc: 0.504| Test F1: 0.421 | Test Acc: 0.729 | Test Loss: 1.431\n",
      "Epoch 009: | Train Loss: 0.47971 | Train F1: 0.694 | Train Acc: 0.501| Test F1: 0.448 | Test Acc: 0.755 | Test Loss: 1.541\n",
      "Epoch 010: | Train Loss: 0.41834 | Train F1: 0.733 | Train Acc: 0.509| Test F1: 0.441 | Test Acc: 0.755 | Test Loss: 1.723\n",
      "Epoch 011: | Train Loss: 0.40118 | Train F1: 0.750 | Train Acc: 0.507| Test F1: 0.446 | Test Acc: 0.767 | Test Loss: 1.838\n",
      "Epoch 012: | Train Loss: 0.38353 | Train F1: 0.759 | Train Acc: 0.504| Test F1: 0.435 | Test Acc: 0.775 | Test Loss: 2.088\n",
      "Epoch 013: | Train Loss: 0.36617 | Train F1: 0.769 | Train Acc: 0.503| Test F1: 0.419 | Test Acc: 0.753 | Test Loss: 1.890\n",
      "Epoch 014: | Train Loss: 0.36264 | Train F1: 0.767 | Train Acc: 0.506| Test F1: 0.412 | Test Acc: 0.753 | Test Loss: 1.928\n",
      "Epoch 015: | Train Loss: 0.34094 | Train F1: 0.773 | Train Acc: 0.509| Test F1: 0.419 | Test Acc: 0.760 | Test Loss: 2.111\n",
      "Epoch 016: | Train Loss: 0.31773 | Train F1: 0.797 | Train Acc: 0.509| Test F1: 0.403 | Test Acc: 0.754 | Test Loss: 2.417\n",
      "Epoch 017: | Train Loss: 0.31147 | Train F1: 0.792 | Train Acc: 0.509| Test F1: 0.403 | Test Acc: 0.747 | Test Loss: 2.391\n",
      "Epoch 018: | Train Loss: 0.28822 | Train F1: 0.804 | Train Acc: 0.511| Test F1: 0.397 | Test Acc: 0.744 | Test Loss: 2.621\n",
      "Epoch 019: | Train Loss: 0.29999 | Train F1: 0.793 | Train Acc: 0.508| Test F1: 0.398 | Test Acc: 0.745 | Test Loss: 2.626\n",
      "Epoch 020: | Train Loss: 0.27995 | Train F1: 0.816 | Train Acc: 0.510| Test F1: 0.423 | Test Acc: 0.747 | Test Loss: 2.883\n",
      "Epoch 021: | Train Loss: 0.29434 | Train F1: 0.808 | Train Acc: 0.509| Test F1: 0.380 | Test Acc: 0.744 | Test Loss: 2.666\n",
      "Epoch 022: | Train Loss: 0.26549 | Train F1: 0.831 | Train Acc: 0.508| Test F1: 0.388 | Test Acc: 0.750 | Test Loss: 2.857\n",
      "Epoch 023: | Train Loss: 0.28401 | Train F1: 0.826 | Train Acc: 0.509| Test F1: 0.408 | Test Acc: 0.768 | Test Loss: 2.691\n",
      "Epoch 024: | Train Loss: 0.26916 | Train F1: 0.830 | Train Acc: 0.508| Test F1: 0.397 | Test Acc: 0.766 | Test Loss: 2.620\n",
      "Epoch 025: | Train Loss: 0.28141 | Train F1: 0.829 | Train Acc: 0.508| Test F1: 0.401 | Test Acc: 0.759 | Test Loss: 2.740\n",
      "Epoch 026: | Train Loss: 0.27681 | Train F1: 0.835 | Train Acc: 0.507| Test F1: 0.390 | Test Acc: 0.752 | Test Loss: 2.824\n",
      "Epoch 027: | Train Loss: 0.23055 | Train F1: 0.849 | Train Acc: 0.503| Test F1: 0.430 | Test Acc: 0.767 | Test Loss: 2.955\n",
      "Epoch 028: | Train Loss: 0.21395 | Train F1: 0.868 | Train Acc: 0.508| Test F1: 0.426 | Test Acc: 0.774 | Test Loss: 3.490\n",
      "Epoch 029: | Train Loss: 0.23106 | Train F1: 0.868 | Train Acc: 0.505| Test F1: 0.396 | Test Acc: 0.769 | Test Loss: 3.147\n",
      "Epoch 030: | Train Loss: 0.24620 | Train F1: 0.861 | Train Acc: 0.507| Test F1: 0.405 | Test Acc: 0.763 | Test Loss: 3.127\n",
      "Epoch 031: | Train Loss: 0.23610 | Train F1: 0.863 | Train Acc: 0.508| Test F1: 0.410 | Test Acc: 0.767 | Test Loss: 3.404\n",
      "Epoch 032: | Train Loss: 0.21578 | Train F1: 0.872 | Train Acc: 0.510| Test F1: 0.417 | Test Acc: 0.775 | Test Loss: 3.445\n",
      "Epoch 033: | Train Loss: 0.19142 | Train F1: 0.888 | Train Acc: 0.504| Test F1: 0.405 | Test Acc: 0.776 | Test Loss: 3.482\n",
      "Epoch 034: | Train Loss: 0.19600 | Train F1: 0.887 | Train Acc: 0.507| Test F1: 0.375 | Test Acc: 0.758 | Test Loss: 3.723\n",
      "Epoch 035: | Train Loss: 0.23260 | Train F1: 0.884 | Train Acc: 0.508| Test F1: 0.400 | Test Acc: 0.775 | Test Loss: 3.110\n",
      "Epoch 036: | Train Loss: 0.23031 | Train F1: 0.872 | Train Acc: 0.503| Test F1: 0.392 | Test Acc: 0.775 | Test Loss: 3.395\n",
      "Epoch 037: | Train Loss: 0.21513 | Train F1: 0.883 | Train Acc: 0.504| Test F1: 0.401 | Test Acc: 0.774 | Test Loss: 3.357\n",
      "Epoch 038: | Train Loss: 0.21740 | Train F1: 0.878 | Train Acc: 0.507| Test F1: 0.402 | Test Acc: 0.763 | Test Loss: 3.840\n",
      "Epoch 039: | Train Loss: 0.21444 | Train F1: 0.875 | Train Acc: 0.507| Test F1: 0.400 | Test Acc: 0.755 | Test Loss: 3.450\n",
      "Epoch 040: | Train Loss: 0.20309 | Train F1: 0.885 | Train Acc: 0.510| Test F1: 0.378 | Test Acc: 0.745 | Test Loss: 3.741\n",
      "Epoch 041: | Train Loss: 0.19439 | Train F1: 0.889 | Train Acc: 0.505| Test F1: 0.398 | Test Acc: 0.769 | Test Loss: 4.249\n",
      "Epoch 042: | Train Loss: 0.19127 | Train F1: 0.898 | Train Acc: 0.503| Test F1: 0.399 | Test Acc: 0.764 | Test Loss: 3.811\n",
      "Epoch 043: | Train Loss: 0.19279 | Train F1: 0.892 | Train Acc: 0.500| Test F1: 0.379 | Test Acc: 0.770 | Test Loss: 3.875\n",
      "Epoch 044: | Train Loss: 0.17194 | Train F1: 0.899 | Train Acc: 0.503| Test F1: 0.412 | Test Acc: 0.770 | Test Loss: 4.124\n",
      "Epoch 045: | Train Loss: 0.16208 | Train F1: 0.904 | Train Acc: 0.502| Test F1: 0.375 | Test Acc: 0.751 | Test Loss: 4.489\n",
      "Epoch 046: | Train Loss: 0.16633 | Train F1: 0.911 | Train Acc: 0.499| Test F1: 0.375 | Test Acc: 0.760 | Test Loss: 4.578\n",
      "Epoch 047: | Train Loss: 0.18659 | Train F1: 0.892 | Train Acc: 0.501| Test F1: 0.390 | Test Acc: 0.758 | Test Loss: 3.910\n",
      "Epoch 048: | Train Loss: 0.18704 | Train F1: 0.900 | Train Acc: 0.505| Test F1: 0.405 | Test Acc: 0.771 | Test Loss: 4.409\n",
      "Epoch 049: | Train Loss: 0.19292 | Train F1: 0.895 | Train Acc: 0.502| Test F1: 0.400 | Test Acc: 0.760 | Test Loss: 4.029\n",
      "Epoch 050: | Train Loss: 0.15277 | Train F1: 0.918 | Train Acc: 0.501| Test F1: 0.414 | Test Acc: 0.769 | Test Loss: 4.660\n",
      "Epoch 051: | Train Loss: 0.12651 | Train F1: 0.937 | Train Acc: 0.500| Test F1: 0.370 | Test Acc: 0.748 | Test Loss: 5.434\n",
      "Epoch 052: | Train Loss: 0.14371 | Train F1: 0.922 | Train Acc: 0.505| Test F1: 0.377 | Test Acc: 0.753 | Test Loss: 5.421\n",
      "Epoch 053: | Train Loss: 0.14751 | Train F1: 0.920 | Train Acc: 0.508| Test F1: 0.370 | Test Acc: 0.748 | Test Loss: 5.589\n",
      "Epoch 054: | Train Loss: 0.21258 | Train F1: 0.895 | Train Acc: 0.505| Test F1: 0.384 | Test Acc: 0.747 | Test Loss: 4.606\n",
      "Epoch 055: | Train Loss: 0.18180 | Train F1: 0.899 | Train Acc: 0.504| Test F1: 0.383 | Test Acc: 0.753 | Test Loss: 4.880\n",
      "Epoch 056: | Train Loss: 0.16941 | Train F1: 0.913 | Train Acc: 0.504| Test F1: 0.403 | Test Acc: 0.748 | Test Loss: 4.751\n",
      "Epoch 057: | Train Loss: 0.14819 | Train F1: 0.920 | Train Acc: 0.502| Test F1: 0.381 | Test Acc: 0.758 | Test Loss: 4.926\n",
      "Epoch 058: | Train Loss: 0.11753 | Train F1: 0.943 | Train Acc: 0.501| Test F1: 0.389 | Test Acc: 0.753 | Test Loss: 5.467\n",
      "Epoch 059: | Train Loss: 0.11406 | Train F1: 0.941 | Train Acc: 0.504| Test F1: 0.393 | Test Acc: 0.757 | Test Loss: 5.919\n",
      "Epoch 060: | Train Loss: 0.11785 | Train F1: 0.932 | Train Acc: 0.503| Test F1: 0.394 | Test Acc: 0.758 | Test Loss: 6.073\n",
      "Epoch 061: | Train Loss: 0.12909 | Train F1: 0.936 | Train Acc: 0.502| Test F1: 0.404 | Test Acc: 0.766 | Test Loss: 5.450\n",
      "Epoch 062: | Train Loss: 0.11943 | Train F1: 0.937 | Train Acc: 0.505| Test F1: 0.396 | Test Acc: 0.759 | Test Loss: 5.785\n",
      "Epoch 063: | Train Loss: 0.13052 | Train F1: 0.931 | Train Acc: 0.503| Test F1: 0.401 | Test Acc: 0.762 | Test Loss: 5.788\n",
      "Epoch 064: | Train Loss: 0.13853 | Train F1: 0.932 | Train Acc: 0.503| Test F1: 0.396 | Test Acc: 0.764 | Test Loss: 5.789\n",
      "Epoch 065: | Train Loss: 0.11074 | Train F1: 0.940 | Train Acc: 0.503| Test F1: 0.391 | Test Acc: 0.768 | Test Loss: 5.821\n",
      "Epoch 066: | Train Loss: 0.10279 | Train F1: 0.945 | Train Acc: 0.503| Test F1: 0.397 | Test Acc: 0.761 | Test Loss: 6.346\n",
      "Epoch 067: | Train Loss: 0.09465 | Train F1: 0.947 | Train Acc: 0.506| Test F1: 0.410 | Test Acc: 0.754 | Test Loss: 6.347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 068: | Train Loss: 0.09898 | Train F1: 0.943 | Train Acc: 0.501| Test F1: 0.406 | Test Acc: 0.768 | Test Loss: 6.144\n",
      "Epoch 069: | Train Loss: 0.09376 | Train F1: 0.948 | Train Acc: 0.501| Test F1: 0.400 | Test Acc: 0.762 | Test Loss: 6.407\n",
      "Epoch 070: | Train Loss: 0.08615 | Train F1: 0.954 | Train Acc: 0.500| Test F1: 0.384 | Test Acc: 0.754 | Test Loss: 7.129\n",
      "Epoch 071: | Train Loss: 0.10286 | Train F1: 0.952 | Train Acc: 0.505| Test F1: 0.376 | Test Acc: 0.748 | Test Loss: 6.584\n",
      "Epoch 072: | Train Loss: 0.15353 | Train F1: 0.922 | Train Acc: 0.505| Test F1: 0.392 | Test Acc: 0.753 | Test Loss: 6.073\n",
      "Epoch 073: | Train Loss: 0.16696 | Train F1: 0.918 | Train Acc: 0.507| Test F1: 0.394 | Test Acc: 0.752 | Test Loss: 5.439\n",
      "Epoch 074: | Train Loss: 0.13221 | Train F1: 0.936 | Train Acc: 0.502| Test F1: 0.385 | Test Acc: 0.770 | Test Loss: 5.516\n",
      "Epoch 075: | Train Loss: 0.10566 | Train F1: 0.949 | Train Acc: 0.505| Test F1: 0.376 | Test Acc: 0.760 | Test Loss: 6.058\n",
      "Epoch 076: | Train Loss: 0.09083 | Train F1: 0.960 | Train Acc: 0.503| Test F1: 0.350 | Test Acc: 0.747 | Test Loss: 7.005\n",
      "Epoch 077: | Train Loss: 0.11567 | Train F1: 0.952 | Train Acc: 0.503| Test F1: 0.382 | Test Acc: 0.769 | Test Loss: 6.073\n",
      "Epoch 078: | Train Loss: 0.08106 | Train F1: 0.964 | Train Acc: 0.502| Test F1: 0.389 | Test Acc: 0.768 | Test Loss: 6.806\n",
      "Epoch 079: | Train Loss: 0.07928 | Train F1: 0.968 | Train Acc: 0.501| Test F1: 0.377 | Test Acc: 0.760 | Test Loss: 7.032\n",
      "Epoch 080: | Train Loss: 0.09798 | Train F1: 0.953 | Train Acc: 0.504| Test F1: 0.384 | Test Acc: 0.752 | Test Loss: 7.197\n",
      "Epoch 081: | Train Loss: 0.15086 | Train F1: 0.923 | Train Acc: 0.505| Test F1: 0.374 | Test Acc: 0.742 | Test Loss: 6.166\n",
      "Epoch 082: | Train Loss: 0.20685 | Train F1: 0.904 | Train Acc: 0.503| Test F1: 0.366 | Test Acc: 0.754 | Test Loss: 5.148\n",
      "Epoch 083: | Train Loss: 0.15944 | Train F1: 0.920 | Train Acc: 0.506| Test F1: 0.352 | Test Acc: 0.740 | Test Loss: 5.173\n",
      "Epoch 084: | Train Loss: 0.11647 | Train F1: 0.944 | Train Acc: 0.505| Test F1: 0.358 | Test Acc: 0.751 | Test Loss: 5.868\n",
      "Epoch 085: | Train Loss: 0.09687 | Train F1: 0.953 | Train Acc: 0.503| Test F1: 0.374 | Test Acc: 0.763 | Test Loss: 6.234\n",
      "Epoch 086: | Train Loss: 0.07292 | Train F1: 0.968 | Train Acc: 0.502| Test F1: 0.386 | Test Acc: 0.762 | Test Loss: 6.865\n",
      "Epoch 087: | Train Loss: 0.06633 | Train F1: 0.972 | Train Acc: 0.502| Test F1: 0.386 | Test Acc: 0.759 | Test Loss: 7.124\n",
      "Epoch 088: | Train Loss: 0.05788 | Train F1: 0.975 | Train Acc: 0.501| Test F1: 0.386 | Test Acc: 0.761 | Test Loss: 7.642\n",
      "Epoch 089: | Train Loss: 0.07219 | Train F1: 0.975 | Train Acc: 0.500| Test F1: 0.387 | Test Acc: 0.761 | Test Loss: 7.463\n",
      "Epoch 090: | Train Loss: 0.07951 | Train F1: 0.966 | Train Acc: 0.501| Test F1: 0.385 | Test Acc: 0.762 | Test Loss: 6.920\n",
      "Epoch 091: | Train Loss: 0.09383 | Train F1: 0.954 | Train Acc: 0.502| Test F1: 0.410 | Test Acc: 0.765 | Test Loss: 7.053\n",
      "Epoch 092: | Train Loss: 0.11043 | Train F1: 0.942 | Train Acc: 0.500| Test F1: 0.395 | Test Acc: 0.765 | Test Loss: 7.089\n",
      "Epoch 093: | Train Loss: 0.11107 | Train F1: 0.939 | Train Acc: 0.503| Test F1: 0.384 | Test Acc: 0.755 | Test Loss: 6.419\n",
      "Epoch 094: | Train Loss: 0.09860 | Train F1: 0.950 | Train Acc: 0.503| Test F1: 0.385 | Test Acc: 0.759 | Test Loss: 6.530\n",
      "Epoch 095: | Train Loss: 0.10808 | Train F1: 0.945 | Train Acc: 0.502| Test F1: 0.392 | Test Acc: 0.763 | Test Loss: 6.708\n",
      "Epoch 096: | Train Loss: 0.09028 | Train F1: 0.953 | Train Acc: 0.503| Test F1: 0.374 | Test Acc: 0.761 | Test Loss: 6.876\n",
      "Epoch 097: | Train Loss: 0.09063 | Train F1: 0.959 | Train Acc: 0.502| Test F1: 0.379 | Test Acc: 0.762 | Test Loss: 6.725\n",
      "Epoch 098: | Train Loss: 0.07735 | Train F1: 0.962 | Train Acc: 0.499| Test F1: 0.384 | Test Acc: 0.764 | Test Loss: 7.025\n",
      "Epoch 099: | Train Loss: 0.07614 | Train F1: 0.967 | Train Acc: 0.500| Test F1: 0.383 | Test Acc: 0.764 | Test Loss: 7.386\n"
     ]
    }
   ],
   "source": [
    "# Training separate models\n",
    "args.lr = 0.02\n",
    "args.latent_dim = args.input_dim\n",
    "classifiers = [NNClassifier(args) for _ in range(args.n_clusters)]\n",
    "args.latent_dim = 20\n",
    "# optimizers = [torch.optim.Adam(classifiers[i].classifier.parameters(), lr=args.lr) for i in range(args.n_clusters)]\n",
    "EPOCHS = 100\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "\n",
    "\n",
    "latents_X = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id_train = model.clustering.update_assign(latents_X.cpu().detach().numpy())\n",
    "\n",
    "X_latents_data_loader = list(zip(torch.FloatTensor(np.array(X_train)).to(args.device), cluster_id_train, y_train))\n",
    "train_loader_latents = torch.utils.data.DataLoader(X_latents_data_loader,\n",
    "    batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "latents_test = model.autoencoder(torch.FloatTensor(np.array(X_test)).to(args.device), latent=True)\n",
    "cluster_id_test = model.clustering.update_assign(latents_test.cpu().detach().numpy())\n",
    "# plot(latents_X, y_train, latents_test, y_test)\n",
    "\n",
    "for e in range(1, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    alpha = (1-e/EPOCHS)\n",
    "    acc = 0\n",
    "#     plot(latents_X, y_train, latents_test, y_test)\n",
    "\n",
    "    for X_batch, cluster_batch, y_batch in train_loader_latents:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        new_y_batch = []\n",
    "        y_pred = []\n",
    "        for k in range(args.n_clusters):\n",
    "            idx = np.where(cluster_batch == k)[0]\n",
    "            y_pred_idx, loss = classifiers[k].fit(X_batch[idx], y_batch[idx])\n",
    "#             print(\"F1 Cluster: \", k, f1_score(np.argmax(y_pred_idx, axis=1), y_batch[idx]))\n",
    "#             print(\"Cluster: \", k, len(idx))\n",
    "            new_y_batch.append(y_batch[idx])\n",
    "            y_pred.append(y_pred_idx)\n",
    "            epoch_loss += loss\n",
    "\n",
    "        y_pred = np.vstack(y_pred)\n",
    "        new_y_batch = np.hstack(new_y_batch)\n",
    "#         print(y_pred.shape, new_y_batch, len(X_batch))\n",
    "        f1 = f1_score(np.argmax(y_pred, axis=1), new_y_batch)\n",
    "        acc = roc_auc_score(y_batch, y_pred[:,1])\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_f1 += f1.item()\n",
    "\n",
    "    test_preds = []\n",
    "    test_loss = 0.0\n",
    "    new_y_test = []\n",
    "    \n",
    "    # Epoch Testing\n",
    "    for k in range(args.n_clusters):\n",
    "        classifiers[k].classifier.eval()\n",
    "        idx = np.where(cluster_id_test == k)[0]\n",
    "        latents_idx = latents_test[idx]\n",
    "        y_pred_idx = classifiers[k](torch.FloatTensor(np.array(X_test)).to(args.device)[idx])\n",
    "        test_loss += nn.CrossEntropyLoss(reduction='mean')(y_pred_idx, torch.tensor(y_test[idx]).to(device))\n",
    "        test_preds.append(y_pred_idx.detach().numpy())\n",
    "        new_y_test.append(y_test[idx])\n",
    "\n",
    "    test_preds = np.vstack(test_preds)\n",
    "    new_y_test = np.hstack(new_y_test).reshape(-1)\n",
    "    test_f1 = f1_score(np.argmax(test_preds, axis=1), new_y_test)\n",
    "    test_acc = roc_auc_score(new_y_test, test_preds[:,1])\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {epoch_loss/len(train_loader):.5f} | Train F1: {epoch_f1/len(train_loader):.3f} | Train Acc: {epoch_acc/len(train_loader):.3f}| Test F1: {test_f1:.3f} | Test Acc: {test_acc:.3f} | Test Loss: {test_loss:.3f}')\n",
    "\n",
    "out = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "cluster_id = model.clustering.update_assign(out.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMNN over CAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMNN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(DMNN, self).__init__()\n",
    "        self.args = args\n",
    "        self.input_dim = args.input_dim\n",
    "        self.output_dim = self.input_dim\n",
    "        self.n_classes = args.n_classes\n",
    "        self.n_clusters = args.n_clusters\n",
    "        self.latent_dim = args.latent_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(args.input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, self.latent_dim)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, args.input_dim)\n",
    "        )\n",
    "        self.classifiers = []\n",
    "        for _ in range(self.n_clusters):\n",
    "            self.classifiers.append(nn.Sequential(\n",
    "                nn.Linear(self.latent_dim, 8),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(8, 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4, args.n_classes)\n",
    "            ))\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, self.n_clusters),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, output=\"latent\"):\n",
    "        x = self.encoder(inputs)\n",
    "        if output == \"latent\":\n",
    "            return x\n",
    "        else:\n",
    "            outs = []\n",
    "            for i in range(self.n_clusters):\n",
    "                outs.append(self.classifiers[i](x))\n",
    "            return self.gate(x), self.decoder(x), outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "criterion_rec = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "dmnn = DMNN(args)\n",
    "dmnn.encoder = model.autoencoder.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 1.01595 | Acc: 0.143\n",
      "Epoch 002: | Loss: 1.00623 | Acc: 0.142\n",
      "Epoch 003: | Loss: 1.01526 | Acc: 0.142\n",
      "Epoch 004: | Loss: 1.01585 | Acc: 0.141\n",
      "Epoch 005: | Loss: 1.00990 | Acc: 0.143\n",
      "Epoch 006: | Loss: 1.00686 | Acc: 0.145\n",
      "Epoch 007: | Loss: 1.00647 | Acc: 0.141\n",
      "Epoch 008: | Loss: 1.01017 | Acc: 0.142\n",
      "Epoch 009: | Loss: 1.00969 | Acc: 0.143\n"
     ]
    }
   ],
   "source": [
    "dmnn.train()\n",
    "device = \"cpu\"\n",
    "EPOCHS = 10\n",
    "for e in range(1, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        X_inp = (X_batch + np.random.normal(0, 0.01, size=X_batch.shape)).clone().detach().float()\n",
    "        gate, X_rec, y_pred_local = dmnn(X_inp, output=\"decoded\")\n",
    "#         print(y_pred)\n",
    "#         print(gate)\n",
    "        y_pred = torch.zeros((len(X_inp), args.n_classes))\n",
    "        for j in range(args.n_clusters):\n",
    "            y_pred += gate[:,j].reshape(-1,1)*y_pred_local[j]\n",
    "        \n",
    "        acc = f1_score(np.argmax(y_pred.detach().numpy(), axis=1), y_batch.unsqueeze(1))\n",
    "\n",
    "        loss_rec = criterion_rec(X_batch, X_rec)\n",
    "        loss_rec.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss_rec.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 1.70800 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 002: | Loss: 1.70806 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 003: | Loss: 1.70827 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 004: | Loss: 1.70864 | Acc: 0.495 | Test Acc: 0.500\n",
      "Epoch 005: | Loss: 1.70823 | Acc: 0.495 | Test Acc: 0.500\n",
      "Epoch 006: | Loss: 1.70844 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 007: | Loss: 1.70799 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 008: | Loss: 1.70808 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 009: | Loss: 1.70822 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 010: | Loss: 1.70845 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 011: | Loss: 1.70791 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 012: | Loss: 1.70862 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 013: | Loss: 1.70786 | Acc: 0.492 | Test Acc: 0.500\n",
      "Epoch 014: | Loss: 1.70811 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 015: | Loss: 1.70786 | Acc: 0.495 | Test Acc: 0.500\n",
      "Epoch 016: | Loss: 1.70861 | Acc: 0.492 | Test Acc: 0.500\n",
      "Epoch 017: | Loss: 1.70820 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 018: | Loss: 1.70843 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 019: | Loss: 1.70846 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 020: | Loss: 1.70835 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 021: | Loss: 1.70829 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 022: | Loss: 1.70829 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 023: | Loss: 1.70802 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 024: | Loss: 1.70858 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 025: | Loss: 1.70795 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 026: | Loss: 1.70842 | Acc: 0.492 | Test Acc: 0.500\n",
      "Epoch 027: | Loss: 1.70863 | Acc: 0.492 | Test Acc: 0.500\n",
      "Epoch 028: | Loss: 1.70776 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 029: | Loss: 1.70817 | Acc: 0.492 | Test Acc: 0.500\n",
      "Epoch 030: | Loss: 1.70836 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 031: | Loss: 1.70758 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 032: | Loss: 1.70775 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 033: | Loss: 1.70823 | Acc: 0.496 | Test Acc: 0.500\n",
      "Epoch 034: | Loss: 1.70816 | Acc: 0.492 | Test Acc: 0.500\n",
      "Epoch 035: | Loss: 1.70830 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 036: | Loss: 1.70795 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 037: | Loss: 1.70813 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 038: | Loss: 1.70782 | Acc: 0.493 | Test Acc: 0.500\n",
      "Epoch 039: | Loss: 1.70839 | Acc: 0.494 | Test Acc: 0.500\n",
      "Epoch 040: | Loss: 1.70794 | Acc: 0.495 | Test Acc: 0.500\n",
      "Epoch 041: | Loss: 1.70854 | Acc: 0.492 | Test Acc: 0.500\n",
      "Epoch 042: | Loss: 1.70825 | Acc: 0.492 | Test Acc: 0.500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-6884c5ddcd07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#         acc = f1_score(np.argmax(y_pred.detach().numpy(), axis=1), y_batch.unsqueeze(1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss_rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training separate models\n",
    "args.lr = 0.002\n",
    "EPOCHS = 300\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "\n",
    "####\n",
    "model.train()\n",
    "device = \"cpu\"\n",
    "EPOCHS = 100\n",
    "\n",
    "latents_X = model.autoencoder(torch.FloatTensor(np.array(X_train)).to(args.device), latent=True)\n",
    "X_latents_data_loader = list(zip(latents_X, cluster_id_train, y_train))\n",
    "train_loader_latents = torch.utils.data.DataLoader(X_latents_data_loader,\n",
    "    batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "latents_test = model.autoencoder(torch.FloatTensor(np.array(X_test)).to(args.device), latent=True)\n",
    "\n",
    "for param in dmnn.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for e in range(1, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    alpha = (1-e/EPOCHS)\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        gate, X_tilde, y_pred_local = dmnn(X_batch, output=\"decoded\")\n",
    "        X_latent = dmnn(X_batch, output=\"latent\")\n",
    "        loss = torch.tensor(0.).to(device)\n",
    "        loss_rec = torch.tensor(0.).to(device)\n",
    "        y_pred = torch.zeros((len(X_batch), args.n_classes))\n",
    "\n",
    "        for j in range(args.n_clusters):\n",
    "            loss += torch.sum(gate[j].reshape(-1,1)*criterion(y_pred_local[j], y_batch))\n",
    "            y_pred += gate[:,j].reshape(-1,1)*y_pred_local[j]\n",
    "\n",
    "#         loss = criterion(y_pred, y_batch)\n",
    "        gate.detach()\n",
    "        loss_rec = 0.4*criterion_rec(X_batch, X_tilde)\n",
    "#         acc = f1_score(np.argmax(y_pred.detach().numpy(), axis=1), y_batch.unsqueeze(1))\n",
    "        acc = roc_auc_score(y_batch.unsqueeze(1), y_pred[:,1].detach().numpy())\n",
    "        loss_rec.backward(retain_graph=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += (loss+loss_rec).item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    gate, X1, y_pred_local = dmnn(torch.tensor(X_test).to(device).float(), output=\"decoded\")\n",
    "    y_pred = torch.zeros((len(X1), args.n_classes))\n",
    "    for j in range(args.n_clusters):\n",
    "        y_pred += gate[:,j].reshape(-1,1)*y_pred_local[j]\n",
    "\n",
    "#     test_f1 = f1_score(np.argmax(y_pred.detach().numpy(), axis=1), y_test)\n",
    "    test_f1 = roc_auc_score(y_test, y_pred[:,1].detach().numpy())\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f} | Test Acc: {test_f1:.3f}')\n",
    "#### Synthetic Max AUC: 0.833: alpha=2.5, 0.848: alpha=0\n",
    "#### wid-- CAC_DMNN: F1=~0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "X_train = X_train.astype(float)\n",
    "gate, X1, preds = dmnn(torch.tensor(X_train).float().to(device), output=\"decoded\")\n",
    "x_emb_train = reducer.fit_transform(X1.detach().numpy())\n",
    "x_emb_test = reducer.transform(dmnn(torch.tensor(X_test).float().to(device)).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 1,  ..., 1, 0, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fcfc4a1b190>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(torch.argmax(gate, axis=1))\n",
    "c_clusters = [color[int(y_train[i])] for i in range(len(y_train))]\n",
    "plt.scatter(x_emb_train[:,0], x_emb_train[:,1], color=c_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017895515130531153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fcfb485c5b0>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "clusters = torch.argmax(gate, axis=1).numpy()\n",
    "print(nmi(clusters, y_train))\n",
    "c_clusters = [color[int(clusters[i])] for i in range(len(y_train))]\n",
    "plt.scatter(x_emb_train[:,0], x_emb_train[:,1], color=c_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
